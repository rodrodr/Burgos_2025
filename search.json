[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "",
    "text": "Esta p√°gina web forma parte del curso ‚ÄúAn√°lisis de textos pol√≠ticos con R‚Äù. En ella, se recogen los materiales y ejemplos que se desarrollar√°n durante las sesiones.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#el-curso",
    "href": "index.html#el-curso",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "",
    "text": "Esta p√°gina web forma parte del curso ‚ÄúAn√°lisis de textos pol√≠ticos con R‚Äù. En ella, se recogen los materiales y ejemplos que se desarrollar√°n durante las sesiones.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#el-profesores",
    "href": "index.html#el-profesores",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "EL profesores",
    "text": "EL profesores\n\n\n\n\n\n\nRodrigo Rodrigues-Silveira\nrodrodr@usal.es\nProfesor de ciencia pol√≠tica de la USAL. Director del proyecto ‚ÄúComportamiento legislativo y erosi√≥n democr√°tica en Am√©rica Latina‚Äù (PELA Comportamiento). Miembro de los GIR ‚ÄúPol√≠tica Comparada en Am√©rica Latina‚Äù y ‚ÄúTecnolog√≠a y poder en el pensamiento y las letras‚Äù.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#el-contenido",
    "href": "index.html#el-contenido",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "El contenido",
    "text": "El contenido\nDurante el curso se abordar√°n los siguientes temas:\n\npreparaci√≥n de textos literarios para an√°lisis cuantitativos\nan√°lisis de frecuencias de palabras\ncodificaci√≥n tem√°tica\nan√°lisis de conglomerados clusters\nescalonado de textos (uni y multidimensional)\nmodelado de t√≥picos\nan√°lisis de redes sociales\nt√©cnicas de visualizaci√≥n de texto",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#sesiones",
    "href": "index.html#sesiones",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "Sesiones",
    "text": "Sesiones\nLas sesiones tendr√°n lugar en el et√©reo espacio de la nube, en la plataforma Zoom. Los enlaces de acceso a las sesiones se enviar√°n a los correos electr√≥nicos de los participantes.\nSesiones virtuales:\nD√çA 1 - 23/10/2025 de 9:30 a 13:30h - Preparaci√≥n de textos\nD√çA 2 - 27/10/2025 de 9:30 a 13:30h - An√°lisis b√°sicos\nD√çA 3 - 10/11/2025 de 9:30 a 13:30h - Codificaci√≥n tem√°tica\n\nSesiones presenciales:\nD√çA 4 - 10/11/2025 de 9:30 a 13:30h - Escalonado de texto\nD√çA 5 - 11/11/2025 de 9:30 a 13:30h - An√°lisis de redes",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#servicio-t√©cnico",
    "href": "index.html#servicio-t√©cnico",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "ü§ñ Servicio t√©cnico ü§ñ",
    "text": "ü§ñ Servicio t√©cnico ü§ñ\nPara que pod√°is reproducir los ejemplos de an√°lisis presentados durante el curso deb√©is instalar en vuestros ordenadores el R y el RStudio Desktop.\nTambi√©n deb√©is ejecutar el siguiente c√≥digo en R que instala los paquetes necesarios.\nPREPARATIVOS\nPara que funcione el c√≥digo abajo:\n\ndebes ejecutar cada l√≠nea de una en una y esperar que finalice antes de ejecutar la siguiente. Primero la que empieza con pc, luego, install.packages(pc) y, finalmente, una vez terminado el paso anterior: devtools::install_github(‚Äúrodrodr/tenet‚Äù, force=T).\nPuede que aparezcan algunos mensajes. El primero es si quieres reiniciar R, le das a ‚ÄúNo‚Äù. El segundo es si deseas actualizar los paquetes y debes decir: ‚Äú1 - All‚Äù. Finalmente, si te pide instalar paquetes ‚Äúfrom source‚Äù (en Mac puede aparecer), elegid, ‚Äún‚Äù (no).\n\nCon eso, tendr√©is para instalar los paquetes necesarios para el curso.\n\n\nCode\n# Crea un vector con los paquetes a instalar\npc &lt;- c(\"miniUI\",\"quanteda\",\"quanteda.textplots\",\"pdftools\",\n        \"quanteda.textmodels\",\"quanteda.textstats\",\n        \"stringi\",\"readr\",\"gutenbergr\",\"ggplot2\",\n        \"ggrepel\",\"reactable\",\"tidyverse\",\"devtools\",\n        \"egg\",\"network\",\"sna\",\"ggnetwork\",\"poliscidata\",\n        \"udpipe\",\"dplyr\",\"syuzhet\",\"ggiraph\",\"networkD3\",\n        \"igraph\",\"topicmodels\",\"wordcloud\",\"readtext\",\n        \"rvest\",\"tesseract\",\"stringdist\",\"htmltools\"\n        \"jsonlite\",\"gridExtra\",\"grid\",\"ngramr\",\n        \"DiagrammeR\")\n\n# Instala los paquetes\ninstall.packages(pc)\n\n# Instala el paquete tenet que no est√° en CRAN\ndevtools::install_github(\"rodrodr/tenet\", force=T)",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "preparacion.html",
    "href": "preparacion.html",
    "title": "Preparaci√≥n de los textos",
    "section": "",
    "text": "El primer paso en cualquier an√°lisis de texto consiste en la obtenci√≥n y preparaci√≥n de los datos. La obtenci√≥n se puede llevar a cabo de distintas maneras, incluyendo el web scraping o la descarga manual de archivos. En este apartado, exploraremos la lectura y preparaci√≥n de archivos ya descargados para su posterior an√°lisis. Por eso, una vez tengamos los archivos descargados y guardados en la nube o en una carpeta en el disco duro de nuestro ordenador, podemos ir un paso m√°s all√° para abrirlos en R, extraer informaci√≥n y limpiarlos si resulta necesario. Veremos c√≥mo abrir archivos con distintos formatos (txt, PDF, docx entre otros) y extraer texto de PDFs sin tratar y limpiarlos antes de pasar a la siguiente fase de organizaci√≥n de los corpora.\nEste peque√±o apartado tiene tres secciones. La primera abre archivos de texto en R que ser√°n luego empleados para la creaci√≥n de un todo coherente, relativamente comparable, que se someter√° al an√°lisis (corpus). La segunda realiza un OCR en archivos en formato PDF para, luego, extraer el texto. Finalmente, la tercera utiliza un conjunto de funciones para la limpieza y b√∫squeda sistem√°tica de texto, as√≠ como introduce nuestras nuevas mejores amigas: las expresiones regulares. Ya advierto, se trata de una relaci√≥n amor-odio, pues se trata de una t√©cnica de extracci√≥n de datos potent√≠sima, pero, a la vez, compleja.\nVamos paso a paso‚Ä¶",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#introducci√≥n",
    "href": "preparacion.html#introducci√≥n",
    "title": "Preparaci√≥n de los textos",
    "section": "",
    "text": "El primer paso en cualquier an√°lisis de texto consiste en la obtenci√≥n y preparaci√≥n de los datos. La obtenci√≥n se puede llevar a cabo de distintas maneras, incluyendo el web scraping o la descarga manual de archivos. En este apartado, exploraremos la lectura y preparaci√≥n de archivos ya descargados para su posterior an√°lisis. Por eso, una vez tengamos los archivos descargados y guardados en la nube o en una carpeta en el disco duro de nuestro ordenador, podemos ir un paso m√°s all√° para abrirlos en R, extraer informaci√≥n y limpiarlos si resulta necesario. Veremos c√≥mo abrir archivos con distintos formatos (txt, PDF, docx entre otros) y extraer texto de PDFs sin tratar y limpiarlos antes de pasar a la siguiente fase de organizaci√≥n de los corpora.\nEste peque√±o apartado tiene tres secciones. La primera abre archivos de texto en R que ser√°n luego empleados para la creaci√≥n de un todo coherente, relativamente comparable, que se someter√° al an√°lisis (corpus). La segunda realiza un OCR en archivos en formato PDF para, luego, extraer el texto. Finalmente, la tercera utiliza un conjunto de funciones para la limpieza y b√∫squeda sistem√°tica de texto, as√≠ como introduce nuestras nuevas mejores amigas: las expresiones regulares. Ya advierto, se trata de una relaci√≥n amor-odio, pues se trata de una t√©cnica de extracci√≥n de datos potent√≠sima, pero, a la vez, compleja.\nVamos paso a paso‚Ä¶",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#abrir-archivos-de-textos",
    "href": "preparacion.html#abrir-archivos-de-textos",
    "title": "Preparaci√≥n de los textos",
    "section": "Abrir archivos de textos",
    "text": "Abrir archivos de textos\nEl primer paso de cualquier an√°lisis de texto consiste en abrir los textos en el R para su posterior procesamiento y an√°lisis. Afortunadamente, existe una serie de opciones que facilitan mucho la apertura de una cantidad grande de textos de un solo golpe, sin la necesidad de ir de uno en uno.\nLa funci√≥n readtext() del paquete hom√≥nimo lee desde archivos de textos a PDFs, documentos de Word y otros formatos como planillas de Excel o json. No solo lee un archivo de cada vez, sino todav√≠a mejor. Basta con suministrar el camino hasta la carpeta y la funci√≥n trata de importar todos los archivos ah√≠ contenidos de un golpe.\nEn la secci√≥n sobre web scraping hemos bajado el texto completo de m√°s de 800 libros en espa√±ol disponibles en los servidores del Proyecto Gutenberg. Los hemos guardado todos en la carpeta ‚ÄúGut_txt/Archivos/‚Äù. Ahora podemos abrirlos de una vez en R utilizando la funci√≥n readtext(). Veamos c√≥mo se hace:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/txt\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Visualiza los textos\nreactable(gt, wrap=F, resizable = T)\n\n\n\n\n\n\n\nEl mismo procedimiento se puede llevar a cabo con archivos PDF que ya han sido sometidos a un OCR o desde un primer momento son digitales. Como vemos abajo, el c√≥digo es exactamente el mismo. Lo √∫nico que cambia es la direcci√≥n de la carpeta, que en esta ocasi√≥n contiene solamente archivos PDF:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/pdf\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Visualiza los textos\nreactable(gt, wrap=F, resizable = T)\n\n\n\n\n\n\n\nComo en el caso anterior, el R genera un data.frame con dos variables: doc_id, conteniendo el nombre del archivo, y text con el texto completo. Este nuevo objeto ser√° utilizado luego para la creaci√≥n de objeto de tipo corpus en la tercera parte de esta secci√≥n.",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#ocr-y-extracci√≥n-de-texto",
    "href": "preparacion.html#ocr-y-extracci√≥n-de-texto",
    "title": "Preparaci√≥n de los textos",
    "section": "OCR y extracci√≥n de texto",
    "text": "OCR y extracci√≥n de texto\nSin embargo, el mundo ser√≠a un lugar m√°s aburrido si las cosas siempre fueran tan sencillas. En muchos casos, nos encontraremos con archivos PDF escaneados con una resoluci√≥n baja y sin reconocimiento de caracteres. En estos casos, nos vemos forzados a procesar los archivos antes de poder llevar a cabo cualquier an√°lisis.\nEn R, el paquete tesseract permite realizar el reconocimiento √≥ptico de caracteres (OCR, en su acr√≥nimo original en ingl√©s) en m√∫ltiples archivos y en distintas lenguas. Combinado con el paquete pdftools, permiten extraer el texto desde fuentes dif√≠ciles de tratar.\nUtilizaremos los mismos PDFs para realizar el OCR y luego extraer los textos. El an√°lisis se dividir√° en dos partes. En la primera, generaremos una lista de los archivos a ser procesados y descargaremos el modelo de OCR para espa√±ol.\n\n\nCode\n# Carga los paquetes\nlibrary(tesseract)\nlibrary(pdftools)\n\n# Genera la lista de todos los PDFs\n# URL de la API de GitHub para listar archivos\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/pdf\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\nfl &lt;- files$download_url\n\n# Baja el modelo para realizar el \n# OCR en espaniol (solo una vez)\ntesseract_download(\"spa\")\n\n# Establece el espaniol como \n# lengua para el OCR\nesp &lt;- tesseract(\"spa\")\n\n\nEn la segunda parte, utilizaremos un bucle for para ir de archivo en archivo, realizar el OCR, extraer el texto y guardarlo en un nuevo formato (.txt) en una nueva carpeta.\n\n\nCode\n# Para cada PDF\nfor (i in 1:length(fl)){\n  \n  # Informa el avace\n  print(paste0(i, \" of \", length(fl)))\n  \n  # Extrae el nombre del archivo\n  ls &lt;- basename(fl[i])\n  \n  # Realiza el OCR\n  text &lt;- tesseract::ocr(\n    fl[i], \n    engine = esp)\n  \n  # Guarda el resultado en formato texto \n  write(text, paste0(\"CARPETA/\",ls,\".txt\"))\n  \n}\n\n\nAhora podemos averiguar los resultados obtenidos por medio de la funci√≥n readtext() utilizando el mismo c√≥digo que hemos visto antes:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/ocr_txt\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Visualiza los textox\nreactable(gt, wrap=F, resizable = T)",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#manipulaci√≥n-y-limpieza-de-textos",
    "href": "preparacion.html#manipulaci√≥n-y-limpieza-de-textos",
    "title": "Preparaci√≥n de los textos",
    "section": "Manipulaci√≥n y limpieza de textos",
    "text": "Manipulaci√≥n y limpieza de textos\nLa limpieza de los datos resulta fundamental para obtener un an√°lisis adecuado de los textos. Se trata de un proceso laborioso, pero muy importante para la obtenci√≥n de datos comparables. A√∫na un conjunto de tareas concretas de manipulaci√≥n que incluye: remover espacios en blanco, tildes, saltos de l√≠nea innecesarios o la extracci√≥n de datos o metadatos.\nLo que veremos aqu√≠ es un conjunto de t√©cnicas que se pueden adaptar a textos de distinta estructura y naturaleza. No existe una soluci√≥n universal de tratamiento de datos que funcione igual para tweets o para textos legales. En el caso de los primeros, habr√° que tratar los elementos no textuales o los ‚Äúemojis‚Äù antes de analizar el contenido. En los segundos, suele haber mucho ruido por la repetici√≥n de los encabezados de p√°gina por su publicaci√≥n en archivos PDF.\nAdem√°s, cada estructura nos brindar√° oportunidades distintas de extracci√≥n y an√°lisis de los datos. Por ejemplo, textos legales suelen ser muy estructurados y contienen la identificaci√≥n de actores, t√≠tulos, cap√≠tulos, etc. Podemos utilizar tales informaciones como ‚Äúmarcadores‚Äù o ‚Äúetiquetas‚Äù a la hora de extraer datos de forma sistem√°tica. Por esa raz√≥n, resulta muy √∫til empezar por la sencilla tarea de explorar y describir cu√°l es la estructura del texto. ¬øSe trata de un texto uniforme o segmentado (divisiones de cap√≠tulos, partes, t√≠tulos, art√≠culos o cualquier otra)? ¬øEl texto tiene un formato digital desde el principio o tenemos que tratar encabezados u otros elementos comunes en PDFs y documentos Word? ¬øEl texto abre con todas las letras legibles o aparecen s√≠mblos raros en las tildes? O sea, ¬øest√° en la codificaci√≥n de caracteres adecuada o tengo que abrirlo utilizando una codificaci√≥n espec√≠fica (‚ÄúLATIN1‚Äù es la m√°s com√∫n para los que trabajamos textos en espa√±ol)? La funci√≥n stri_enc_list() del paquete stringi proporciona un listado completo de las codificaciones.\nEn esta parte del laboratorio, veremos algunas t√©cnicas de manipulaci√≥n de textos que permiten prepararlos para el an√°lisis. Dividiremos el contenido en tres secciones. La primera examina las funciones de manipulaci√≥n de texto de R y de los paquetes stringr y stringi. La segunda introduce brevemente las expresiones regulares, que representan un recurso muy √∫til para la identificaci√≥n de patrones en textos. Finalmente, la tercera aplica el contenido de las dos anteriores en los textos que empleamos de ejemplo: los libros en espa√±ol del Proyecto Gutenmberg y los decretos presidenciales de Paraguay.\n\nCuenta, busca, extrae, divide, combina, sustituye, compara\nExiste un n√∫mero amplio de funciones en R para la manipulaci√≥n de texto. Podemos hacer casi cualquier operaci√≥n desde buscar expresiones concretas hasta combinar textos o transformarlos en otras estructuras. Aqu√≠ exploraremos algunas tareas b√°sicas muy √∫tiles para trabajar con textos en R.\n\nCuenta\nUna tarea de an√°lisis de texto consiste en contar las veces que determinados temas, contenidos o conceptos aparecen. Esto se puede hacer utilizando ciertas palabras o diccionarios que ayudan a definir el peso de un t√≥pico en el conjunto de elementos de un texto.\nPor ejemplo, ¬øcu√°ntas veces aparecen palabras que empiezan con ‚Äúdemo‚Äù en una variable? La funci√≥n stri_count() del paquete stringi retorna el n√∫mero de texto que un patr√≥n cualquiera (en nuestro caso ‚Äúdemo‚Äù) aparece en un texto o en una variable.\n\n\nCode\n# Crea una variable de texto\ntx &lt;- \"La democracia es la forma de gobierno originada a partir del demos, o pueblo.\"\n\n# Carga el paquete stringi\nlibrary(stringi)\n\n# Cuenta las palabras que contienen \"demo\"\nstri_count(tx, regex = \"demo\")\n\n\n[1] 2\n\n\nCode\n# Ahora con una variable\ntx &lt;- c(\"democracia\",\"demostenes democr√°tico\",\"nada\",\"demora\")\n\n# Cuenta las palabras que contienen \"demo\"\n# para cada elemento\nstri_count(tx, regex = \"demo\")\n\n\n[1] 1 2 0 1\n\n\nComo vemos, en el primer caso, el R nos ha retornado las dos veces en las que alguna palabra conteniendo ‚Äúdemo‚Äù aparec√≠a en la frase. En el segundo, dos elementos llaman la atenci√≥n. Primero, ya no es el total de veces en general, sino que el n√∫mero se divide por observaci√≥n de la variable. Segundo, debemos tener cuidado con la ra√≠z que utilizamos para evitar ambiguedades y generar falsos positivos. Por ejemplo, demostenes y demora no tienen ninguna relaci√≥n con democracia.\nOtra forma de contar que puede ser √∫til en algunos procesos de manipulaci√≥n de texto. Por ejemplo, los c√≥digos INE de los municipios de Espa√±a incluyen dos caracteres iniciales con el c√≥digo de la provincia y luego tres caracteres con el orden alfab√©tico del municipio. As√≠ que Almer√≠a tiene el c√≥digo ‚Äú04‚Äù y est√° en el 13¬∫ puesto en orden alfab√©tico. No obstante, muchas veces, ciertas agencias informan el c√≥digo como ‚Äú04013‚Äù mientras otras lo informan como ‚Äú4013‚Äù. Sin tratamiento, esto resulta un problema a la hora de comparar los datos.\nLa funci√≥n stri_length() del paquete stringi soluciona el problema al contar cu√°ntos caracteres hay en cada observaci√≥n de una variable de texto. A partir de ese dato, podemos identificar cu√°les elementos debemos tratar. En el ejemplo abajo a√±adimos un 0 al texto solo para aquellos c√≥digos que son menores de 5 caracteres. De ese modo, uniformizamos el sistema de acuerdo con el est√°ndar definido por el INE:\n\n\nCode\n# Crea una variable con los c√≥digos INE para los municipios de\n# Almer√≠a, Barcelona, Madrid, Salamanca y Zamora\ntx &lt;- c(\"4013\",\"8019\",\"28079\",\"37274\",\"49275\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Cuenta los caracteres\nstri_length(tx)\n\n\n[1] 4 4 5 5 5\n\n\nCode\n# Incluye un cero en el codigo del municipio\ntx[stri_length(tx)&lt;5] &lt;- paste0(\"0\", tx[stri_length(tx)&lt;5]) \n\n# Inspecciona los resultados\ntx\n\n\n[1] \"04013\" \"08019\" \"28079\" \"37274\" \"49275\"\n\n\n\n\nBusca\nEn otras ocasiones, lo que deseamos es saber cu√°les elementos del texto contienen ciertas ideas o palabras-clave que buscamos. En ese caso, se trata de identificar o si dichas expresiones se encuentran o no en el texto o, al reves, aquellos textos que contienen la palabra.\nPor ejemplo, ¬øcu√°les elementos de una variable contienen la palabra ministerio o ministro? La funci√≥n stri_detect() del paquete stringi lleva a cabo dicha tarea.\n\n\nCode\n# Crea una variable con distinto contenido\ntx &lt;- c(\"ministro de telecomunicaciones\",\n        \"secretaria adjunto de la presidencia\", \n        \"ministerio de agricultura\", \n        \"director de la polic√≠a nacional\",\n        \"ministerio de seguridad social\",\n        \"ministra de educaci√≥n\",\n        \"secretar√≠a nacional de derechos humanos\",\n        \"director√≠a de asuntos exteriores\")\n\n# Carga el paquete sgtringi\nlibrary(stringi)\n\n# Detecta cu√°les elementos contienen ministro o ministerio\nstri_detect(tx, regex = \"minist\")\n\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n\n\nCode\n# Podemos seleccionarlos si queremos\ntx[stri_detect(tx, regex = \"minist\")]\n\n\n[1] \"ministro de telecomunicaciones\" \"ministerio de agricultura\"     \n[3] \"ministerio de seguridad social\" \"ministra de educaci√≥n\"         \n\n\nComo se puede observar, el R retorna los elementos de la variable que contienen el patr√≥n ‚Äúminist‚Äù. En la primera forma es solamente una indicando TRUE o FALSE. En la segunda, hemos pedido que nos regrese el texto completo de cada observaci√≥n.\nEjercicio: Pod√©is ejercitar el nuevo conocimiento intentando buscar ‚Äúsecretario‚Äù o ‚Äúsecretar√≠a‚Äù y ‚Äúdirector‚Äù o ‚Äúdiretor√≠a‚Äù.\n\n\nExtrae\nEn otras ocasiones, queremos extraer los patrones para, por ejemplo, contar el n√∫mero de veces que ocurren. En el siguiente ejemplo, extraeremos del discurso de investidura de Pedro S√°nchez todas las palabras empezadas por igual (igualdad, igualitario, etc.) y por libert (libertad, libertades). Esto es posible gracias a la funci√≥n stri_extract_all() del paquete stringi.\n\n\nCode\n# Carga el paquete readtext\nlibrary(readtext)\n\n# Lee el discurso de investidura de Pedro S√°nchez de 2020\ntx &lt;- readtext(\"https://raw.githubusercontent.com/rodrodr/tenet_texts/refs/heads/main/spa.inaugural/15_XIV_Leg_Sanchez.txt\",encoding =\"LATIN1\")\n\n# Carga el paquete stringi\nlibrary(stringi)\n\n# Extrae las palabras con raiz igual\nfr &lt;- unlist(stri_extract_all(tx, regex = \"igual[a-z]+\"))\n\n# Cuenta la frecuencia\ntable(fr)\n\n\nfr\n  igualdad igualdades    iguales igualmente \n        26          2          2          1 \n\n\nCode\n# Extrae las palabras con raiz libert\nfr &lt;- unlist(stri_extract_all(tx, regex = \"libert[a-z]+\"))\n\n# Cuenta la frecuencia\ntable(fr)\n\n\nfr\n  libertad libertades \n        15          3 \n\n\nSe ve como hay una frecuencia mayor de palabras relacionadas a la igualdad que a la libertad, aunque estas √∫ltimas tambi√©n est√©n presentes en una proporci√≥n no muy inferior.\n\n\nDivide\nOtra tarea de manipulaci√≥n de textos consiste en dividirlos seg√∫n diferentes criterios que requieren cada an√°lisis. Por ejemplo, una tarea muy com√∫n consiste en fragmentar los textos en palabras, algo que se denomina tokenization. Hay dos funciones en el paquete stringi que nos permiten dividir un texto: stri_split(), que utiliza un patr√≥n para dividirlo y stri_split_fixed() que limita el n√∫mero de fragmentos. Veamos un ejemplo:\n\n\nCode\n# Crea una variable de texto\ntx &lt;- \"Pepi, Luci, Bom y otras chicas del mont√≥n.\"\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Separa utilizando la coma\nstri_split(tx, regex =\",\")\n\n\n[[1]]\n[1] \"Pepi\"                            \" Luci\"                          \n[3] \" Bom y otras chicas del mont√≥n.\"\n\n\nCode\n# Separa utilizando la coma, pero en solo dos fragmentos\nstri_split_fixed(str = tx, pattern = \", \", n = 2)\n\n\n[[1]]\n[1] \"Pepi\"                                \n[2] \"Luci, Bom y otras chicas del mont√≥n.\"\n\n\nCode\n# Un poco m√°s avanzado - separa utilizando tanto la coma\n# como la y\nstri_split(tx, regex =\"[,y]\")\n\n\n[[1]]\n[1] \"Pepi\"                      \" Luci\"                    \n[3] \" Bom \"                     \" otras chicas del mont√≥n.\"\n\n\n\n\nCombina\nEn algunas ocasiones, necesitamos combinar distintos textos para trabajar con t√©rminos compuestos, bigramas o cualquier otra finalidad. El c√≥digo abajo nos ense√±a c√≥mo hacerlo utilizando la funci√≥n stri_join() del paquete stringi.\n\n\nCode\n# Crea una variable de cantidades\nval &lt;- c(1, 2, 3, 4)\n\n# Crea una variable de texto\ntx &lt;- c(\"coche\", \"bicicletas\", \"hijos\", \"libros\")\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Combina los dos textos \nstri_join(val, tx, sep=\" \", collapse=\", \")\n\n\n[1] \"1 coche, 2 bicicletas, 3 hijos, 4 libros\"\n\n\n\n\nSustituye\nLa sustituci√≥n resulta muy √∫til para trabajar textos cuando se require el reemplazo de un valor por otro. Imaginemos que eres profesor y tienes una clase de 130 estudiantes. Como eres atento, les enviar√°s un informe con las notas por correo electr√≥nico. Ya tienes un archivo Excel con sus nombres y calificaciones. No obstante, resulta muy trabajoso escribir a cada uno copiando y pegando el mismo texto.\nLa funci√≥n stri_replace, del paquete que ya conoc√©is, permite reemplazar los datos como nombre y nota y facilitar el trabajo de redacci√≥n. Luego, se pueden utilizar otros paquetes como el gmailr para enviar los correos de forma automatizada (este √∫ltimo paso no lo haremos aqu√≠).\n\n\nCode\n# Crea una variable de texto\ntx &lt;- \"EstimadART NOMBRE,\\n\\nEspero que este correo le encuentre bien.\\n\\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\\nSu nota final ha sido NOTA.EMOJI\\n\\nReciba un cordial saludo,\\n\\nRodrigo\\n\\n\"\n\n# Pongamos unos emojis solo para divertirnos.\nemo &lt;- c(\"\\U1F937\",\"\\U1F64C\",\"\\U1F44D\",\"\\U1F947\")\n\n# Crea una lista de nombres\nnm &lt;- c(\"Pepe\", \"Manuel\",\"Mar√≠a\",\"Lola\")\n\n# Lista de notas\nnota &lt;- c(0, 5, 7.5, 8)\n\n# Art√≠culo definido\nart &lt;- c(\"o\",\"o\",\"a\",\"a\")\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Reemplaza el nombre\nst &lt;- stri_replace(tx, nm, regex =\"NOMBRE\")\n\n# Reemplaza el art√≠culo definido\nst &lt;- stri_replace(st, art, regex =\"ART\")\n\n# Reemplaza el emoji\nst &lt;- stri_replace(st, emo, regex =\"EMOJI\")\n\n# Ahora reemplaza la nota\nst &lt;- stri_replace(st, nota, regex =\"NOTA\")\n\n# Imprime los resultados\ncat(st)\n\n\nEstimado Pepe,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 0.ü§∑\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimado Manuel,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 5.üôå\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimada Mar√≠a,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 7.5.üëç\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimada Lola,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 8.ü•á\n\nReciba un cordial saludo,\n\nRodrigo\n\n\n\n\nCompara\nOtra tarea muy √∫til consiste en comparar textos y determinar su similitud. Imaginemos que comparamos direcciones, o nombres de personas o entidades en fuentes que pueden contener errores ortogr√°ficos o de digitaci√≥n. En esos casos, resulta fundamental poder medir el grado de similitud o diferencia para tomar una decisi√≥n sobre si se trata de la misma entidad o no.\nEl paquete stringdist posee diversas funciones orientadas a esta finalidad. Que permiten comparar desde dos textos entre s√≠ hasta m√∫ltiple textos entre ellos.\nIlustremos c√≥mo hacerlo utilizando dos textos literarios. En junio de 1580 muere en Lisboa el poeta Luis de Cam√µes. En septiembre de este mismo a√±o, Madrid asiste a la llegada al mundo de otro inmenso escritor, Francisco de Quevedo. Cualquier nativo o hablante fluyente de portugu√©s o espa√±ol no puede dejar de sorprenderse por la similitud entre dos sonetos de ambos autores sobre el amor. Incluso, en algunas estrofas, la redacci√≥n es id√©ntica.\nEl objetivo del c√≥digo abajo resulta comparar ambos sonetos, estrofa por estrofa, y determinar el grado de similitud entre ellas. Nos restringiremos aqu√≠ solamente a algoritmos de similitud que comparan palabras sin atenernos a su funci√≥n sint√°ctica o la carga sem√°ntica que conlleva. Por lo tanto, se trata de un an√°lisis sencillo de la estructura de las estrofas.\n\n\nCode\n# Soneto del amor (Luis de Cam√µes, 1598 - p√≥stumo)\ncam1598 &lt;- c(\"amor es fuego que arde sin verse\",\n            \"es herida que duele y no se siente\",\n            \"es un contentamiento descontento\",\n            \"es dolor que lastima sin doler\",\n            \"es un no querer mas que bien querer\",\n            \"es andar solitario entre la gente\",\n            \"es nunca contentarse de contento\",\n            \"es un cuidar que gana en perderse\",\n            \"es querer estar aprisionado por voluntad\",\n            \"es servir a quien vence, el vencedor\",\n            \"es tener con quien nos mata, lealtad\",\n            \"pero c√≥mo causar puede su favor\",\n            \"en los corazones humanos amistad\",\n            \"si tan contrario a si mismo es el amor\")\n  \n# Soneto del amor (Francisco de Quevedo, 1670 - p√≥stumo)  \nqev1670 &lt;- c(\"es yelo abrasador, es fuego helado\", \n            \"es herida que duele y no se siente\",\n            \"es un so√±ado bien, un mal presente\",\n            \"es un breve descanso muy cansado\",\n            \"es un descuido que nos da cuidado\",\n            \"un cobarde, con nombre de valiente\",\n            \"un andar solitario entre la gente\",\n            \"un amar solamente ser amado\",\n            \"es una libertad encarcelada\",\n            \"que dura hasta el postrero parasismo\",\n            \"enfermedad que crece si es curada\",\n            \"este es el nino amor, este es su abismo\",\n            \"mirad cual amistad tendra con nada\",\n            \"el que en todo es contrario de si mismo\")  \n\n\n# Carga los paquetes stringdist - para calcular la similitud \n# y reshape2 - para cambiar el formato de un data.frame\nlibrary(stringdist)\nlibrary(reshape2)\n\n# Calcula la matriz de similitud entre los dos textos\n# La matriz permitir√° identificar estrofas incluso si\n# se ha cambiado el orden.\nrd &lt;- round(stringsimmatrix(cam1598, \n                            qev1670, \n                            method = \"lcs\"),2)\n\n# Establece los nombres del las lineas como de Cam√µes\n# y el nombre de las columnas como de Quevedo\nrownames(rd) &lt;- cam1598\ncolnames(rd) &lt;- qev1670\n\n# Transforma la matriz en un data.frame\ndrd &lt;- melt(rd)\n\n# Da nombre a las variables\nnames(drd) &lt;- c(\"Camoes (1598)\",\"Quevedo (1670)\",\"Similitud\")\n\n# Selecciona solamente los resultados cuya \n# similitud resulta superior a 50%.\ndrd &lt;- drd[drd$Similitud&gt;0.5,]\n\n# Ordena las estrofas restantes de la m√°s\n# similar a la menos\ndrd &lt;- drd[order(drd$Similitud, decreasing = T),]\n\n# Inspecciona los resultados\nreactable(data = drd, resizable = T, striped = T)\n\n\n\n\n\n\n\nSe puede ver que, al menos cuatro estrofas de las 14 (28,6%) son muy similares. Resulta claro que esos dos textos presentan un fuerte parentesco e indican que Quevedo ha sido lector de Cam√µes. Este mismo m√©todo puede ser aplicado para cualquier otro tipo de texto. El aspecto crucial es la elecci√≥n de la unidad de comparaci√≥n b√°sica. En este ejemplo, la estrofa se emple√≥ como unidad de an√°lisis. En otras fuentes quiz√°s p√°rrafos o cuasi-frases sean las m√°s indicadas. Siempre hay que explorar diferentes posibilidades y m√©todos antes de aplicar un algoritmo a un n√∫mero amplio de casos.\n\n\n\nExpresiones regulares\n¬øQu√© es un texto? Parece broma, pero la pregunta no resulta trivial y saber responderla resulta fundamental para llevar a cabo an√°lisis de textos en cualquier programa inform√°tico. Desde el punto de vista de la inform√°tica, un texto es una secuencia de caracteres. Estos caracteres pueden ser letras, n√∫meros, s√≠mbolos o espacios en blanco. Cada car√°cter ocupa un lugar en la secuencia y el conjunto de caracteres forma palabras, frases y p√°rrafos.\nNo obstante, adem√°s de los caracteres que vemos, hay algunos que no son directamente visibles y que tenemos que conocer para poder manipular bien los textos. Existe una nomenclatura que debemos conocer para referino\n\nEspacios en blanco: Son caracteres que representan espacios entre palabras o l√≠neas. Pueden ser espacios simples, dobles o tabulaciones. Suelen representarse como \\s.\nSaltos de l√≠nea: Son caracteres que indican el final de una l√≠nea y el comienzo de otra. Se representan como \\n.\nD√≠gitos num√©ricos: Son caracteres que representan n√∫meros del 0 al 9. Si buscamos n√∫meros de 0 a 9, tenemos que escribir 0-9 en la expresi√≥n regular.\nLetras may√∫sculas y min√∫sculas: Son caracteres que representan letras del alfabeto en sus formas may√∫sculas y min√∫sculas. Las may√∫sculas se representan como A-Z y las min√∫sculas como a-z.\npuntuaci√≥n: Son caracteres que representan signos de puntuaci√≥n como comas, puntos, signos de interrogaci√≥n, etc. Se representan como [:punct:].\ntildes y acentos: Son caracteres que representan letras con tildes o acentos, como √°, √©, √≠, √≥, √∫, √±, etc. Se pueden representar como \\p{L} para letras en general o \\p{Lu} para may√∫sculas.\n\nPor lo tanto, las expresiones regulares son formas de sintaxis que permiten encontrar patrones en textos. Es como una gram√°tica de patrones textuales. Resultan tremendamente √∫tiles a la hora de eliminar espacios en blanco, remover puntuaci√≥n o acentos. Posibilitan, adem√°s, encontrar palabras o n√∫meros seg√∫n patrones concretos. Su uso nos facilita buscar informaci√≥n, eliminar secciones que no nos sirven y evitar errores.\nPor ejemplo, el c√≥digo abajo remueve los dobles espacios en blanco del texto:\n\n\nCode\n# Crea una variable con muchos espacios\ntx &lt;- \"Este    texto      tiene    muchos espacios en      blanco.\"\n\n# Sustituye los m√∫ltiples espacios por solo uno \ngsub(\"\\\\s+\",\" \", tx)\n\n\n[1] \"Este texto tiene muchos espacios en blanco.\"\n\n\nCode\n# Sustituye dos espacios por uno \ngsub(\"\\\\s{2}\",\" \", tx)\n\n\n[1] \"Este  texto   tiene  muchos espacios en   blanco.\"\n\n\nLa funci√≥n gsub() sirve para reemplazar textos en una variable. En el primer ejemplo, la expresi√≥n regular \\\\s+ indica al R que busque cualquier secuencia de texto en la que haya un espacio en blanco o m√°s y la reemplaza por solo un espacio. En la segunda, \\\\s{2} busca dos espacios y los sustituye por uno. Como vemos, los resultados son distintos porque hemos solicitado que R hiciera b√∫squedas diferentes.\nImaginemos que hay una variable de texto y necesitamos encontrar todos los n√∫meros contenidos en ella. La funci√≥n stri_extract_all_regex() del paquete stringi permite extraer informaci√≥n de una variable de texto. Si la combinamos con la expresi√≥n regular \\\\d+ (d√≠gitos num√©ricos), el resultado es un conjunto de n√∫meros.\n\n\nCode\n# Crea una variable textos conteniendo n√∫meros\ntx &lt;- c(\"Tengo 10 euros y debo 1000.\",\n        \"De los 18 equipos, sono 1 puede llegar a campe√≥n.\", \n        \"M√°s vale 8 que 80. Estoy 100% de acuerdo.\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Extrae los n√∫meros \nstri_extract_all_regex(tx, pattern = \"\\\\d+\", simplify = T)\n\n\n     [,1] [,2]   [,3] \n[1,] \"10\" \"1000\" \"\"   \n[2,] \"18\" \"1\"    \"\"   \n[3,] \"8\"  \"80\"   \"100\"\n\n\nEn el ejemplo abajo, se utiliza otra expresi√≥n regular [A-Z] (may√∫sculas), luego *[a-z]** (seguida de min√∫sculas) para encontrar y extraer las palabras iniciadas en may√∫sculas en el texto.\n\n\nCode\n# Carga el paquete\nlibrary(stringi)\n\n# Crea un texto de ejemplo\ntx &lt;- \"Aqui pondremos algunos Ministerios, la Presidencia y el presidente.\"\n\n## Extrae del texto expresiones con mayusculas\nstri_extract_all_regex(tx, pattern = \"[A-Z][a-z]*\")\n\n\n[[1]]\n[1] \"Aqui\"        \"Ministerios\" \"Presidencia\"\n\n\nTambi√©n se puede dividir un texto utilizando un caracter o palabra. La funci√≥n stri_split_regex() fragmenta una variable de texto a partir de un patr√≥n que puede ser un caracter, como un espacio, una palabra, o un s√≠mbolo, como el de salto de linea.\n\n\nCode\n# Carga el paquete\nlibrary(stringi)\n\n## Define el texto a ser dividido\ntx &lt;- c(\"Esta es la primera frase.\\nEsta es la segunda frase.\")\n\n## Divide utilizando salto de linea\nstri_split_regex(tx, \"\\n\")\n\n\n[[1]]\n[1] \"Esta es la primera frase.\" \"Esta es la segunda frase.\"\n\n\nCode\n## Divide utilizando espacio\nstri_split_regex(tx, \" \")\n\n\n[[1]]\n[1] \"Esta\"         \"es\"           \"la\"           \"primera\"      \"frase.\\nEsta\"\n[6] \"es\"           \"la\"           \"segunda\"      \"frase.\"      \n\n\nCode\n## Divide utilizando cualquier separador\nstri_split_regex(tx, \"\\\\s\")\n\n\n[[1]]\n [1] \"Esta\"    \"es\"      \"la\"      \"primera\" \"frase.\"  \"Esta\"    \"es\"     \n [8] \"la\"      \"segunda\" \"frase.\" \n\n\n¬øYa has intentado comparar los t√©rminos con o sin tildes? Acentuaci√≥n y puntuaci√≥n representan obst√°culos comunes para la comparaci√≥n de textos, especialmente cuando se aplican t√©cnicas como la de bag-of-words. En estos casos, aqu√≠, aqui y aqu√≠. son consideradas palabras distintas. Para ello, hace falta remover la puntuaci√≥n y los acentos para poder compararlas y encontrar su semejanza.\n\n\nCode\n# Carga el paquete\nlibrary(stringi)\n\n## Declara el texto\ntx &lt;- c(\"Jos√©, Mar√≠a y Elena quieren ir a la fiesta de ensue√±o. Pero, ¬øde qu√© fiesta hablas, Pepe?\")\n\n# Elimina la puntuacion\nstri_replace_all(tx, regex = \"[:punct:]\",\"\")\n\n\n[1] \"Jos√© Mar√≠a y Elena quieren ir a la fiesta de ensue√±o Pero de qu√© fiesta hablas Pepe\"\n\n\nCode\n# Elimina todos los acentos\nstri_trans_general(tx, id = \"Latin-ASCII\")\n\n\n[1] \"Jose, Maria y Elena quieren ir a la fiesta de ensueno. Pero, ?de que fiesta hablas, Pepe?\"\n\n\nEstos ejemplos constituyen una peque√±a introducci√≥n a las expresiones regulares. Hay un mundo de referencias a ser exploradas y hace falta tener siempre a mano un conjunto de chuletas para ayudarnos a buscar patrones de texto dependiendo del tipo de texto que estamos utilizando en cada momento.\n\n\nExpresiones regulares: nivel PRO\nYa hemos visto lo b√°sico de expresiones regulares. Parece f√°cil, ¬øverdad? üòé. Pues bien, las expresiones regulares pueden llegar a ser muy complejas y potentes. Permiten buscar patrones muy espec√≠ficos en textos y extraer informaci√≥n de forma sistem√°tica. A continuaci√≥n, veremos algunos ejemplos m√°s avanzados para que os hag√°is una idea de su potencial.\nEmpecemos por mirar un diario de sesiones: clicar aqu√≠. ¬øQu√© marcas pueden ser reconocidas para identificar las intervenciones? ¬øY los debates? ¬øLos n√∫meros de p√°gina? ¬øSon todas las marcas iguales o hay variaciones? ¬øCu√°les?\nPrimero, seleccionemos un par de identificaciones de intervenciones:\nLa se√±ora PRESIDENTA:\nEl se√±or PRESIDENTE DEL GOBIERNO (S√°nchez P√©rez-Castej√≥n):\nEl se√±or N√ö√ëEZ FEIJ√ìO:\nLa se√±ora VAQUERO MONTERO:\nEl se√±or REGO CANDAMIL:\nLa se√±ora MU√ëOZ DE LA IGLESIA:\nLa se√±ora VICEPRESIDENTA PRIMERA Y MINISTRA DE HACIENDA (Montero Cuadrado):\nLa se√±ora VICEPRESIDENTA TERCERA Y MINISTRA PARA LA TRANSICI√ìN ECOL√ìGICA Y EL RETO DEMOGR√ÅFICO (Aagesen Mu√±oz):\nObservamos un patr√≥n general: todos los apellidos se preceden de ‚ÄúEl se√±or‚Äù o ‚ÄúLa se√±ora‚Äù y son sucedidos por dos puntos :. Adem√°s, inician el p√°rrafo siempre. Tambi√©n observamos variaciones. En algunos casos, hay nombres o siglas entre par√©ntesis. Algunos tienen solo una descripci√≥n (PRESIDENTA) mientras que en otros casos √©stos se componen de dos o tres palabras. Cada forma requiere tratamiento distinto.\nNuestro reto aqu√≠ consiste en crear una expresi√≥n regular que permita identificar todas las intervenciones arriba. Para ello, la mejor herramienta es la p√°gina Regex101.com. M√°s que una p√°gina web, se trata de una aplicaci√≥n o mesa de trabajo virtual en la que podemos desarrollar expresiones regulares que se adecuen a nuestros textos. Abramos la p√°gina, copiemos y peguemos las identificaciones en el campo TEST STRING.\nEn el campo REGULAR EXPRESSION, escribamos la siguiente expresi√≥n regular:\n(^El se√±or|La se√±ora)\nExplico:\n\nlos par√©ntesis () indican el inicio y el fin de un grupo, algo muy √∫til para organizar las partes de la expresi√≥n regular.\nel s√≠mbolo ^ indica el inicio de una l√≠nea o p√°rrafo. El s√≠mbolo $ indica el fin de una l√≠nea o p√°rrafo.\nEl se√±or y La se√±ora son los textos que queremos encontrar. El espacio \\s indica que debe haber un espacio despu√©s de cada uno.\nPor otro lado, el s√≠mbolo | indica una alternativa. O sea, puede ser El se√±or o la Se√±ora.\nLuego, cada uno viene sucedido por \\s, que indica un espacio en blanco.\n\nAs√≠ que estoy diciendo al programa que identifique todas las expresiones que inician la frase con El se√±or o La se√±ora seguidos de un espacio. No hace falta mencionar que El se√±or es distinto de el se√±or o El Se√±or y que si uno intenta usar min√∫sculas cuando se trata de may√∫sculas las cosas no ir√°n bien, ¬øverdad?\nA continuaci√≥n, tenemos que identificar los apellidos:\nPara ello, debemos a√±adir una expresi√≥n regular que encuentre los apellidos. Hay variaciones m√∫ltiples: apellidos que empiezan con may√∫sculas y min√∫sculas, con tilde y sin tilde, seguidos de expresiones en par√©ntesis o no. Un l√≠o. ¬øC√≥mo podemos solucionar ese problema? Existen dos alternativas. La primera es intentar abarcar todas las variaciones en una expresi√≥n regular largu√≠sima. No obstante, una expresi√≥n regular de tal naturaleza resultar√≠a muy compleja y dif√≠cil de recordar. Adem√°s, siempre pueden aparecer otras formas. Por esa raz√≥n, una segunda alternativa consiste en abstraer y trascender al patr√≥n m√°s amplio. En este caso, todas las intervenciones empiezan igual (El se√±or o La se√±ora) y terminan igual (:):\n(^El se√±or\\s|La se√±ora\\s)(.+?)(:)\nLa clave est√° en el grupo del medio (.+?). El s√≠mbolo . indica cualquier caracter (letra, n√∫mero, espacio, s√≠mbolo, etc.), el s√≠mbolo + indica que puede haber uno o m√°s caracteres y ? dice al algoritmo que pare de buscar cuando encuentre el primer :. De ese modo, estamos diciendo al programa que cualquier cosa que est√© entre el inicio y el final de la intervenci√≥n debe ser considerada como v√°lida.\n\n\n\n\n\n\nImportantEjercicio\n\n\n\nAbajo encontrar√©is algunos t√≠tulos de secciones del mismo diario. Copia los t√≠tulos a regex101.com y desarrolla la expresi√≥n regular para capturar dichos t√≠tulos.\nAlgunas informaciones que pueden ayudar:\n\n\\n - salto de l√≠nea\n[0-9] - encuentra d√≠gitos num√©ricos\n[A-Z] - encuentra letras may√∫sculas\n\n- 1 -\\n\\n\\nIZAMIENTO DE LA BANDERA NACIONAL\\n\\n\n- 2 - \\n\\n\\nMOCIONES\\n\\n\n- 3 - \\n\\n\\nDECLARACI√ìN DE ZONA DE DESASTRE Y EMERGENCIA EN DISTINTOS DISTRITOS DE LA PROVINCIA DE BUENOS AIRES\\n\\n\n- 4 -\\n\\n\\nCONSIDERACI√ìN CONJUNTA DE ASUNTOS\\n\\n\n- 14 -\\n\\n\\nEMERGENCIA EN DISCAPACIDAD EN TODO EL TERRITORIO NACIONAL HASTA EL 31 DE DICIEMBRE DE 2027 INCLUSIVE\\n\\n\nLa respuesta est√° en el final del documento\n\n\n\n\nEjemplos de textos reales\nEn esta secci√≥n realizaremos la limpieza y extracci√≥n de datos de los textos empleados en los ejemplos anteriores de web scraping y de lectura de PDFs. Utilizaremos, primero, los libros en espa√±ol contenidos en la p√°gina del Proyecto Gutenberg y, segundo, los decretos presidenciales de Paraguay.\n\nLibros del Proyecto Gutenberg\nUna breve inspecci√≥n a los libros contenidos en el Proyecto Gutenberg revela que, tanto al principio como al final de cada archivo, se pueden encontrar textos descriptivos en ingl√©s relativos a la licencia o otros metatados. Antes de iniciar cualquier an√°lisis, por lo tanto, resultar√≠a muy √∫til remover estos pasajes de los originales para que solo contuvieran los textos en espa√±ol.\nEl primer paso consiste en abrir los archivos y buscar indicadores claros y sistem√°ticos que puedan servir para automatizar la limpieza de los archivos. Despu√©s de una r√°pida b√∫squeda, hemos podido identificar dos frases que marcan de forma expl√≠cita el inicio y el fin del texto en espa√±ol. Casi todos los libros empiezan con ‚Äú‚Äú*** START OF THIS PROJECT GUTENBERG EBOOK‚Äù y terminan con ‚ÄúEnd of the Project Gutenberg EBook‚Äù. Sin embargo, ni todos contienen tales textos. De ese modo, el algoritmo debe buscar esas dos frases para delimitar el texto que seguir√° y eliminar los pasajes en ingl√©s. Tambi√©n debe mantener todo el texto en el caso de que no se encuentre ninguna de las dos o solo la frase inicial o final.\nEl c√≥digo abajo lleva a cabo la tarea mencionada:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/tenet_texts/contents/Ficcion\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Carga el paquete\nlibrary(stringi)\n\n# Define el texto que identifica el inicio del texto en espa√±ol\nst &lt;- \"START OF TH\"\n\n# Define el texto que identifica el fin del texto en espa√±ol\ned &lt;- \"End of the Project Gutenberg EBook\"\n\n# Para cada texto\nfor(i in 1:nrow(gt)){\n\n  # Informa el texto\n  print(i)\n  \n  # Divide el texto en l√≠neas\n  tx &lt;- stri_split_lines(gt$text[i])\n  \n  # Convierte en un largo vector de texto\n  tx &lt;- unlist(tx)\n  \n  # Identifica la posicion INICIAL del texto en espa√±ol\n  nst &lt;- grep(st, tx, fixed = F, ignore.case = F)\n  \n  # Identifica la posici√≥n FINAL del texto en espa√±ol\n  ned &lt;- grep(ed, tx, fixed = F, ignore.case = T)\n  \n  # Si no encuentra la identificaci√≥n del inicio\n  if(length(nst)==0){\n    \n    # Define como inicio la l√≠nea 0\n    nst &lt;-0\n    }\n\n  # Si no encuentra la identificacion del final\n  if(length(ned)==0){\n    \n    # Define como final la √∫ltima l√≠nea m√°s 1\n    ned &lt;- length(tx)+1\n  }\n  \n  # Selecciona solo el texto deste la posici√≥n de inicio\n  # m√°s una l√≠nea y la de final menos una l√≠nea\n  tx &lt;- tx[(nst+1):(ned-1)]\n  \n  # Busca el titulo en mayusculas\n  n &lt;- which(stri_detect(tx, regex = \"[A-Z]{2,}\\\\s+[A-Z]{2,}\")==T)[1]\n  \n  # Si encuentra un titulo en mayusculas, lee desde la posicion\n  # del titulo encontrado\n  if(! is.na(n)) tx &lt;- tx[n:length(tx)]\n\n  # Remueve la divisi√≥n en l√≠neas\n  tx &lt;- paste(tx, collapse = \" \")\n  \n  # Reemplaza dos espacios por un salto de l√≠nea\n  # Reproduce los p√°rrafos originales\n  tx &lt;- gsub(\"\\\\s{2,}\",\"\\n\",tx, fixed = F)\n\n  # Actualiza el texto en la base de datos\n  gt$text[i] &lt;- tx\n\n}\n\n\n\n\nDecretos presidenciales de Paraguay\nEn el √∫ltimo ejemplo de esta parte, seleccionaremos algunos datos clave para clasificar los decretos presidenciales de Paraguay: el √≥rgano a que se refiere el decreto (en general un ministerio), la exposici√≥n de motivos o el resumen del mismo y el texto de sus art√≠culos.\nEl c√≥digo abajo aplica uno o m√°s estrategias introducidas anteriormente para llevar a cabo dicha tarea.\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/pdf\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Selecciona solo los PDFs\ngt &lt;- gt[grep(\".pdf\",gt$doc_id),]\n\n# Crea tres variables nuevas\ngt$organo &lt;- NA    # √ìrgano a que se refiere\ngt$motivo &lt;- NA    # Motivo expuesto\ngt$teor &lt;- NA      # Art√≠culos del decreto\n\n# Para cada decreto\nfor(i in 1:nrow(gt)){\n\n  # Divide los textos en l√≠neas\n  tx &lt;- stri_split_lines1(gt$text[i])\n\n  # Elimina los espacios en blanco\n  tx &lt;- sapply(tx, trimws, USE.NAMES = F)\n\n  # Elimina los m√∫ltiples espacios en blanco\n  tx &lt;- gsub(\"\\\\s{2,}\",\" \", tx)\n  \n  # Prepara las partes del texto para que\n  # se dividan seg√∫n un salto de l√≠nea\n  tx &lt;- gsub(\"VISTO\",\"  VISTO\", tx)\n  tx &lt;- gsub(\"Asunc\",\"  Asunc\", tx)\n  tx &lt;- gsub(\"DECRETA:\",\"  DECRETA: \", tx)\n  \n  # Elimina las l√≠neas\n  tx &lt;- paste(tx, collapse = \" \")\n  \n  # Reemplaza los dobles espacios por \n  # saltos de l√≠nea\n  tx &lt;- gsub(\"\\\\s{2,}\",\"\\n\",tx)\n  \n  # Vuelve a dividir, pero ahora en\n  # p√°rrafos consistentes\n  tx &lt;- stri_split_lines1(tx)\n  \n  # Averigua si contiene la expresi√≥n cexter\n  ce &lt;- grep(\"cexter\",tolower(tx), fixed = T)\n\n  # En caso positivo, la elimina\n  if(length(ce)&gt;0){\n    tx &lt;- tx[-ce]\n  }\n  \n  # Encuentra la posicion de los motivos\n  # en el texto\n  mo &lt;- tx[grep(\"POR EL CUAL\", tx)[1]]\n  \n  # Encuentra la informaci√≥n sobre el √≥rgano\n  pres &lt;- tolower(tx[grep(\"PRESIDEN\", tx)[1]])\n  \n  # Elimina la presidencia de la identificaci√≥n\n  # del √≥rgano o ministerio\n  pres &lt;- gsub(\"presidencia de la rep√∫blica del paraguay\",\"\", pres)\n  pres &lt;- trimws(pres)\n\n  # Encuentra el texto de los art√≠culos\n  teor &lt;- tx[(grep(\"DECRETA:\", tx)+1):length(tx)]\n  \n  # Lo convierte en un p√°rrafo\n  teor &lt;- paste(teor, collapse = \"\\n\")\n\n  # Si queda texto de ruido, lo elimina\n  teor &lt;- gsub(\"NA\\nDECRETA: \",\"\", teor, fixed = T)\n  \n  # Atribuye cada informaci√≥n a su respectiva\n  # variable en la base de datos\n  gt$organo[i] &lt;- pres\n  gt$motivo[i] &lt;- mo\n  gt$teor[i] &lt;- teor\n  \n}\n\n# Visualiza los resultados\nreactable(gt, wrap = F, resizable = T, defaultPageSize = 10)",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#di√°logos-a-bases-de-datos",
    "href": "preparacion.html#di√°logos-a-bases-de-datos",
    "title": "Preparaci√≥n de los textos",
    "section": "Di√°logos a bases de datos",
    "text": "Di√°logos a bases de datos\nEl √∫ltimo paso consiste en convertir un diario de sesiones en una base de datos de intervenciones. Para ello, necesitaremos emplear lo que hemos visto m√°s arriba para identificar los oradores y los temas y dividir el texto de acuerdo con la estructura del debate. Emplearemos aqu√≠ un diario de sesi√≥n de la actual legislatura del Congreso de Diputados de Espa√±a (legislatura XV).\nSe trata de un proceso que puede ser complejo y requerir varios pasos. A continuaci√≥n, se presenta un ejemplo de c√≥mo llevar a cabo esta tarea utilizando R para los diarios del Congreso de Diputados de Espa√±a. El primer paso consiste en preparar el ambiente: cargar paquetes y funciones. He creado una funci√≥n, llamada fixParagraph que corrige los saltos de linea en los p√°rrafos, algo muy com√∫n y molesto cuando trabajamos con PDFs. Con esto, podemos empezar:\n\n\nCode\nlibrary(readtext)\nlibrary(stringi)\nlibrary(dplyr)\nlibrary(reactable)\n\n# Crea la funci√≥n para arreglar los p√°rrafos\nfixParagraph &lt;- function(text){\n  \n  text &lt;- gsub(\"(?&lt;=\\\\w)\\\\n(?=\\\\w)\",\" \", text, perl = TRUE)\n  text &lt;- stringi::stri_replace_all_regex(text, \"\\\\n\",\"\\n\\n\")\n  text &lt;- stringi::stri_replace_all_regex(text, \"([A-Za-z\\\\p{L}\\\\p{Lu}\\\\)0-9])([%‚Äî‚Äï,;:]+)(\\n+)\", \"$1$2 \")\n  text &lt;- stringi::stri_replace_all_regex(text, \"([A-Za-z\\\\p{L}\\\\p{Lu}])(\\n+)([a-z\\\\p{L}\\\\p{Lu}])\", \"$1 $3\")\n  text &lt;- stringi::stri_replace_all_regex(text, \"([A-Za-z\\\\p{L}])(\\n+)([a-z\\\\p{L}‚Äï\\\\(])\", \"$1 $3\")\n    \n  return(text)\n  \n}\n\n\nEl siguiente paso es leer el pdf y preparar el texto para la posterior identificaci√≥n de di√°logos y reestructuraci√≥n como base de datos. En las l√≠neas de c√≥digo abajo llevamos a cabo las siguientes tareas:\n\nLeemos el PDF y extraemos el texto.\nConvertimos el texto en l√≠neas individuales.\nEliminamos espacios en blanco al inicio y al final de cada l√≠nea.\nEliminamos encabezados repetidos, n√∫meros de p√°gina y otros elementos no deseados.\nCorregimos los p√°rrafos para asegurar que est√©n bien formateados.\nRealizamos algunas correcciones espec√≠ficas en el texto.\nSeparamos los oradores del texto de su intervenci√≥n.\nEliminamos la informaci√≥n del final que no es relevante para el an√°lisis.\n\n\n\nCode\n# Define la ruta del archivo PDF\nfile &lt;- \"https://raw.githubusercontent.com/rodrodr/Burgos_2025/main/diarios/DSCD-15-PL-141.pdf\"\n\n# Carga los datos\ntt &lt;- readtext(file)\n\n# Extrae el texto\ntx &lt;- tt$text\n\n# Lo convierte en lineas\ntx &lt;- read_lines(tx)\n\n# Elimina espacios en blanco al inicio y al final\ntx &lt;- stri_trim(tx)\n\n# Elimina el encabezado repetido\n\n# Identificacion de los diarios\nnn &lt;- which(stri_detect_fixed(tx, \"DIARIO DE SESIONES DEL CONGRESO DE LOS DIPUTADOS\")==TRUE) \ntx &lt;- tx[-nn]\n\n# Pleno\nnn &lt;- which(stri_detect_fixed(tx, \"PLENO Y DIPUTACI√ìN PERMANENTE\")==TRUE) \ntx &lt;- tx[-nn]\n\n# Archivo\nnn &lt;- which(stri_detect_regex(tx, \"^cve:\\\\sDSCD-[0-9]{1,2}-PL-[0-9]{1,3}\")==TRUE) \ntx &lt;- tx[-nn]\n\n# N√∫mero de p√°ginas\nnn &lt;- which(stri_detect_regex(tx, \"^N√∫m.\\\\s[0-9]{1,3}\\\\s+\")==TRUE) \ntx &lt;- tx[-nn]\n\ntx &lt;- tx[tx!=\"\"]\n\n# Corrige los parrafos\ntx &lt;- paste(tx, collapse = \"\\n\")\ntx &lt;- fixParagraph(tx)\n\n# Elimina un error tipogr√°fico del diario\ntx &lt;- stri_replace_all_regex(tx, \"([A-Z\\\\.])(\\n+)(\\\\(N√∫mero de expediente)\", \"$1 $3\")\n\n# Arregla los titulos pegados a subtitulos\ntx &lt;- stri_replace_all_regex(tx, \"([A-Z\\\\)])(:)(\\\\s)(‚Äî\\\\s[A-Z])\", \"$1$2\\n$4\")\n\n# Lo convierte en lineas\ntx &lt;- read_lines(tx)\n\n# Separa los oradores del texto\ntx &lt;- stri_replace_all_regex(tx, \"(^El se√±or |^La se√±ora )([A-Z\\\\p{Lu}]{2,}?)(.+?)(:)(\\\\s)\", \"$1$2$3$4\\n\\n\")\n\n# Lo vuelve a juntar\ntx &lt;- paste(tx, collapse = \"\\n\")\n\n# Lo convierte en lineas\ntx &lt;- read_lines(tx)\n\n# Elimina la informaci√≥n del final\ntx &lt;- tx[1:which(\n            stri_detect_regex(tx, \n                              \"^Eran las|^Era la\")==TRUE)]\n\n\nEl resultado ya no es un texto completo, sino un conjunto de l√≠neas. Cada l√≠nea corresponde a una parte del texto. Un p√°rrafo, la identificaci√≥n de un t√≠tulo o un orador. Con esa informaci√≥n en mano, podemos identificar los t√≠tulos, subt√≠tulos y oradores en el texto. Para ello, utilizamos expresiones regulares espec√≠ficas que buscan patrones caracter√≠sticos de cada uno de estos elementos. Una vez identificados, almacenamos estos elementos junto con el texto en un data.frame para su posterior an√°lisis.\nLos pasos son los siguientes:\n\nIdentificamos los t√≠tulos\nIdentificamos los subt√≠tulos\nIdentificamos los oradores\nCreamos un data.frame con las identificaciones y el texto\nFiltramos las l√≠neas vac√≠as\n\n\n\nCode\n# Identifica los t√≠tulos\npat_tit &lt;- \"^(?!.*VOX|XX)^[A-Z\\\\p{LU}]{2,}(.+)(:|\\\\.)$\"\n\ntit &lt;- stri_extract_all_regex(tx, pat_tit, simplify = T)\ntit &lt;- as.character(tit)\n\n# Identifica los subtitulos  \npat_sub &lt;- \"(^[‚Äî‚Äï] [A-Z\\\\p{Lu}]+)(.+)(\\\\(N√∫mero de expediente [0-9]{3}\\\\/[0-9]{6}\\\\))(\\\\.$|$)\"  \n\nsub &lt;- stri_extract_all_regex(tx, pat_sub, simplify = T)\nsub &lt;- as.character(sub)\n\n# Identifica los oradores\n\npat_ora &lt;- \"(^El se√±or |^La se√±ora )([A-Z\\\\p{Lu}]{2,}?)(.+?)(:)\"\n\n\nora &lt;- stri_extract_all_regex(tx, pat_ora, simplify = T)\nora &lt;- as.character(ora)\n\n# Creamos un data.frame con toda la informaci√≥n\nd &lt;- data.frame(title=tit, subtitle=sub, speaker=ora, text=tx)\nd &lt;- d[d$text!=\"\",]\n\n# Visualizamos los resultados\nreactable(d,\n          wrap=F,\n          sortable = T, \n          resizable = T)\n\n\n\n\n\n\nAhora las cosas empiezan a ganar forma. Ya tenemos los textos, los t√≠tulos (secciones del debate), los subt√≠tulos (detalles de la secci√≥n del debate) y los oradores. No obstante, hay mucha informaci√≥n incompleta. Espacios vac√≠os que hay que rellenar.\nPara ello:\n\nA√±adimos informaci√≥n al inicio para indicar los datos del sumario\nRellenamos los espacios vac√≠os en las variables t√≠tulo, subt√≠tulo y orador\nEliminamos duplicidades\nTratamos los t√≠tulos sin subt√≠tulos\nCreamos una variable de orden de la intervenci√≥n\nAgregamos el texto para que a cada linea corresponda a una intervenci√≥n.\n\n\n\nCode\n# A√±ade informaci√≥n al inicio\nd$title[1] &lt;- \"Sumario\"\nd$subtitle[1] &lt;- \"Sumario\"\nd$speaker[1] &lt;- \"Sumario\"\n\n# Rellena los espacios vac√≠os entre\n# t√≠tulos y diputados\nd$title &lt;- zoo::na.locf(d$title, na.rm = F)\nd$subtitle &lt;- zoo::na.locf(d$subtitle, na.rm = F)\nd$speaker &lt;- zoo::na.locf(d$speaker, na.rm = F)\n\n# Eliminamos la identificacion de\n# titulos, subtitulos y oradores\n# del texto de las intervenciones\nd &lt;- d[d$title!=d$text,]\nd &lt;- d[d$subtitle!=d$text,]\nd &lt;- d[d$speaker!=d$text,]\n\n# T√≠tulos sin subtitulos\nfor(i in 2:nrow(d)){\n  \n  tm1 &lt;- d$title[i-1]\n  t0 &lt;- d$title[i]\n  \n  sm1 &lt;- d$subtitle[i-1]\n  s0 &lt;- d$subtitle[i]\n  \n  if(s0==sm1 & t0!=tm1){\n    \n    d$subtitle[i] &lt;- d$title[i]\n  }\n}\n\n# Crea una variable con el t√≠tulo, subtitulo y \n# el speaker (para las transiciones de temas)\nd$sub_speak &lt;- paste0(d$title,\"_\", d$subtitle, \"_\", d$speaker)\n\n# Crea una variable de orden\nd &lt;- d |&gt;\n  mutate(order = cumsum(\n                    sub_speak != lag(sub_speak, \n                    default = first(sub_speak))))\n\n\n# Borro la variable de trabajo\nd$sub_speak &lt;- NULL\n\n# Agrega el texto por t√≠tulo, subtitulo, \n# diputado y orden\nd &lt;- aggregate(list(text=d$text), \n               by=list(title=d$title, \n                       subtitle=d$subtitle, \n                       speaker=d$speaker, \n                       order=d$order), \n               FUN=paste, \n               collapse=\"\\n\") \n\n# A√±adimos como brindis:\n# \n# el n√∫mero de palabras\nd$nwords &lt;- stri_count_words(d$text)\n\n# Indica si es miembro de la mesa\nd$mesa &lt;- 0\nd$mesa[which(stri_detect_fixed(d$speaker, \"La se√±ora PRESIDENTA\")==TRUE)] &lt;- 1\nd$mesa[which(stri_detect_fixed(d$speaker, \"La se√±ora VICEPRESIDENTA (\")==TRUE)] &lt;- 1\nd$mesa[which(stri_detect_fixed(d$speaker, \"El se√±or SECRETARIO\")==TRUE)] &lt;- 1\nd$mesa[which(stri_detect_fixed(d$speaker, \"El se√±or VICEPRESIDENTE (\")==TRUE)] &lt;- 1\n\n# Visualiza los resultados\nreactable(d, \n          wrap = F, \n          resizable = T, \n          defaultPageSize = 10)\n\n\n\n\n\n\nComo hab√©is visto, ahora tenemos nuestra base de datos de intervenciones. Todav√≠a no podemos hacer an√°lisis por partido, puesto que tendr√≠amos que a√±adir informaci√≥n adicional (o seguir extray√©ndola del texto, algo que ser√≠a m√°s complicado). No obstante, ahora podemos a√±adir datos a partir de otras fuentes para enriquecer el an√°lisis.",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#respuestas",
    "href": "preparacion.html#respuestas",
    "title": "Preparaci√≥n de los textos",
    "section": "Respuestas",
    "text": "Respuestas\nRespuesta al ejercicio de expresi√≥n regular:\nLa expresi√≥n regular que captura los t√≠tulos es la siguiente:\n(^-\\s[0-9]+\\s-|^-\\s[0-9]+\\s-\\s)(\\n{3})([\\sA-Z\\p{Lu}0-9]+)(\\n{2})\n\nEl primer grupo (^-\\s[0-9]+\\s-|^-\\s[0-9]+\\s-\\s) identifica el inicio del t√≠tulo. El s√≠mbolo ^ indica el inicio de una l√≠nea, seguido de un guion -, un espacio \\s, uno o m√°s d√≠gitos num√©ricos [0-9]+, otro espacio \\s y otro guion -. La alternativa | permite capturar t√≠tulos que tienen un espacio adicional despu√©s del segundo guion.\nEl segundo grupo (\\n{3}) identifica los tres saltos de l√≠nea que preceden al t√≠tulo. Aqu√≠, \\n representa un salto de l√≠nea y el grupo captura tres saltos consecutivos.\nEl tercer grupo ([\\sA-Z\\p{Lu}0-9]+) captura el contenido del t√≠tulo. Aqu√≠, [\\sA-Z\\p{Lu}0-9]+ indica que el t√≠tulo puede contener espacios \\s, letras may√∫sculas [A-Z], letras may√∫sculas con tildes \\p{Lu} y d√≠gitos num√©ricos [0-9]. El s√≠mbolo + indica que puede haber uno o m√°s de estos caracteres.\nEl cuarto grupo (\\n{2}) identifica los dos saltos de l√≠nea que siguen al t√≠tulo. Aqu√≠, \\n representa un salto de l√≠nea y el grupo captura dos saltos consecutivos.]",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#referencias-adicionales",
    "href": "preparacion.html#referencias-adicionales",
    "title": "Preparaci√≥n de los textos",
    "section": "Referencias adicionales",
    "text": "Referencias adicionales\nExiste un enorme material disponible sobre expresiones regulares, os recomiendo los siguientes:\n\nWickam - ‚ÄúStrings‚Äù En R for Data Science\n‚ÄúRegular Expressions‚Äù en la documentaci√≥n del paquete stringr\n‚ÄúRegular Expressions‚Äù en el laboratorio LADAU\n\nTambi√©n vale la pena consultar las referencias de los dos paquetes m√°s importantes para la manipulaci√≥n de datos en R, el stringr y el stringi:\n\nstringi: Fast and Portable Character String Processing in R\nstringr",
    "crumbs": [
      "Preparaci√≥n"
    ]
  }
]