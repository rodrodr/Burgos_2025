[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "",
    "text": "Esta p√°gina web forma parte del curso ‚ÄúAn√°lisis de textos pol√≠ticos con R‚Äù. En ella, se recogen los materiales y ejemplos que se desarrollar√°n durante las sesiones.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#el-curso",
    "href": "index.html#el-curso",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "",
    "text": "Esta p√°gina web forma parte del curso ‚ÄúAn√°lisis de textos pol√≠ticos con R‚Äù. En ella, se recogen los materiales y ejemplos que se desarrollar√°n durante las sesiones.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#el-profesores",
    "href": "index.html#el-profesores",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "EL profesores",
    "text": "EL profesores\n\n\n\n\n\n\nRodrigo Rodrigues-Silveira\nrodrodr@usal.es\nProfesor de ciencia pol√≠tica de la USAL. Director del proyecto ‚ÄúComportamiento legislativo y erosi√≥n democr√°tica en Am√©rica Latina‚Äù (PELA Comportamiento). Miembro de los GIR ‚ÄúPol√≠tica Comparada en Am√©rica Latina‚Äù y ‚ÄúTecnolog√≠a y poder en el pensamiento y las letras‚Äù.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#el-contenido",
    "href": "index.html#el-contenido",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "El contenido",
    "text": "El contenido\nDurante el curso se abordar√°n los siguientes temas:\n\npreparaci√≥n de textos literarios para an√°lisis cuantitativos\nan√°lisis de frecuencias de palabras\ncodificaci√≥n tem√°tica\nan√°lisis de conglomerados clusters\nescalonado de textos (uni y multidimensional)\nmodelado de t√≥picos\nan√°lisis de redes sociales\nt√©cnicas de visualizaci√≥n de texto",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#sesiones",
    "href": "index.html#sesiones",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "Sesiones",
    "text": "Sesiones\nLas sesiones tendr√°n lugar en el et√©reo espacio de la nube, en la plataforma Zoom. Los enlaces de acceso a las sesiones se enviar√°n a los correos electr√≥nicos de los participantes.\nSesiones virtuales:\nD√çA 1 - 23/10/2025 de 9:30 a 13:30h - Preparaci√≥n de textos\nD√çA 2 - 27/10/2025 de 9:30 a 13:30h - An√°lisis b√°sicos\nD√çA 3 - 03/11/2025 de 9:30 a 13:30h - Codificaci√≥n tem√°tica\n\nSesiones presenciales:\nD√çA 4 - 10/11/2025 de 9:30 a 13:30h - Escalonado de texto\nD√çA 5 - 11/11/2025 de 9:30 a 13:30h - An√°lisis de redes",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "index.html#servicio-t√©cnico",
    "href": "index.html#servicio-t√©cnico",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "ü§ñ Servicio t√©cnico ü§ñ",
    "text": "ü§ñ Servicio t√©cnico ü§ñ\nPara que pod√°is reproducir los ejemplos de an√°lisis presentados durante el curso deb√©is instalar en vuestros ordenadores el R y el RStudio Desktop.\nTambi√©n deb√©is ejecutar el siguiente c√≥digo en R que instala los paquetes necesarios.\nPREPARATIVOS\nPara que funcione el c√≥digo abajo:\n\ndebes ejecutar cada l√≠nea de una en una y esperar que finalice antes de ejecutar la siguiente. Primero la que empieza con pc, luego, install.packages(pc) y, finalmente, una vez terminado el paso anterior: devtools::install_github(‚Äúrodrodr/tenet‚Äù, force=T).\nPuede que aparezcan algunos mensajes. El primero es si quieres reiniciar R, le das a ‚ÄúNo‚Äù. El segundo es si deseas actualizar los paquetes y debes decir: ‚Äú1 - All‚Äù. Finalmente, si te pide instalar paquetes ‚Äúfrom source‚Äù (en Mac puede aparecer), elegid, ‚Äún‚Äù (no).\n\nCon eso, tendr√©is para instalar los paquetes necesarios para el curso.\n\n\nCode\n# Crea un vector con los paquetes a instalar\npc &lt;- c(\"miniUI\",\"quanteda\",\"quanteda.textplots\",\"pdftools\",\n        \"quanteda.textmodels\",\"quanteda.textstats\",\n        \"stringi\",\"readr\",\"gutenbergr\",\"ggplot2\",\n        \"ggrepel\",\"reactable\",\"tidyverse\",\"devtools\",\n        \"egg\",\"network\",\"sna\",\"ggnetwork\",\"poliscidata\",\n        \"udpipe\",\"dplyr\",\"syuzhet\",\"ggiraph\",\"networkD3\",\n        \"igraph\",\"topicmodels\",\"wordcloud\",\"readtext\",\n        \"rvest\",\"tesseract\",\"stringdist\",\"htmltools\"\n        \"jsonlite\",\"gridExtra\",\"grid\",\"ngramr\",\n        \"DiagrammeR\")\n\n# Instala los paquetes\ninstall.packages(pc)\n\n# Instala el paquete tenet que no est√° en CRAN\ndevtools::install_github(\"rodrodr/tenet\", force=T)",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "preparacion.html",
    "href": "preparacion.html",
    "title": "Preparaci√≥n de los textos",
    "section": "",
    "text": "El primer paso en cualquier an√°lisis de texto consiste en la obtenci√≥n y preparaci√≥n de los datos. La obtenci√≥n se puede llevar a cabo de distintas maneras, incluyendo el web scraping o la descarga manual de archivos. En este apartado, exploraremos la lectura y preparaci√≥n de archivos ya descargados para su posterior an√°lisis. Por eso, una vez tengamos los archivos descargados y guardados en la nube o en una carpeta en el disco duro de nuestro ordenador, podemos ir un paso m√°s all√° para abrirlos en R, extraer informaci√≥n y limpiarlos si resulta necesario. Veremos c√≥mo abrir archivos con distintos formatos (txt, PDF, docx entre otros) y extraer texto de PDFs sin tratar y limpiarlos antes de pasar a la siguiente fase de organizaci√≥n de los corpora.\nEste peque√±o apartado tiene tres secciones. La primera abre archivos de texto en R que ser√°n luego empleados para la creaci√≥n de un todo coherente, relativamente comparable, que se someter√° al an√°lisis (corpus). La segunda realiza un OCR en archivos en formato PDF para, luego, extraer el texto. Finalmente, la tercera utiliza un conjunto de funciones para la limpieza y b√∫squeda sistem√°tica de texto, as√≠ como introduce nuestras nuevas mejores amigas: las expresiones regulares. Ya advierto, se trata de una relaci√≥n amor-odio, pues se trata de una t√©cnica de extracci√≥n de datos potent√≠sima, pero, a la vez, compleja.\nVamos paso a paso‚Ä¶",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#introducci√≥n",
    "href": "preparacion.html#introducci√≥n",
    "title": "Preparaci√≥n de los textos",
    "section": "",
    "text": "El primer paso en cualquier an√°lisis de texto consiste en la obtenci√≥n y preparaci√≥n de los datos. La obtenci√≥n se puede llevar a cabo de distintas maneras, incluyendo el web scraping o la descarga manual de archivos. En este apartado, exploraremos la lectura y preparaci√≥n de archivos ya descargados para su posterior an√°lisis. Por eso, una vez tengamos los archivos descargados y guardados en la nube o en una carpeta en el disco duro de nuestro ordenador, podemos ir un paso m√°s all√° para abrirlos en R, extraer informaci√≥n y limpiarlos si resulta necesario. Veremos c√≥mo abrir archivos con distintos formatos (txt, PDF, docx entre otros) y extraer texto de PDFs sin tratar y limpiarlos antes de pasar a la siguiente fase de organizaci√≥n de los corpora.\nEste peque√±o apartado tiene tres secciones. La primera abre archivos de texto en R que ser√°n luego empleados para la creaci√≥n de un todo coherente, relativamente comparable, que se someter√° al an√°lisis (corpus). La segunda realiza un OCR en archivos en formato PDF para, luego, extraer el texto. Finalmente, la tercera utiliza un conjunto de funciones para la limpieza y b√∫squeda sistem√°tica de texto, as√≠ como introduce nuestras nuevas mejores amigas: las expresiones regulares. Ya advierto, se trata de una relaci√≥n amor-odio, pues se trata de una t√©cnica de extracci√≥n de datos potent√≠sima, pero, a la vez, compleja.\nVamos paso a paso‚Ä¶",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#abrir-archivos-de-textos",
    "href": "preparacion.html#abrir-archivos-de-textos",
    "title": "Preparaci√≥n de los textos",
    "section": "Abrir archivos de textos",
    "text": "Abrir archivos de textos\nEl primer paso de cualquier an√°lisis de texto consiste en abrir los textos en el R para su posterior procesamiento y an√°lisis. Afortunadamente, existe una serie de opciones que facilitan mucho la apertura de una cantidad grande de textos de un solo golpe, sin la necesidad de ir de uno en uno.\nLa funci√≥n readtext() del paquete hom√≥nimo lee desde archivos de textos a PDFs, documentos de Word y otros formatos como planillas de Excel o json. No solo lee un archivo de cada vez, sino todav√≠a mejor. Basta con suministrar el camino hasta la carpeta y la funci√≥n trata de importar todos los archivos ah√≠ contenidos de un golpe.\nEn la secci√≥n sobre web scraping hemos bajado el texto completo de m√°s de 800 libros en espa√±ol disponibles en los servidores del Proyecto Gutenberg. He guardado algunos en una carpeta de Github. Ahora podemos abrirlos de una vez en R utilizando la funci√≥n readtext(). Veamos c√≥mo se hace:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/txt\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Visualiza los textos\nreactable(gt, wrap=F, resizable = T)\n\n\n\n\n\n\n\nEl mismo procedimiento se puede llevar a cabo con archivos PDF que ya han sido sometidos a un OCR o desde un primer momento son digitales. Como vemos abajo, el c√≥digo es exactamente el mismo. Lo √∫nico que cambia es la direcci√≥n de la carpeta, que en esta ocasi√≥n contiene solamente archivos PDF:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/pdf\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Visualiza los textos\nreactable(gt, wrap=F, resizable = T)\n\n\n\n\n\n\n\nComo en el caso anterior, el R genera un data.frame con dos variables: doc_id, conteniendo el nombre del archivo, y text con el texto completo. Este nuevo objeto ser√° utilizado luego para la creaci√≥n de objeto de tipo corpus en la tercera parte de esta secci√≥n.",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#ocr-y-extracci√≥n-de-texto",
    "href": "preparacion.html#ocr-y-extracci√≥n-de-texto",
    "title": "Preparaci√≥n de los textos",
    "section": "OCR y extracci√≥n de texto",
    "text": "OCR y extracci√≥n de texto\nSin embargo, el mundo ser√≠a un lugar m√°s aburrido si las cosas siempre fueran tan sencillas. En muchos casos, nos encontraremos con archivos PDF escaneados con una resoluci√≥n baja y sin reconocimiento de caracteres. En estos casos, nos vemos forzados a procesar los archivos antes de poder llevar a cabo cualquier an√°lisis.\nEn R, el paquete tesseract permite realizar el reconocimiento √≥ptico de caracteres (OCR, en su acr√≥nimo original en ingl√©s) en m√∫ltiples archivos y en distintas lenguas. Combinado con el paquete pdftools, permiten extraer el texto desde fuentes dif√≠ciles de tratar.\nUtilizaremos los mismos PDFs para realizar el OCR y luego extraer los textos. El an√°lisis se dividir√° en dos partes. En la primera, generaremos una lista de los archivos a ser procesados y descargaremos el modelo de OCR para espa√±ol.\n\n\nCode\n# Carga los paquetes\nlibrary(tesseract)\nlibrary(pdftools)\n\n# Genera la lista de todos los PDFs\n# URL de la API de GitHub para listar archivos\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/pdf\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\nfl &lt;- files$download_url\n\n# Baja el modelo para realizar el \n# OCR en espaniol (solo una vez)\ntesseract_download(\"spa\")\n\n# Establece el espaniol como \n# lengua para el OCR\nesp &lt;- tesseract(\"spa\")\n\n\nEn la segunda parte, utilizaremos un bucle for para ir de archivo en archivo, realizar el OCR, extraer el texto y guardarlo en un nuevo formato (.txt) en una nueva carpeta.\n\n\nCode\n# Para cada PDF\nfor (i in 1:length(fl)){\n  \n  # Informa el avace\n  print(paste0(i, \" of \", length(fl)))\n  \n  # Extrae el nombre del archivo\n  ls &lt;- basename(fl[i])\n  \n  # Realiza el OCR\n  text &lt;- tesseract::ocr(\n    fl[i], \n    engine = esp)\n  \n  # Guarda el resultado en formato texto \n  write(text, paste0(\"CARPETA/\",ls,\".txt\"))\n  \n}\n\n\nAhora podemos averiguar los resultados obtenidos por medio de la funci√≥n readtext() utilizando el mismo c√≥digo que hemos visto antes:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/ocr_txt\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Visualiza los textox\nreactable(gt, wrap=F, resizable = T)",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#manipulaci√≥n-y-limpieza-de-textos",
    "href": "preparacion.html#manipulaci√≥n-y-limpieza-de-textos",
    "title": "Preparaci√≥n de los textos",
    "section": "Manipulaci√≥n y limpieza de textos",
    "text": "Manipulaci√≥n y limpieza de textos\nLa limpieza de los datos resulta fundamental para obtener un an√°lisis adecuado de los textos. Se trata de un proceso laborioso, pero muy importante para la obtenci√≥n de datos comparables. A√∫na un conjunto de tareas concretas de manipulaci√≥n que incluye: remover espacios en blanco, tildes, saltos de l√≠nea innecesarios o la extracci√≥n de datos o metadatos.\nLo que veremos aqu√≠ es un conjunto de t√©cnicas que se pueden adaptar a textos de distinta estructura y naturaleza. No existe una soluci√≥n universal de tratamiento de datos que funcione igual para tweets o para textos legales. En el caso de los primeros, habr√° que tratar los elementos no textuales o los ‚Äúemojis‚Äù antes de analizar el contenido. En los segundos, suele haber mucho ruido por la repetici√≥n de los encabezados de p√°gina por su publicaci√≥n en archivos PDF.\nAdem√°s, cada estructura nos brindar√° oportunidades distintas de extracci√≥n y an√°lisis de los datos. Por ejemplo, textos legales suelen ser muy estructurados y contienen la identificaci√≥n de actores, t√≠tulos, cap√≠tulos, etc. Podemos utilizar tales informaciones como ‚Äúmarcadores‚Äù o ‚Äúetiquetas‚Äù a la hora de extraer datos de forma sistem√°tica. Por esa raz√≥n, resulta muy √∫til empezar por la sencilla tarea de explorar y describir cu√°l es la estructura del texto. ¬øSe trata de un texto uniforme o segmentado (divisiones de cap√≠tulos, partes, t√≠tulos, art√≠culos o cualquier otra)? ¬øEl texto tiene un formato digital desde el principio o tenemos que tratar encabezados u otros elementos comunes en PDFs y documentos Word? ¬øEl texto abre con todas las letras legibles o aparecen s√≠mblos raros en las tildes? O sea, ¬øest√° en la codificaci√≥n de caracteres adecuada o tengo que abrirlo utilizando una codificaci√≥n espec√≠fica (‚ÄúLATIN1‚Äù es la m√°s com√∫n para los que trabajamos textos en espa√±ol)? La funci√≥n stri_enc_list() del paquete stringi proporciona un listado completo de las codificaciones.\nEn esta parte del laboratorio, veremos algunas t√©cnicas de manipulaci√≥n de textos que permiten prepararlos para el an√°lisis. Dividiremos el contenido en tres secciones. La primera examina las funciones de manipulaci√≥n de texto de R y de los paquetes stringr y stringi. La segunda introduce brevemente las expresiones regulares, que representan un recurso muy √∫til para la identificaci√≥n de patrones en textos. Finalmente, la tercera aplica el contenido de las dos anteriores en los textos que empleamos de ejemplo: los libros en espa√±ol del Proyecto Gutenmberg y los decretos presidenciales de Paraguay.\n\nCuenta, busca, extrae, divide, combina, sustituye, compara\nExiste un n√∫mero amplio de funciones en R para la manipulaci√≥n de texto. Podemos hacer casi cualquier operaci√≥n desde buscar expresiones concretas hasta combinar textos o transformarlos en otras estructuras. Aqu√≠ exploraremos algunas tareas b√°sicas muy √∫tiles para trabajar con textos en R.\n\nCuenta\nUna tarea de an√°lisis de texto consiste en contar las veces que determinados temas, contenidos o conceptos aparecen. Esto se puede hacer utilizando ciertas palabras o diccionarios que ayudan a definir el peso de un t√≥pico en el conjunto de elementos de un texto.\nPor ejemplo, ¬øcu√°ntas veces aparecen palabras que empiezan con ‚Äúdemo‚Äù en una variable? La funci√≥n stri_count() del paquete stringi retorna el n√∫mero de texto que un patr√≥n cualquiera (en nuestro caso ‚Äúdemo‚Äù) aparece en un texto o en una variable.\n\n\nCode\n# Crea una variable de texto\ntx &lt;- \"La democracia es la forma de gobierno originada a partir del demos, o pueblo.\"\n\n# Carga el paquete stringi\nlibrary(stringi)\n\n# Cuenta las palabras que contienen \"demo\"\nstri_count(tx, regex = \"demo\")\n\n\n[1] 2\n\n\nCode\n# Ahora con una variable\ntx &lt;- c(\"democracia\",\"demostenes democr√°tico\",\"nada\",\"demora\")\n\n# Cuenta las palabras que contienen \"demo\"\n# para cada elemento\nstri_count(tx, regex = \"demo\")\n\n\n[1] 1 2 0 1\n\n\nComo vemos, en el primer caso, el R nos ha retornado las dos veces en las que alguna palabra conteniendo ‚Äúdemo‚Äù aparec√≠a en la frase. En el segundo, dos elementos llaman la atenci√≥n. Primero, ya no es el total de veces en general, sino que el n√∫mero se divide por observaci√≥n de la variable. Segundo, debemos tener cuidado con la ra√≠z que utilizamos para evitar ambiguedades y generar falsos positivos. Por ejemplo, demostenes y demora no tienen ninguna relaci√≥n con democracia.\nOtra forma de contar que puede ser √∫til en algunos procesos de manipulaci√≥n de texto. Por ejemplo, los c√≥digos INE de los municipios de Espa√±a incluyen dos caracteres iniciales con el c√≥digo de la provincia y luego tres caracteres con el orden alfab√©tico del municipio. As√≠ que Almer√≠a tiene el c√≥digo ‚Äú04‚Äù y est√° en el 13¬∫ puesto en orden alfab√©tico. No obstante, muchas veces, ciertas agencias informan el c√≥digo como ‚Äú04013‚Äù mientras otras lo informan como ‚Äú4013‚Äù. Sin tratamiento, esto resulta un problema a la hora de comparar los datos.\nLa funci√≥n stri_length() del paquete stringi soluciona el problema al contar cu√°ntos caracteres hay en cada observaci√≥n de una variable de texto. A partir de ese dato, podemos identificar cu√°les elementos debemos tratar. En el ejemplo abajo a√±adimos un 0 al texto solo para aquellos c√≥digos que son menores de 5 caracteres. De ese modo, uniformizamos el sistema de acuerdo con el est√°ndar definido por el INE:\n\n\nCode\n# Crea una variable con los c√≥digos INE para los municipios de\n# Almer√≠a, Barcelona, Madrid, Salamanca y Zamora\ntx &lt;- c(\"4013\",\"8019\",\"28079\",\"37274\",\"49275\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Cuenta los caracteres\nstri_length(tx)\n\n\n[1] 4 4 5 5 5\n\n\nCode\n# Incluye un cero en el codigo del municipio\ntx[stri_length(tx)&lt;5] &lt;- paste0(\"0\", tx[stri_length(tx)&lt;5]) \n\n# Inspecciona los resultados\ntx\n\n\n[1] \"04013\" \"08019\" \"28079\" \"37274\" \"49275\"\n\n\n\n\nBusca\nEn otras ocasiones, lo que deseamos es saber cu√°les elementos del texto contienen ciertas ideas o palabras-clave que buscamos. En ese caso, se trata de identificar o si dichas expresiones se encuentran o no en el texto o, al reves, aquellos textos que contienen la palabra.\nPor ejemplo, ¬øcu√°les elementos de una variable contienen la palabra ministerio o ministro? La funci√≥n stri_detect() del paquete stringi lleva a cabo dicha tarea.\n\n\nCode\n# Crea una variable con distinto contenido\ntx &lt;- c(\"ministro de telecomunicaciones\",\n        \"secretaria adjunto de la presidencia\", \n        \"ministerio de agricultura\", \n        \"director de la polic√≠a nacional\",\n        \"ministerio de seguridad social\",\n        \"ministra de educaci√≥n\",\n        \"secretar√≠a nacional de derechos humanos\",\n        \"director√≠a de asuntos exteriores\")\n\n# Carga el paquete sgtringi\nlibrary(stringi)\n\n# Detecta cu√°les elementos contienen ministro o ministerio\nstri_detect(tx, regex = \"minist\")\n\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n\n\nCode\n# Podemos seleccionarlos si queremos\ntx[stri_detect(tx, regex = \"minist\")]\n\n\n[1] \"ministro de telecomunicaciones\" \"ministerio de agricultura\"     \n[3] \"ministerio de seguridad social\" \"ministra de educaci√≥n\"         \n\n\nComo se puede observar, el R retorna los elementos de la variable que contienen el patr√≥n ‚Äúminist‚Äù. En la primera forma es solamente una indicando TRUE o FALSE. En la segunda, hemos pedido que nos regrese el texto completo de cada observaci√≥n.\nEjercicio: Pod√©is ejercitar el nuevo conocimiento intentando buscar ‚Äúsecretario‚Äù o ‚Äúsecretar√≠a‚Äù y ‚Äúdirector‚Äù o ‚Äúdiretor√≠a‚Äù.\n\n\nExtrae\nEn otras ocasiones, queremos extraer los patrones para, por ejemplo, contar el n√∫mero de veces que ocurren. En el siguiente ejemplo, extraeremos del discurso de investidura de Pedro S√°nchez todas las palabras empezadas por igual (igualdad, igualitario, etc.) y por libert (libertad, libertades). Esto es posible gracias a la funci√≥n stri_extract_all() del paquete stringi.\n\n\nCode\n# Carga el paquete readtext\nlibrary(readtext)\n\n# Lee el discurso de investidura de Pedro S√°nchez de 2020\ntx &lt;- readtext(\"https://raw.githubusercontent.com/rodrodr/tenet_texts/refs/heads/main/spa.inaugural/15_XIV_Leg_Sanchez.txt\",encoding =\"LATIN1\")\n\n# Carga el paquete stringi\nlibrary(stringi)\n\n# Extrae las palabras con raiz igual\nfr &lt;- unlist(stri_extract_all(tx, regex = \"igual[a-z]+\"))\n\n# Cuenta la frecuencia\ntable(fr)\n\n\nfr\n  igualdad igualdades    iguales igualmente \n        26          2          2          1 \n\n\nCode\n# Extrae las palabras con raiz libert\nfr &lt;- unlist(stri_extract_all(tx, regex = \"libert[a-z]+\"))\n\n# Cuenta la frecuencia\ntable(fr)\n\n\nfr\n  libertad libertades \n        15          3 \n\n\nSe ve como hay una frecuencia mayor de palabras relacionadas a la igualdad que a la libertad, aunque estas √∫ltimas tambi√©n est√©n presentes en una proporci√≥n no muy inferior.\n\n\nDivide\nOtra tarea de manipulaci√≥n de textos consiste en dividirlos seg√∫n diferentes criterios que requieren cada an√°lisis. Por ejemplo, una tarea muy com√∫n consiste en fragmentar los textos en palabras, algo que se denomina tokenization. Hay dos funciones en el paquete stringi que nos permiten dividir un texto: stri_split(), que utiliza un patr√≥n para dividirlo y stri_split_fixed() que limita el n√∫mero de fragmentos. Veamos un ejemplo:\n\n\nCode\n# Crea una variable de texto\ntx &lt;- \"Pepi, Luci, Bom y otras chicas del mont√≥n.\"\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Separa utilizando la coma\nstri_split(tx, regex =\",\")\n\n\n[[1]]\n[1] \"Pepi\"                            \" Luci\"                          \n[3] \" Bom y otras chicas del mont√≥n.\"\n\n\nCode\n# Separa utilizando la coma, pero en solo dos fragmentos\nstri_split_fixed(str = tx, pattern = \", \", n = 2)\n\n\n[[1]]\n[1] \"Pepi\"                                \n[2] \"Luci, Bom y otras chicas del mont√≥n.\"\n\n\nCode\n# Un poco m√°s avanzado - separa utilizando tanto la coma\n# como la y\nstri_split(tx, regex =\"[,y]\")\n\n\n[[1]]\n[1] \"Pepi\"                      \" Luci\"                    \n[3] \" Bom \"                     \" otras chicas del mont√≥n.\"\n\n\n\n\nCombina\nEn algunas ocasiones, necesitamos combinar distintos textos para trabajar con t√©rminos compuestos, bigramas o cualquier otra finalidad. El c√≥digo abajo nos ense√±a c√≥mo hacerlo utilizando la funci√≥n stri_join() del paquete stringi.\n\n\nCode\n# Crea una variable de cantidades\nval &lt;- c(1, 2, 3, 4)\n\n# Crea una variable de texto\ntx &lt;- c(\"coche\", \"bicicletas\", \"hijos\", \"libros\")\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Combina los dos textos \nstri_join(val, tx, sep=\" \", collapse=\", \")\n\n\n[1] \"1 coche, 2 bicicletas, 3 hijos, 4 libros\"\n\n\n\n\nSustituye\nLa sustituci√≥n resulta muy √∫til para trabajar textos cuando se require el reemplazo de un valor por otro. Imaginemos que eres profesor y tienes una clase de 130 estudiantes. Como eres atento, les enviar√°s un informe con las notas por correo electr√≥nico. Ya tienes un archivo Excel con sus nombres y calificaciones. No obstante, resulta muy trabajoso escribir a cada uno copiando y pegando el mismo texto.\nLa funci√≥n stri_replace, del paquete que ya conoc√©is, permite reemplazar los datos como nombre y nota y facilitar el trabajo de redacci√≥n. Luego, se pueden utilizar otros paquetes como el gmailr para enviar los correos de forma automatizada (este √∫ltimo paso no lo haremos aqu√≠).\n\n\nCode\n# Crea una variable de texto\ntx &lt;- \"EstimadART NOMBRE,\\n\\nEspero que este correo le encuentre bien.\\n\\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\\nSu nota final ha sido NOTA.EMOJI\\n\\nReciba un cordial saludo,\\n\\nRodrigo\\n\\n\"\n\n# Pongamos unos emojis solo para divertirnos.\nemo &lt;- c(\"\\U1F937\",\"\\U1F64C\",\"\\U1F44D\",\"\\U1F947\")\n\n# Crea una lista de nombres\nnm &lt;- c(\"Pepe\", \"Manuel\",\"Mar√≠a\",\"Lola\")\n\n# Lista de notas\nnota &lt;- c(0, 5, 7.5, 8)\n\n# Art√≠culo definido\nart &lt;- c(\"o\",\"o\",\"a\",\"a\")\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Reemplaza el nombre\nst &lt;- stri_replace(tx, nm, regex =\"NOMBRE\")\n\n# Reemplaza el art√≠culo definido\nst &lt;- stri_replace(st, art, regex =\"ART\")\n\n# Reemplaza el emoji\nst &lt;- stri_replace(st, emo, regex =\"EMOJI\")\n\n# Ahora reemplaza la nota\nst &lt;- stri_replace(st, nota, regex =\"NOTA\")\n\n# Imprime los resultados\ncat(st)\n\n\nEstimado Pepe,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 0.ü§∑\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimado Manuel,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 5.üôå\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimada Mar√≠a,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 7.5.üëç\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimada Lola,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, env√≠o la calificaci√≥n de la asignatura.\nSu nota final ha sido 8.ü•á\n\nReciba un cordial saludo,\n\nRodrigo\n\n\n\n\nCompara\nOtra tarea muy √∫til consiste en comparar textos y determinar su similitud. Imaginemos que comparamos direcciones, o nombres de personas o entidades en fuentes que pueden contener errores ortogr√°ficos o de tipeo. En esos casos, resulta fundamental poder medir el grado de similitud o diferencia para tomar una decisi√≥n sobre si se trata de la misma entidad o no.\nEl paquete stringdist posee diversas funciones orientadas a esta finalidad. Que permiten comparar desde dos textos entre s√≠ hasta m√∫ltiple textos entre ellos.\nIlustremos c√≥mo hacerlo utilizando dos textos literarios. En junio de 1580 muere en Lisboa el poeta Luis de Cam√µes. En septiembre de este mismo a√±o, Madrid asiste a la llegada al mundo de otro inmenso escritor, Francisco de Quevedo. Cualquier nativo o hablante fluyente de portugu√©s o espa√±ol no puede dejar de sorprenderse por la similitud entre dos sonetos de ambos autores sobre el amor. Incluso, en algunas estrofas, la redacci√≥n es id√©ntica.\nEl objetivo del c√≥digo abajo resulta comparar ambos sonetos, estrofa por estrofa, y determinar el grado de similitud entre ellas. Nos restringiremos aqu√≠ solamente a algoritmos de similitud que comparan palabras sin atenernos a su funci√≥n sint√°ctica o la carga sem√°ntica que conlleva. Por lo tanto, se trata de un an√°lisis sencillo de la estructura de las estrofas.\n\n\nCode\n# Soneto del amor (Luis de Cam√µes, 1598 - p√≥stumo)\ncam1598 &lt;- c(\"amor es fuego que arde sin verse\",\n            \"es herida que duele y no se siente\",\n            \"es un contentamiento descontento\",\n            \"es dolor que lastima sin doler\",\n            \"es un no querer mas que bien querer\",\n            \"es andar solitario entre la gente\",\n            \"es nunca contentarse de contento\",\n            \"es un cuidar que gana en perderse\",\n            \"es querer estar aprisionado por voluntad\",\n            \"es servir a quien vence, el vencedor\",\n            \"es tener con quien nos mata, lealtad\",\n            \"pero c√≥mo causar puede su favor\",\n            \"en los corazones humanos amistad\",\n            \"si tan contrario a si mismo es el amor\")\n  \n# Soneto del amor (Francisco de Quevedo, 1670 - p√≥stumo)  \nqev1670 &lt;- c(\"es yelo abrasador, es fuego helado\", \n            \"es herida que duele y no se siente\",\n            \"es un so√±ado bien, un mal presente\",\n            \"es un breve descanso muy cansado\",\n            \"es un descuido que nos da cuidado\",\n            \"un cobarde, con nombre de valiente\",\n            \"un andar solitario entre la gente\",\n            \"un amar solamente ser amado\",\n            \"es una libertad encarcelada\",\n            \"que dura hasta el postrero parasismo\",\n            \"enfermedad que crece si es curada\",\n            \"este es el nino amor, este es su abismo\",\n            \"mirad cual amistad tendra con nada\",\n            \"el que en todo es contrario de si mismo\")  \n\n\n# Carga los paquetes stringdist - para calcular la similitud \n# y reshape2 - para cambiar el formato de un data.frame\nlibrary(stringdist)\nlibrary(reshape2)\n\n# Calcula la matriz de similitud entre los dos textos\n# La matriz permitir√° identificar estrofas incluso si\n# se ha cambiado el orden.\nrd &lt;- round(stringsimmatrix(cam1598, \n                            qev1670, \n                            method = \"lcs\"),2)\n\n# Establece los nombres del las lineas como de Cam√µes\n# y el nombre de las columnas como de Quevedo\nrownames(rd) &lt;- cam1598\ncolnames(rd) &lt;- qev1670\n\n# Transforma la matriz en un data.frame\ndrd &lt;- melt(rd)\n\n# Da nombre a las variables\nnames(drd) &lt;- c(\"Camoes (1598)\",\"Quevedo (1670)\",\"Similitud\")\n\n# Selecciona solamente los resultados cuya \n# similitud resulta superior a 50%.\ndrd &lt;- drd[drd$Similitud&gt;0.5,]\n\n# Ordena las estrofas restantes de la m√°s\n# similar a la menos\ndrd &lt;- drd[order(drd$Similitud, decreasing = T),]\n\n# Inspecciona los resultados\nreactable(data = drd, resizable = T, striped = T)\n\n\n\n\n\n\n\nSe puede ver que, al menos cuatro estrofas de las 14 (28,6%) son muy similares. Resulta claro que esos dos textos presentan un fuerte parentesco e indican que Quevedo ha sido lector de Cam√µes. Este mismo m√©todo puede ser aplicado para cualquier otro tipo de texto. El aspecto crucial es la elecci√≥n de la unidad de comparaci√≥n b√°sica. En este ejemplo, la estrofa se emple√≥ como unidad de an√°lisis. En otras fuentes quiz√°s p√°rrafos o cuasi-frases sean las m√°s indicadas. Siempre hay que explorar diferentes posibilidades y m√©todos antes de aplicar un algoritmo a un n√∫mero amplio de casos.\n\n\n\nExpresiones regulares\n¬øQu√© es un texto? Parece broma, pero la pregunta no resulta trivial y saber responderla resulta fundamental para llevar a cabo an√°lisis de textos en cualquier programa inform√°tico. Desde el punto de vista de la inform√°tica, un texto es una secuencia de caracteres. Estos caracteres pueden ser letras, n√∫meros, s√≠mbolos o espacios en blanco. Cada car√°cter ocupa un lugar en la secuencia y el conjunto de caracteres forma palabras, frases y p√°rrafos.\nNo obstante, adem√°s de los caracteres que vemos, hay algunos que no son directamente visibles y que tenemos que conocer para poder manipular bien los textos. Existe una nomenclatura que debemos conocer para referino\n\nEspacios en blanco: Son caracteres que representan espacios entre palabras o l√≠neas. Pueden ser espacios simples, dobles o tabulaciones. Suelen representarse como \\s.\nSaltos de l√≠nea: Son caracteres que indican el final de una l√≠nea y el comienzo de otra. Se representan como \\n.\nD√≠gitos num√©ricos: Son caracteres que representan n√∫meros del 0 al 9. Si buscamos n√∫meros de 0 a 9, tenemos que escribir 0-9 en la expresi√≥n regular.\nLetras may√∫sculas y min√∫sculas: Son caracteres que representan letras del alfabeto en sus formas may√∫sculas y min√∫sculas. Las may√∫sculas se representan como A-Z y las min√∫sculas como a-z.\npuntuaci√≥n: Son caracteres que representan signos de puntuaci√≥n como comas, puntos, signos de interrogaci√≥n, etc. Se representan como [:punct:].\ntildes y acentos: Son caracteres que representan letras con tildes o acentos, como √°, √©, √≠, √≥, √∫, √±, etc. Se pueden representar como \\p{L} para letras en general o \\p{Lu} para may√∫sculas.\n\nPor lo tanto, las expresiones regulares son formas de sintaxis que permiten encontrar patrones en textos. Es como una gram√°tica de patrones textuales. Resultan tremendamente √∫tiles a la hora de eliminar espacios en blanco, remover puntuaci√≥n o acentos. Posibilitan, adem√°s, encontrar palabras o n√∫meros seg√∫n patrones concretos. Su uso nos facilita buscar informaci√≥n, eliminar secciones que no nos sirven y evitar errores.\nPor ejemplo, el c√≥digo abajo remueve los dobles espacios en blanco del texto:\n\n\nCode\n# Crea una variable con muchos espacios\ntx &lt;- \"Este    texto      tiene    muchos espacios en      blanco.\"\n\n# Sustituye los m√∫ltiples espacios por solo uno \ngsub(\"\\\\s+\",\" \", tx)\n\n\n[1] \"Este texto tiene muchos espacios en blanco.\"\n\n\nCode\n# Sustituye dos espacios por uno \ngsub(\"\\\\s{2}\",\" \", tx)\n\n\n[1] \"Este  texto   tiene  muchos espacios en   blanco.\"\n\n\nLa funci√≥n gsub() sirve para reemplazar textos en una variable. En el primer ejemplo, la expresi√≥n regular \\\\s+ indica al R que busque cualquier secuencia de texto en la que haya un espacio en blanco o m√°s y la reemplaza por solo un espacio. En la segunda, \\\\s{2} busca dos espacios y los sustituye por uno. Como vemos, los resultados son distintos porque hemos solicitado que R hiciera b√∫squedas diferentes.\nImaginemos que hay una variable de texto y necesitamos encontrar todos los n√∫meros contenidos en ella. La funci√≥n stri_extract_all_regex() del paquete stringi permite extraer informaci√≥n de una variable de texto. Si la combinamos con la expresi√≥n regular \\\\d+ (d√≠gitos num√©ricos), el resultado es un conjunto de n√∫meros.\n\n\nCode\n# Crea una variable textos conteniendo n√∫meros\ntx &lt;- c(\"Tengo 10 euros y debo 1000.\",\n        \"De los 18 equipos, sono 1 puede llegar a campe√≥n.\", \n        \"M√°s vale 8 que 80. Estoy 100% de acuerdo.\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Extrae los n√∫meros \nstri_extract_all_regex(tx, pattern = \"\\\\d+\", simplify = T)\n\n\n     [,1] [,2]   [,3] \n[1,] \"10\" \"1000\" \"\"   \n[2,] \"18\" \"1\"    \"\"   \n[3,] \"8\"  \"80\"   \"100\"\n\n\nEn el ejemplo abajo, se utiliza otra expresi√≥n regular [A-Z] (may√∫sculas), luego *[a-z]** (seguida de min√∫sculas) para encontrar y extraer las palabras iniciadas en may√∫sculas en el texto.\n\n\nCode\n# Carga el paquete\nlibrary(stringi)\n\n# Crea un texto de ejemplo\ntx &lt;- \"Aqui pondremos algunos Ministerios, la Presidencia y el presidente.\"\n\n## Extrae del texto expresiones con mayusculas\nstri_extract_all_regex(tx, pattern = \"[A-Z][a-z]*\")\n\n\n[[1]]\n[1] \"Aqui\"        \"Ministerios\" \"Presidencia\"\n\n\nTambi√©n se puede dividir un texto utilizando un caracter o palabra. La funci√≥n stri_split_regex() fragmenta una variable de texto a partir de un patr√≥n que puede ser un caracter, como un espacio, una palabra, o un s√≠mbolo, como el de salto de linea.\n\n\nCode\n# Carga el paquete\nlibrary(stringi)\n\n## Define el texto a ser dividido\ntx &lt;- c(\"Esta es la primera frase.\\nEsta es la segunda frase.\")\n\n## Divide utilizando salto de linea\nstri_split_regex(tx, \"\\n\")\n\n\n[[1]]\n[1] \"Esta es la primera frase.\" \"Esta es la segunda frase.\"\n\n\nCode\n## Divide utilizando espacio\nstri_split_regex(tx, \" \")\n\n\n[[1]]\n[1] \"Esta\"         \"es\"           \"la\"           \"primera\"      \"frase.\\nEsta\"\n[6] \"es\"           \"la\"           \"segunda\"      \"frase.\"      \n\n\nCode\n## Divide utilizando cualquier separador\nstri_split_regex(tx, \"\\\\s\")\n\n\n[[1]]\n [1] \"Esta\"    \"es\"      \"la\"      \"primera\" \"frase.\"  \"Esta\"    \"es\"     \n [8] \"la\"      \"segunda\" \"frase.\" \n\n\n¬øYa has intentado comparar los t√©rminos con o sin tildes? Acentuaci√≥n y puntuaci√≥n representan obst√°culos comunes para la comparaci√≥n de textos, especialmente cuando se aplican t√©cnicas como la de bag-of-words. En estos casos, aqu√≠, aqui y aqu√≠. son consideradas palabras distintas. Para ello, hace falta remover la puntuaci√≥n y los acentos para poder compararlas y encontrar su semejanza.\n\n\nCode\n# Carga el paquete\nlibrary(stringi)\n\n## Declara el texto\ntx &lt;- c(\"Jos√©, Mar√≠a y Elena quieren ir a la fiesta de ensue√±o. Pero, ¬øde qu√© fiesta hablas, Pepe?\")\n\n# Elimina la puntuacion\nstri_replace_all(tx, regex = \"[:punct:]\",\"\")\n\n\n[1] \"Jos√© Mar√≠a y Elena quieren ir a la fiesta de ensue√±o Pero de qu√© fiesta hablas Pepe\"\n\n\nCode\n# Elimina todos los acentos\nstri_trans_general(tx, id = \"Latin-ASCII\")\n\n\n[1] \"Jose, Maria y Elena quieren ir a la fiesta de ensueno. Pero, ?de que fiesta hablas, Pepe?\"\n\n\nEstos ejemplos constituyen una peque√±a introducci√≥n a las expresiones regulares. Hay un mundo de referencias a ser exploradas y hace falta tener siempre a mano un conjunto de chuletas para ayudarnos a buscar patrones de texto dependiendo del tipo de texto que estamos utilizando en cada momento.\n\n\nExpresiones regulares: nivel PRO\nYa hemos visto lo b√°sico de expresiones regulares. Parece f√°cil, ¬øverdad? üòé. Pues bien, las expresiones regulares pueden llegar a ser muy complejas y potentes. Permiten buscar patrones muy espec√≠ficos en textos y extraer informaci√≥n de forma sistem√°tica. A continuaci√≥n, veremos algunos ejemplos m√°s avanzados para que os hag√°is una idea de su potencial.\nEmpecemos por mirar un diario de sesiones: clicar aqu√≠. ¬øQu√© marcas pueden ser reconocidas para identificar las intervenciones? ¬øY los debates? ¬øLos n√∫meros de p√°gina? ¬øSon todas las marcas iguales o hay variaciones? ¬øCu√°les?\nPrimero, seleccionemos un par de identificaciones de intervenciones:\nLa se√±ora PRESIDENTA:\nEl se√±or PRESIDENTE DEL GOBIERNO (S√°nchez P√©rez-Castej√≥n):\nEl se√±or N√ö√ëEZ FEIJ√ìO:\nLa se√±ora VAQUERO MONTERO:\nEl se√±or REGO CANDAMIL:\nLa se√±ora MU√ëOZ DE LA IGLESIA:\nLa se√±ora VICEPRESIDENTA PRIMERA Y MINISTRA DE HACIENDA (Montero Cuadrado):\nLa se√±ora VICEPRESIDENTA TERCERA Y MINISTRA PARA LA TRANSICI√ìN ECOL√ìGICA Y EL RETO DEMOGR√ÅFICO (Aagesen Mu√±oz):\nObservamos un patr√≥n general: todos los apellidos se preceden de ‚ÄúEl se√±or‚Äù o ‚ÄúLa se√±ora‚Äù y son sucedidos por dos puntos :. Adem√°s, inician el p√°rrafo siempre. Tambi√©n observamos variaciones. En algunos casos, hay nombres o siglas entre par√©ntesis. Algunos tienen solo una descripci√≥n (PRESIDENTA) mientras que en otros casos √©stos se componen de dos, tres o incluso cuatro palabras. Cada forma requiere tratamiento distinto.\nNuestro reto aqu√≠ consiste en crear una expresi√≥n regular que permita identificar todas las intervenciones arriba. Para ello, la mejor herramienta es la p√°gina Regex101.com. M√°s que una p√°gina web, se trata de una aplicaci√≥n o mesa de trabajo virtual en la que podemos desarrollar expresiones regulares que se adecuen a nuestros textos. Abramos la p√°gina, copiemos y peguemos las identificaciones en el campo TEST STRING.\nEn el campo REGULAR EXPRESSION, escribamos la siguiente expresi√≥n regular:\n(^El se√±or|La se√±ora)\nExplico:\n\nlos par√©ntesis () indican el inicio y el fin de un grupo, algo muy √∫til para organizar las partes de la expresi√≥n regular.\nel s√≠mbolo ^ indica el inicio de una l√≠nea o p√°rrafo. El s√≠mbolo $ indica el fin de una l√≠nea o p√°rrafo.\nEl se√±or y La se√±ora son los textos que queremos encontrar. El espacio \\s indica que debe haber un espacio despu√©s de cada uno.\nPor otro lado, el s√≠mbolo | indica una alternativa. O sea, puede ser El se√±or o la Se√±ora.\nLuego, cada uno viene sucedido por \\s, que indica un espacio en blanco.\n\nAs√≠ que estoy diciendo al programa que identifique todas las expresiones que inician la frase con El se√±or o La se√±ora seguidos de un espacio. No hace falta mencionar que El se√±or es distinto de el se√±or o El Se√±or y que si uno intenta usar min√∫sculas cuando se trata de may√∫sculas las cosas no ir√°n bien, ¬øverdad?\nA continuaci√≥n, tenemos que identificar los apellidos:\nPara ello, debemos a√±adir una expresi√≥n regular que encuentre los apellidos. Hay variaciones m√∫ltiples: apellidos que empiezan con may√∫sculas y min√∫sculas, con tilde y sin tilde, seguidos de expresiones en par√©ntesis o no. Un l√≠o. ¬øC√≥mo podemos solucionar ese problema? Existen dos alternativas. La primera es intentar abarcar todas las variaciones en una expresi√≥n regular largu√≠sima. No obstante, una expresi√≥n regular de tal naturaleza resultar√≠a muy compleja y dif√≠cil de recordar. Adem√°s, siempre pueden aparecer otras formas. Por esa raz√≥n, una segunda alternativa consiste en abstraer y trascender al patr√≥n m√°s amplio. En este caso, todas las intervenciones empiezan igual (El se√±or o La se√±ora) y terminan igual (:):\n(^El se√±or\\s|La se√±ora\\s)(.+?)(:)\nLa clave est√° en el grupo del medio (.+?). El s√≠mbolo . indica cualquier caracter (letra, n√∫mero, espacio, s√≠mbolo, etc.), el s√≠mbolo + indica que puede haber uno o m√°s caracteres y ? dice al algoritmo que pare de buscar cuando encuentre el primer :. De ese modo, estamos diciendo al programa que cualquier cosa que est√© entre el inicio y el final de la intervenci√≥n debe ser considerada como v√°lida.\n\n\n\n\n\n\nImportantEjercicio\n\n\n\nAbajo encontrar√©is algunos t√≠tulos de secciones del mismo diario. Copia los t√≠tulos a regex101.com y desarrolla la expresi√≥n regular para capturar dichos t√≠tulos.\nAlgunas informaciones que pueden ayudar:\n\n\\n - salto de l√≠nea\n[0-9] - encuentra d√≠gitos num√©ricos\n[A-Z] - encuentra letras may√∫sculas\n\n- 1 -\\n\\n\\nIZAMIENTO DE LA BANDERA NACIONAL\\n\\n\n- 2 - \\n\\n\\nMOCIONES\\n\\n\n- 3 - \\n\\n\\nDECLARACI√ìN DE ZONA DE DESASTRE Y EMERGENCIA EN DISTINTOS DISTRITOS DE LA PROVINCIA DE BUENOS AIRES\\n\\n\n- 4 -\\n\\n\\nCONSIDERACI√ìN CONJUNTA DE ASUNTOS\\n\\n\n- 14 -\\n\\n\\nEMERGENCIA EN DISCAPACIDAD EN TODO EL TERRITORIO NACIONAL HASTA EL 31 DE DICIEMBRE DE 2027 INCLUSIVE\\n\\n\nLa respuesta est√° en el final del documento\n\n\n\n\nEjemplos de textos reales\nEn esta secci√≥n realizaremos la limpieza y extracci√≥n de datos de los textos empleados en los ejemplos anteriores de web scraping y de lectura de PDFs. Utilizaremos, primero, los libros en espa√±ol contenidos en la p√°gina del Proyecto Gutenberg y, segundo, los decretos presidenciales de Paraguay.\n\nLibros del Proyecto Gutenberg\nUna breve inspecci√≥n a los libros contenidos en el Proyecto Gutenberg revela que, tanto al principio como al final de cada archivo, se pueden encontrar textos descriptivos en ingl√©s relativos a la licencia o otros metatados. Antes de iniciar cualquier an√°lisis, por lo tanto, resultar√≠a muy √∫til remover estos pasajes de los originales para que solo contuvieran los textos en espa√±ol.\nEl primer paso consiste en abrir los archivos y buscar indicadores claros y sistem√°ticos que puedan servir para automatizar la limpieza de los archivos. Despu√©s de una r√°pida b√∫squeda, hemos podido identificar dos frases que marcan de forma expl√≠cita el inicio y el fin del texto en espa√±ol. Casi todos los libros empiezan con ‚Äú‚Äú*** START OF THIS PROJECT GUTENBERG EBOOK‚Äù y terminan con ‚ÄúEnd of the Project Gutenberg EBook‚Äù. Sin embargo, ni todos contienen tales textos. De ese modo, el algoritmo debe buscar esas dos frases para delimitar el texto que seguir√° y eliminar los pasajes en ingl√©s. Tambi√©n debe mantener todo el texto en el caso de que no se encuentre ninguna de las dos o solo la frase inicial o final.\nEl c√≥digo abajo lleva a cabo la tarea mencionada:\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/tenet_texts/contents/Ficcion\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Carga el paquete\nlibrary(stringi)\n\n# Define el texto que identifica el inicio del texto en espa√±ol\nst &lt;- \"START OF TH\"\n\n# Define el texto que identifica el fin del texto en espa√±ol\ned &lt;- \"End of the Project Gutenberg EBook\"\n\n# Para cada texto\nfor(i in 1:nrow(gt)){\n\n  # Informa el texto\n  print(i)\n  \n  # Divide el texto en l√≠neas\n  tx &lt;- stri_split_lines(gt$text[i])\n  \n  # Convierte en un largo vector de texto\n  tx &lt;- unlist(tx)\n  \n  # Identifica la posicion INICIAL del texto en espa√±ol\n  nst &lt;- grep(st, tx, fixed = F, ignore.case = F)\n  \n  # Identifica la posici√≥n FINAL del texto en espa√±ol\n  ned &lt;- grep(ed, tx, fixed = F, ignore.case = T)\n  \n  # Si no encuentra la identificaci√≥n del inicio\n  if(length(nst)==0){\n    \n    # Define como inicio la l√≠nea 0\n    nst &lt;-0\n    }\n\n  # Si no encuentra la identificacion del final\n  if(length(ned)==0){\n    \n    # Define como final la √∫ltima l√≠nea m√°s 1\n    ned &lt;- length(tx)+1\n  }\n  \n  # Selecciona solo el texto deste la posici√≥n de inicio\n  # m√°s una l√≠nea y la de final menos una l√≠nea\n  tx &lt;- tx[(nst+1):(ned-1)]\n  \n  # Busca el titulo en mayusculas\n  n &lt;- which(stri_detect(tx, regex = \"[A-Z]{2,}\\\\s+[A-Z]{2,}\")==T)[1]\n  \n  # Si encuentra un titulo en mayusculas, lee desde la posicion\n  # del titulo encontrado\n  if(! is.na(n)) tx &lt;- tx[n:length(tx)]\n\n  # Remueve la divisi√≥n en l√≠neas\n  tx &lt;- paste(tx, collapse = \" \")\n  \n  # Reemplaza dos espacios por un salto de l√≠nea\n  # Reproduce los p√°rrafos originales\n  tx &lt;- gsub(\"\\\\s{2,}\",\"\\n\",tx, fixed = F)\n\n  # Actualiza el texto en la base de datos\n  gt$text[i] &lt;- tx\n\n}\n\n\n\n\nDecretos presidenciales de Paraguay\nEn el √∫ltimo ejemplo de esta parte, seleccionaremos algunos datos clave para clasificar los decretos presidenciales de Paraguay: el √≥rgano a que se refiere el decreto (en general un ministerio), la exposici√≥n de motivos o el resumen del mismo y el texto de sus art√≠culos.\nEl c√≥digo abajo aplica uno o m√°s estrategias introducidas anteriormente para llevar a cabo dicha tarea.\n\n\nCode\n# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\nlibrary(httr)\nlibrary(jsonlite)\n\n# Obtiene la lista de archivos de la carpeta en GitHub\nurl_api &lt;- \"https://api.github.com/repos/rodrodr/Burgos_2025/contents/pdf\"\nresp &lt;- GET(url_api)\nfiles &lt;- fromJSON(content(resp, \"text\"))\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(files$download_url)\n\n# Selecciona solo los PDFs\ngt &lt;- gt[grep(\".pdf\",gt$doc_id),]\n\n# Crea tres variables nuevas\ngt$organo &lt;- NA    # √ìrgano a que se refiere\ngt$motivo &lt;- NA    # Motivo expuesto\ngt$teor &lt;- NA      # Art√≠culos del decreto\n\n# Para cada decreto\nfor(i in 1:nrow(gt)){\n\n  # Divide los textos en l√≠neas\n  tx &lt;- stri_split_lines1(gt$text[i])\n\n  # Elimina los espacios en blanco\n  tx &lt;- sapply(tx, trimws, USE.NAMES = F)\n\n  # Elimina los m√∫ltiples espacios en blanco\n  tx &lt;- gsub(\"\\\\s{2,}\",\" \", tx)\n  \n  # Prepara las partes del texto para que\n  # se dividan seg√∫n un salto de l√≠nea\n  tx &lt;- gsub(\"VISTO\",\"  VISTO\", tx)\n  tx &lt;- gsub(\"Asunc\",\"  Asunc\", tx)\n  tx &lt;- gsub(\"DECRETA:\",\"  DECRETA: \", tx)\n  \n  # Elimina las l√≠neas\n  tx &lt;- paste(tx, collapse = \" \")\n  \n  # Reemplaza los dobles espacios por \n  # saltos de l√≠nea\n  tx &lt;- gsub(\"\\\\s{2,}\",\"\\n\",tx)\n  \n  # Vuelve a dividir, pero ahora en\n  # p√°rrafos consistentes\n  tx &lt;- stri_split_lines1(tx)\n  \n  # Averigua si contiene la expresi√≥n cexter\n  ce &lt;- grep(\"cexter\",tolower(tx), fixed = T)\n\n  # En caso positivo, la elimina\n  if(length(ce)&gt;0){\n    tx &lt;- tx[-ce]\n  }\n  \n  # Encuentra la posicion de los motivos\n  # en el texto\n  mo &lt;- tx[grep(\"POR EL CUAL\", tx)[1]]\n  \n  # Encuentra la informaci√≥n sobre el √≥rgano\n  pres &lt;- tolower(tx[grep(\"PRESIDEN\", tx)[1]])\n  \n  # Elimina la presidencia de la identificaci√≥n\n  # del √≥rgano o ministerio\n  pres &lt;- gsub(\"presidencia de la rep√∫blica del paraguay\",\"\", pres)\n  pres &lt;- trimws(pres)\n\n  # Encuentra el texto de los art√≠culos\n  teor &lt;- tx[(grep(\"DECRETA:\", tx)+1):length(tx)]\n  \n  # Lo convierte en un p√°rrafo\n  teor &lt;- paste(teor, collapse = \"\\n\")\n\n  # Si queda texto de ruido, lo elimina\n  teor &lt;- gsub(\"NA\\nDECRETA: \",\"\", teor, fixed = T)\n  \n  # Atribuye cada informaci√≥n a su respectiva\n  # variable en la base de datos\n  gt$organo[i] &lt;- pres\n  gt$motivo[i] &lt;- mo\n  gt$teor[i] &lt;- teor\n  \n}\n\n# Visualiza los resultados\nreactable(gt, wrap = F, resizable = T, defaultPageSize = 10)",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#di√°logos-a-bases-de-datos",
    "href": "preparacion.html#di√°logos-a-bases-de-datos",
    "title": "Preparaci√≥n de los textos",
    "section": "Di√°logos a bases de datos",
    "text": "Di√°logos a bases de datos\nEl √∫ltimo paso consiste en convertir un diario de sesiones en una base de datos de intervenciones. Para ello, necesitaremos emplear lo que hemos visto m√°s arriba para identificar los oradores y los temas y dividir el texto de acuerdo con la estructura del debate. Emplearemos aqu√≠ un diario de sesi√≥n reciente de la actual legislatura del Congreso de Diputados de Espa√±a (legislatura XV). Se trata de un proceso que puede ser complejo y requerir varios pasos. A continuaci√≥n, se presenta un ejemplo de c√≥mo llevar a cabo esta tarea utilizando R para los diarios del Congreso de Diputados de Espa√±a.\nEl primer paso consiste en preparar el ambiente: cargar paquetes y funciones. He creado una funci√≥n, llamada fixParagraph que corrige los saltos de linea en los p√°rrafos, algo muy com√∫n y molesto cuando trabajamos con PDFs. Con esto, podemos empezar:\n\n\nCode\nlibrary(readtext)\nlibrary(stringi)\nlibrary(dplyr)\nlibrary(reactable)\n\n# Crea la funci√≥n para arreglar los p√°rrafos\nfixParagraph &lt;- function(text){\n  \n  text &lt;- gsub(\"(?&lt;=\\\\w)\\\\n(?=\\\\w)\",\" \", text, perl = TRUE)\n  text &lt;- stringi::stri_replace_all_regex(text, \"\\\\n\",\"\\n\\n\")\n  text &lt;- stringi::stri_replace_all_regex(text, \"([A-Za-z\\\\p{L}\\\\p{Lu}\\\\)0-9])([%‚Äî‚Äï,;:]+)(\\n+)\", \"$1$2 \")\n  text &lt;- stringi::stri_replace_all_regex(text, \"([A-Za-z\\\\p{L}\\\\p{Lu}])(\\n+)([a-z\\\\p{L}\\\\p{Lu}])\", \"$1 $3\")\n  text &lt;- stringi::stri_replace_all_regex(text, \"([A-Za-z\\\\p{L}])(\\n+)([a-z\\\\p{L}‚Äï\\\\(])\", \"$1 $3\")\n    \n  return(text)\n  \n}\n\n\nEl siguiente paso es leer el pdf y preparar el texto para la posterior identificaci√≥n de di√°logos y reestructuraci√≥n como base de datos. En las l√≠neas de c√≥digo abajo llevamos a cabo las siguientes tareas:\n\nLeemos el PDF y extraemos el texto.\nConvertimos el texto en l√≠neas individuales.\nEliminamos espacios en blanco al inicio y al final de cada l√≠nea.\nEliminamos encabezados repetidos, n√∫meros de p√°gina y otros elementos no deseados.\nCorregimos los p√°rrafos para asegurar que est√©n bien formateados.\nRealizamos algunas correcciones espec√≠ficas en el texto.\nSeparamos los oradores del texto de su intervenci√≥n.\nEliminamos la informaci√≥n del final que no es relevante para el an√°lisis.\n\n\n\nCode\n# Define la ruta del archivo PDF\nfile &lt;- \"https://raw.githubusercontent.com/rodrodr/Burgos_2025/main/diarios/DSCD-15-PL-141.pdf\"\n\n# Carga los datos\ntt &lt;- readtext(file)\n\n# Extrae el texto\ntx &lt;- tt$text\n\n# Lo convierte en lineas\ntx &lt;- read_lines(tx)\n\n# Elimina espacios en blanco al inicio y al final\ntx &lt;- stri_trim(tx)\n\n# Elimina el encabezado repetido\n\n# Identificacion de los diarios\nnn &lt;- which(stri_detect_fixed(tx, \"DIARIO DE SESIONES DEL CONGRESO DE LOS DIPUTADOS\")==TRUE) \ntx &lt;- tx[-nn]\n\n# Pleno\nnn &lt;- which(stri_detect_fixed(tx, \"PLENO Y DIPUTACI√ìN PERMANENTE\")==TRUE) \ntx &lt;- tx[-nn]\n\n# Archivo\nnn &lt;- which(stri_detect_regex(tx, \"^cve:\\\\sDSCD-[0-9]{1,2}-PL-[0-9]{1,3}\")==TRUE) \ntx &lt;- tx[-nn]\n\n# N√∫mero de p√°ginas\nnn &lt;- which(stri_detect_regex(tx, \"^N√∫m.\\\\s[0-9]{1,3}\\\\s+\")==TRUE) \ntx &lt;- tx[-nn]\n\ntx &lt;- tx[tx!=\"\"]\n\n# Corrige los parrafos\ntx &lt;- paste(tx, collapse = \"\\n\")\ntx &lt;- fixParagraph(tx)\n\n# Elimina un error tipogr√°fico del diario\ntx &lt;- stri_replace_all_regex(tx, \"([A-Z\\\\.])(\\n+)(\\\\(N√∫mero de expediente)\", \"$1 $3\")\n\n# Arregla los titulos pegados a subtitulos\ntx &lt;- stri_replace_all_regex(tx, \"([A-Z\\\\)])(:)(\\\\s)(‚Äî\\\\s[A-Z])\", \"$1$2\\n$4\")\n\n# Lo convierte en lineas\ntx &lt;- read_lines(tx)\n\n# Separa los oradores del texto\ntx &lt;- stri_replace_all_regex(tx, \"(^El se√±or |^La se√±ora )([A-Z\\\\p{Lu}]{2,}?)(.+?)(:)(\\\\s)\", \"$1$2$3$4\\n\\n\")\n\n# Lo vuelve a juntar\ntx &lt;- paste(tx, collapse = \"\\n\")\n\n# Lo convierte en lineas\ntx &lt;- read_lines(tx)\n\n# Elimina la informaci√≥n del final\ntx &lt;- tx[1:which(\n            stri_detect_regex(tx, \n                              \"^Eran las|^Era la\")==TRUE)]\n\n\nEl resultado ya no es un texto completo, sino un conjunto de l√≠neas. Cada l√≠nea corresponde a una parte del texto. Un p√°rrafo, la identificaci√≥n de un t√≠tulo o un orador. Con esa informaci√≥n en mano, podemos identificar los t√≠tulos, subt√≠tulos y oradores en el texto. Para ello, utilizamos expresiones regulares espec√≠ficas que buscan patrones caracter√≠sticos de cada uno de estos elementos. Una vez identificados, almacenamos estos elementos junto con el texto en un data.frame para su posterior an√°lisis.\nLos pasos son los siguientes:\n\nIdentificamos los t√≠tulos\nIdentificamos los subt√≠tulos\nIdentificamos los oradores\nCreamos un data.frame con las identificaciones y el texto\nFiltramos las l√≠neas vac√≠as\n\n\n\nCode\n# Identifica los t√≠tulos\npat_tit &lt;- \"^(?!.*VOX|XX|UPN)^[A-Z\\\\p{LU}]{2,}(.+)(:|\\\\.)$\"\n\ntit &lt;- stri_extract_all_regex(tx, pat_tit, simplify = T)\ntit &lt;- as.character(tit)\n\n# Identifica los subtitulos  \npat_sub &lt;- \"(^[‚Äî‚Äï] [A-Z\\\\p{Lu}]+)(.+)(\\\\(N√∫mero de expediente [0-9]{3}\\\\/[0-9]{6}\\\\))(\\\\.$|$)\"  \n\nsub &lt;- stri_extract_all_regex(tx, pat_sub, simplify = T)\nsub &lt;- as.character(sub)\n\n# Identifica los oradores\n\npat_ora &lt;- \"(^El se√±or |^La se√±ora )([A-Z\\\\p{Lu}]{2,}?)(.+?)(:)\"\n\n\nora &lt;- stri_extract_all_regex(tx, pat_ora, simplify = T)\nora &lt;- as.character(ora)\n\n# Creamos un data.frame con toda la informaci√≥n\nd &lt;- data.frame(title=tit, subtitle=sub, speaker=ora, text=tx)\nd &lt;- d[d$text!=\"\",]\n\n# Visualizamos los resultados\nreactable(d,\n          wrap=F,\n          sortable = T, \n          resizable = T)\n\n\n\n\n\n\nAhora las cosas empiezan a ganar forma. Ya tenemos los textos, los t√≠tulos (secciones del debate), los subt√≠tulos (detalles de la secci√≥n del debate) y los oradores. No obstante, hay mucha informaci√≥n incompleta. Espacios vac√≠os que hay que rellenar.\nPara ello:\n\nA√±adimos informaci√≥n al inicio para indicar los datos del sumario\nRellenamos los espacios vac√≠os en las variables t√≠tulo, subt√≠tulo y orador\nEliminamos duplicidades\nTratamos los t√≠tulos sin subt√≠tulos\nCreamos una variable de orden de la intervenci√≥n\nAgregamos el texto para que a cada linea corresponda a una intervenci√≥n.\n\n\n\nCode\n# A√±ade informaci√≥n al inicio\nd$title[1] &lt;- \"Sumario\"\nd$subtitle[1] &lt;- \"Sumario\"\nd$speaker[1] &lt;- \"Sumario\"\n\n# Rellena los espacios vac√≠os entre\n# t√≠tulos y diputados\nd$title &lt;- zoo::na.locf(d$title, na.rm = F)\nd$subtitle &lt;- zoo::na.locf(d$subtitle, na.rm = F)\nd$speaker &lt;- zoo::na.locf(d$speaker, na.rm = F)\n\n# Eliminamos la identificacion de\n# titulos, subtitulos y oradores\n# del texto de las intervenciones\nd &lt;- d[d$title!=d$text,]\nd &lt;- d[d$subtitle!=d$text,]\nd &lt;- d[d$speaker!=d$text,]\n\ntcheck &lt;- \"\"\n\n# T√≠tulos sin subtitulos\nfor(i in 2:nrow(d)){\n  \n  tm1 &lt;- d$title[i-1]\n  t0 &lt;- d$title[i]\n  \n  sm1 &lt;- d$subtitle[i-1]\n  s0 &lt;- d$subtitle[i]\n  \n  \n  if(s0==sm1 & t0!=tm1){\n    \n    d$subtitle[i] &lt;- d$title[i]\n    \n    tcheck &lt;- d$title[i]\n    \n  }else if(s0!=sm1 & tcheck==t0){\n    \n    d$subtitle[i] &lt;- d$title[i]\n    \n    tcheck &lt;- d$title[i]\n    \n  }\n    \n}\n\n# Para el caso de dos t√≠tulos seguidos sin subt√≠tulo\n# justo al principio de la sesion\nd$subtitle[d$title!=\"Sumario\" & d$subtitle==\"Sumario\"] &lt;- d$title[d$title!=\"Sumario\" & d$subtitle==\"Sumario\"]\n\n# Crea una variable con el t√≠tulo, subtitulo y \n# el speaker (para las transiciones de temas)\nd$sub_speak &lt;- paste0(d$title,\"_\", d$subtitle, \"_\", d$speaker)\n\n# Crea una variable de orden\nd &lt;- d |&gt;\n  mutate(order = cumsum(\n                    sub_speak != lag(sub_speak, \n                    default = first(sub_speak))))\n\n\n# Borro la variable de trabajo\nd$sub_speak &lt;- NULL\n\n# Agrega el texto por t√≠tulo, subtitulo, \n# diputado y orden\nd &lt;- aggregate(list(text=d$text), \n               by=list(title=d$title, \n                       subtitle=d$subtitle, \n                       speaker=d$speaker, \n                       order=d$order), \n               FUN=paste, \n               collapse=\"\\n\") \n\n# A√±adimos como brindis:\n# \n# el n√∫mero de palabras\nd$nwords &lt;- stri_count_words(d$text)\n\n# Indica si es miembro de la mesa\nd$mesa &lt;- 0\nd$mesa[which(stri_detect_fixed(d$speaker, \"La se√±ora PRESIDENTA\")==TRUE)] &lt;- 1\nd$mesa[which(stri_detect_fixed(d$speaker, \"La se√±ora VICEPRESIDENTA (\")==TRUE)] &lt;- 1\nd$mesa[which(stri_detect_fixed(d$speaker, \"El se√±or SECRETARIO\")==TRUE)] &lt;- 1\nd$mesa[which(stri_detect_fixed(d$speaker, \"El se√±or VICEPRESIDENTE (\")==TRUE)] &lt;- 1\n\n# Visualiza los resultados\nreactable(d, \n          wrap = F, \n          resizable = T, \n          defaultPageSize = 10)\n\n\n\n\n\n\nComo hab√©is visto, ahora tenemos nuestra base de datos de intervenciones. Todav√≠a no podemos hacer an√°lisis por partido, puesto que tendr√≠amos que a√±adir informaci√≥n adicional (o seguir extray√©ndola del texto, algo que ser√≠a m√°s complicado). No obstante, ahora podemos a√±adir datos a partir de otras fuentes para enriquecer el an√°lisis.",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#respuestas",
    "href": "preparacion.html#respuestas",
    "title": "Preparaci√≥n de los textos",
    "section": "Respuestas",
    "text": "Respuestas\nRespuesta al ejercicio de expresi√≥n regular:\nLa expresi√≥n regular que captura los t√≠tulos es la siguiente:\n(^-\\s[0-9]+\\s-|^-\\s[0-9]+\\s-\\s)(\\n{3})([\\sA-Z\\p{Lu}0-9]+)(\\n{2})\n\nEl primer grupo (^-\\s[0-9]+\\s-|^-\\s[0-9]+\\s-\\s) identifica el inicio del t√≠tulo. El s√≠mbolo ^ indica el inicio de una l√≠nea, seguido de un guion -, un espacio \\s, uno o m√°s d√≠gitos num√©ricos [0-9]+, otro espacio \\s y otro guion -. La alternativa | permite capturar t√≠tulos que tienen un espacio adicional despu√©s del segundo guion.\nEl segundo grupo (\\n{3}) identifica los tres saltos de l√≠nea que preceden al t√≠tulo. Aqu√≠, \\n representa un salto de l√≠nea y el grupo captura tres saltos consecutivos.\nEl tercer grupo ([\\sA-Z\\p{Lu}0-9]+) captura el contenido del t√≠tulo. Aqu√≠, [\\sA-Z\\p{Lu}0-9]+ indica que el t√≠tulo puede contener espacios \\s, letras may√∫sculas [A-Z], letras may√∫sculas con tildes \\p{Lu} y d√≠gitos num√©ricos [0-9]. El s√≠mbolo + indica que puede haber uno o m√°s de estos caracteres.\nEl cuarto grupo (\\n{2}) identifica los dos saltos de l√≠nea que siguen al t√≠tulo. Aqu√≠, \\n representa un salto de l√≠nea y el grupo captura dos saltos consecutivos.]",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "preparacion.html#referencias-adicionales",
    "href": "preparacion.html#referencias-adicionales",
    "title": "Preparaci√≥n de los textos",
    "section": "Referencias adicionales",
    "text": "Referencias adicionales\nExiste un enorme material disponible sobre expresiones regulares, os recomiendo los siguientes:\n\nWickam - ‚ÄúStrings‚Äù En R for Data Science\n‚ÄúRegular Expressions‚Äù en la documentaci√≥n del paquete stringr\n‚ÄúRegular Expressions‚Äù en el laboratorio LADAU\n\nTambi√©n vale la pena consultar las referencias de los dos paquetes m√°s importantes para la manipulaci√≥n de datos en R, el stringr y el stringi:\n\nstringi: Fast and Portable Character String Processing in R\nstringr",
    "crumbs": [
      "Preparaci√≥n"
    ]
  },
  {
    "objectID": "index.html#el-profesor",
    "href": "index.html#el-profesor",
    "title": "PolTextAn√°lisis de textos pol√≠ticos con R",
    "section": "El profesor",
    "text": "El profesor\n\n\n\n\n\n\nRodrigo Rodrigues-Silveira\nrodrodr@usal.es\nProfesor de ciencia pol√≠tica de la USAL. Director del proyecto ‚ÄúComportamiento legislativo y erosi√≥n democr√°tica en Am√©rica Latina‚Äù (PELA Comportamiento). Miembro de los GIR ‚ÄúPol√≠tica Comparada en Am√©rica Latina‚Äù y ‚ÄúTecnolog√≠a y poder en el pensamiento y las letras‚Äù.",
    "crumbs": [
      "Introducci√≥n al curso"
    ]
  },
  {
    "objectID": "basico.html",
    "href": "basico.html",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "",
    "text": "Una vez los documentos han sido preparados y pre-procesados, pueden ser abiertos en R. La funci√≥n readtext del paquete con el mismo nombre permite importar (o ‚Äúabrir‚Äù) textos individuales o carpetas enteras. Los documentos pueden ser de diferentes formatos: txt, doc(x), pdf, html, csv, tab, tsv, xml, xls(x), json, odt, o rtf. Se trata de una funci√≥n muy √∫til para importar vol√∫menes grandes de texto.\n\n\nCode\n# Obtiene una lista de archivos en\n# una carpeta online de Github\nlibrary(jsonlite)\n\nurl &lt;- \"https://api.github.com/repos/rodrodr/tenet_texts/contents/spa.inaugural\"\n\nnm &lt;- read_json(url)\nnm &lt;- list2DF(nm)\nnm &lt;- sort(as.character(unlist(nm[8,])))\n\n# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(nm)\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo se puede observar, se cargan 15 discursos de investidura de los Presidentes de gobierno de Espa√±a desde 1979 hasta la actualidad. Se trata de un objeto de tipo data.frame con dos columnas: doc_id, en general el nombre del archivo, y text, que contiene el texto integral. Este formato servir√° de base y resulta obligatorio para la transformaci√≥n de esos textos en un objeto de tipo corpus perteneciente al paquete quanteda, base o infraestructura de la mayor parte de los an√°lisis realizados durante todo el curso.\nAdem√°s de doc_id y text, el data.frame, uno puede a√±adir m√°s variables que ayuden a contextualizar los documentos y suministren informaci√≥n √∫til para el posterior an√°lisis. No obstante, hay que tener claro que la funci√≥n readtext solamente genera las dos primeras variables. Los metadatos adicionales deben ser a√±adidos a posteriori, sea justo despu√©s de la importaci√≥n o, luego, como documentaci√≥n del corpus, como veremos m√°s adelante.\nEl hecho de que utilicemos textos guardados en una carpeta en la nube hace con que el c√≥digo arriba sea un poco m√°s complejo del que ser√≠a necesario. En el caso de que los archivos est√©n en el disco duro bastar√≠a con informar el camino hacia la carpeta:\n\n\nCode\n# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(\"/Escritorio/Carpeta/\")\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\nUna vez abiertos los datos, existen dos opciones. La primera es tratar los datos para extraer metadatos o agregar/fragmentar los textos en otras unidades de observaci√≥n (como los tweets de un mismo partido, o fragmentar un libro por cap√≠tulos). La segunda consiste en transformar el data.frame en un objeto corpus y seguir con el an√°lisis:\n\n\nCode\n# Carga el paquete quanteda\nlibrary(quanteda)\n\n# Transforma los textos en corpus\ncp &lt;- corpus(tx)\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAl explorar el objeto corpus por medio de la funci√≥n summary(cp), vemos un conjunto de variables descriptivas:\n\nText, nombre del documento;\nTypes, se√±ala el n√∫mero de palabras y s√≠mbolos √∫nicos en el documento;\nTokens, n√∫mero total de palabras y s√≠mbolos; y\nSentences, cantidad de frases en el texto.\n\n\n\n\nEl siguiente paso consiste en adicionar m√°s informaci√≥n contextual (metadatos) sobre los textos. Tales informaciones resultar√°n de mucha utilidad en las siguientes etapas de an√°lisis, puesto que permitir√°n agregar las informaciones seg√∫n distintas caracter√≠sticas. Por ejemplo, podemos decidir agrupar los textos seg√∫n presidente (y no gesti√≥n o legislatura). Tambi√©n podr√≠amos organizar el an√°lisis seg√∫n partido del presidente o por su ideolog√≠a.\nCuanto mayor la documentaci√≥n de los textos, mayores las posibilidades de reagrupar, fragmentar o reordenar los textos seg√∫n distintas categor√≠as anal√≠ticas. Adem√°s, se posibilitan distintas comparaciones entre grupos y entre √©stos con el patr√≥n general.\nLa funci√≥n docvars posibilita crear nuevas variables contextuales o de metadatos en un corpus. Su sintaxe resulta muy sencilla:\ndocvars(corpus,‚Äúvariable name‚Äù ) &lt;- variable con el contenido.\n\n\nCode\ndocvars(cp, \"Presidente\") &lt;- c(\"Adolfo Su√°rez\",\n                               \"Leopoldo Calvo Sotelo\",\n                               \"Felipe Gonz√°lez\",\n                               \"Felipe Gonz√°lez\",\n                               \"Felipe Gonz√°lez\",\n                               \"Felipe Gonz√°lez\",\n                               \"Jos√© Mar√≠a Aznar\",\n                               \"Jos√© Mar√≠a Aznar\",\n                               \"Jos√© Luis Zapatero\",\n                               \"Jos√© Luis Zapatero\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Pedro S√°nchez\",\n                               \"Pedro S√°nchez\",\n                               \"Pedro S√°nchez\")\n\ndocvars(cp, \"Nombramiento\") &lt;- c(\"1979-03-31\",\n                                 \"1981-02-26\",\n                                 \"1982-12-02\",\n                                 \"1986-06-23\",\n                                 \"1989-12-05\",\n                                 \"1993-07-09\",\n                                 \"1996-05-04\",\n                                 \"2000-04-26\",\n                                 \"2004-04-17\",\n                                 \"2008-04-11\",\n                                 \"2011-12-20\",\n                                 \"2015-12-21\",\n                                 \"2016-10-30\",\n                                 \"2018-06-01\",\n                                 \"2020-01-07\",\n                                 \"2023-11-15\")\n\n\ndocvars(cp, \"Cese\") &lt;- c(\"1981-02-26\",\n                         \"1982-12-02\",\n                         \"1986-06-23\",\n                         \"1989-10-30\",\n                         \"1993-06-07\",\n                         \"1996-03-04\",\n                         \"2000-03-13\",\n                         \"2004-03-15\",\n                         \"2008-03-10\",\n                         \"2011-11-21\",\n                         \"2015-12-21\",\n                         \"2016-10-29\",\n                         \"2018-06-01\",\n                         \"2019-04-29\",\n                         \"2023-11-14\",\n                         NA)\n\n\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\", \n                            \"PSOE\", \"PSOE\",\"PSOE\",\"PP\",\n                            \"PP\",\"PSOE\",\"PSOE\",\n                            \"PP\",\"PP\",\"PP\",\n                            \"PSOE\",\"PSOE\",\"PSOE\")\n\n\n\ndocvars(cp, \"Ideolog√≠a\") &lt;- c(\"Derecha\", \"Derecha\", \"Izquierda\", \n                            \"Izquierda\", \"Izquierda\",\"Izquierda\",\n                            \"Derecha\", \"Derecha\", \"Izquierda\",\n                            \"Izquierda\", \"Derecha\", \"Derecha\", \n                            \"Derecha\", \"Izquierda\", \"Izquierda\",\"Izquierda\")\n\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\nComo se puede observar en la tabla arriba, se han a√±adido las variables con el nombre del presidente, fecha de nombramiento, cese, el partido pol√≠tico al que pertenec√≠a y la ideolog√≠a de la mayor parte de los miembros de los partidos. Estas categor√≠as permitir√°n separar en la fase de an√°lisis diferentes perfiles de grupo como, por ejemplo, los conceptos o expresiones m√°s utilizados por l√≠deres de derecha e izquierda o por cada partido.\n\n\n\nOtra opci√≥n consiste en reorganizar el texto seg√∫n nuevas unidades de an√°lisis. A veces, algunos aspectos del discurso se desvelan de modo m√°s claro cuando las informaci√≥n se organiza desde una perspectiva distinta. Esa pseudo-alteridad se puede alcanzar a veces por mirar a un mismo texto desde otro √°ngulo. ¬øQu√© cambios se pueden observar en la importancia de los conceptos cuando organizamos los textos seg√∫n partido o ideolog√≠a y no m√°s de acuerdo con cada una de las legislaturas? ¬øAparece algo nuevo? ¬øExisten contradicciones o patrones distintos frente a lo que hab√≠amos percibido en el an√°lisis anterior?\nSe pueden adoptar dos estrategias fundamentales. La primera consiste en fragmentar los textos en unidades menores como p√°rrafos o sentencias, por ejemplo. La segunda trata de agregar los textos a partir de caracter√≠sticas comunes, como juntar todos los documentos de una misma ideolog√≠a. Adem√°s, se pueden combinar entre s√≠. Podemos juntar todos los textos por partido y luego fragmentarlos por frase. De cualquier forma, el cambio en la unidad de observaci√≥n debe tener un prop√≥sito anal√≠tico claro. ¬øQu√© se quiere aprender al estructurar los textos de una manera determinada?\nEmpecemos con la fragmentaci√≥n. Utilicemos el corpus de discursos de inauguraci√≥n de los presidentes de gobierno espa√±oles y dividamos el corpus por p√°rrafo. Esto se puede hacer con la funci√≥n corpus_reshape de quanteda.\n\n\nCode\n# Reorganiza el corpus segun frases\ncs &lt;- corpus_reshape(x = cp, to = \"sentences\")\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cs),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nPara agregar los textos, hay que dar un paso atr√°s y aunar los textos de un mismo grupo en un √∫nico documento. Para ello, podemos convertir el corpus documentado en un objeto de tipo data.frame y, luego, agregar los textos y volver a crear un corpus con la nueva unidad de observaci√≥n. Utilizaremos ahora los presidentes como unidad.\n\n\nCode\n# Convierte el corpus documentado en un data.frame\ntd &lt;- convert(cp, to=\"data.frame\")\n\n# Unifica los textos en un solo documento a partir\n# de las funciones aggregate (que agrega por grupos)\n# y paste0, que colapsa textos.\n# Hemos decidido utilizar dos separadores de linea (\\n\\n)\n# para indicar la separacion entre un texto y otro\ntd &lt;- aggregate(list(text=td$text), by=list(Presidente=td$Presidente,\n                                      Partido=td$Partido,\n                                      Ideologia=td$Ideolog√≠a),\n                                paste0,\n                                collapse=\"\\n\\n\")\n\n# vuelve a crear un corpus con el nuevo\n# objeto agregado\ncx &lt;- corpus(td)\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cx),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAhora mismo tenemos solamente siete documentos en el corpus. Corresponden a la nueva unidad de agregaci√≥n: Presidente. Con dichas informaciones, la comparaci√≥n se hace entre estilos discursivos de los l√≠deres, m√°s que un per√≠odo sobre otro. Har√≠amos lo mismo para los partidos o la ideolog√≠a. Incluso podr√≠amos utilizar un corpus para cada unidad y comparar los resultados.",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "basico.html#primeros-pasos",
    "href": "basico.html#primeros-pasos",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "",
    "text": "Una vez los documentos han sido preparados y pre-procesados, pueden ser abiertos en R. La funci√≥n readtext del paquete con el mismo nombre permite importar (o ‚Äúabrir‚Äù) textos individuales o carpetas enteras. Los documentos pueden ser de diferentes formatos: txt, doc(x), pdf, html, csv, tab, tsv, xml, xls(x), json, odt, o rtf. Se trata de una funci√≥n muy √∫til para importar vol√∫menes grandes de texto.\n\n\nCode\n# Obtiene una lista de archivos en\n# una carpeta online de Github\nlibrary(jsonlite)\n\nurl &lt;- \"https://api.github.com/repos/rodrodr/tenet_texts/contents/spa.inaugural\"\n\nnm &lt;- read_json(url)\nnm &lt;- list2DF(nm)\nnm &lt;- sort(as.character(unlist(nm[8,])))\n\n# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(nm)\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo se puede observar, se cargan 15 discursos de investidura de los Presidentes de gobierno de Espa√±a desde 1979 hasta la actualidad. Se trata de un objeto de tipo data.frame con dos columnas: doc_id, en general el nombre del archivo, y text, que contiene el texto integral. Este formato servir√° de base y resulta obligatorio para la transformaci√≥n de esos textos en un objeto de tipo corpus perteneciente al paquete quanteda, base o infraestructura de la mayor parte de los an√°lisis realizados durante todo el curso.\nAdem√°s de doc_id y text, el data.frame, uno puede a√±adir m√°s variables que ayuden a contextualizar los documentos y suministren informaci√≥n √∫til para el posterior an√°lisis. No obstante, hay que tener claro que la funci√≥n readtext solamente genera las dos primeras variables. Los metadatos adicionales deben ser a√±adidos a posteriori, sea justo despu√©s de la importaci√≥n o, luego, como documentaci√≥n del corpus, como veremos m√°s adelante.\nEl hecho de que utilicemos textos guardados en una carpeta en la nube hace con que el c√≥digo arriba sea un poco m√°s complejo del que ser√≠a necesario. En el caso de que los archivos est√©n en el disco duro bastar√≠a con informar el camino hacia la carpeta:\n\n\nCode\n# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(\"/Escritorio/Carpeta/\")\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\nUna vez abiertos los datos, existen dos opciones. La primera es tratar los datos para extraer metadatos o agregar/fragmentar los textos en otras unidades de observaci√≥n (como los tweets de un mismo partido, o fragmentar un libro por cap√≠tulos). La segunda consiste en transformar el data.frame en un objeto corpus y seguir con el an√°lisis:\n\n\nCode\n# Carga el paquete quanteda\nlibrary(quanteda)\n\n# Transforma los textos en corpus\ncp &lt;- corpus(tx)\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAl explorar el objeto corpus por medio de la funci√≥n summary(cp), vemos un conjunto de variables descriptivas:\n\nText, nombre del documento;\nTypes, se√±ala el n√∫mero de palabras y s√≠mbolos √∫nicos en el documento;\nTokens, n√∫mero total de palabras y s√≠mbolos; y\nSentences, cantidad de frases en el texto.\n\n\n\n\nEl siguiente paso consiste en adicionar m√°s informaci√≥n contextual (metadatos) sobre los textos. Tales informaciones resultar√°n de mucha utilidad en las siguientes etapas de an√°lisis, puesto que permitir√°n agregar las informaciones seg√∫n distintas caracter√≠sticas. Por ejemplo, podemos decidir agrupar los textos seg√∫n presidente (y no gesti√≥n o legislatura). Tambi√©n podr√≠amos organizar el an√°lisis seg√∫n partido del presidente o por su ideolog√≠a.\nCuanto mayor la documentaci√≥n de los textos, mayores las posibilidades de reagrupar, fragmentar o reordenar los textos seg√∫n distintas categor√≠as anal√≠ticas. Adem√°s, se posibilitan distintas comparaciones entre grupos y entre √©stos con el patr√≥n general.\nLa funci√≥n docvars posibilita crear nuevas variables contextuales o de metadatos en un corpus. Su sintaxe resulta muy sencilla:\ndocvars(corpus,‚Äúvariable name‚Äù ) &lt;- variable con el contenido.\n\n\nCode\ndocvars(cp, \"Presidente\") &lt;- c(\"Adolfo Su√°rez\",\n                               \"Leopoldo Calvo Sotelo\",\n                               \"Felipe Gonz√°lez\",\n                               \"Felipe Gonz√°lez\",\n                               \"Felipe Gonz√°lez\",\n                               \"Felipe Gonz√°lez\",\n                               \"Jos√© Mar√≠a Aznar\",\n                               \"Jos√© Mar√≠a Aznar\",\n                               \"Jos√© Luis Zapatero\",\n                               \"Jos√© Luis Zapatero\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Pedro S√°nchez\",\n                               \"Pedro S√°nchez\",\n                               \"Pedro S√°nchez\")\n\ndocvars(cp, \"Nombramiento\") &lt;- c(\"1979-03-31\",\n                                 \"1981-02-26\",\n                                 \"1982-12-02\",\n                                 \"1986-06-23\",\n                                 \"1989-12-05\",\n                                 \"1993-07-09\",\n                                 \"1996-05-04\",\n                                 \"2000-04-26\",\n                                 \"2004-04-17\",\n                                 \"2008-04-11\",\n                                 \"2011-12-20\",\n                                 \"2015-12-21\",\n                                 \"2016-10-30\",\n                                 \"2018-06-01\",\n                                 \"2020-01-07\",\n                                 \"2023-11-15\")\n\n\ndocvars(cp, \"Cese\") &lt;- c(\"1981-02-26\",\n                         \"1982-12-02\",\n                         \"1986-06-23\",\n                         \"1989-10-30\",\n                         \"1993-06-07\",\n                         \"1996-03-04\",\n                         \"2000-03-13\",\n                         \"2004-03-15\",\n                         \"2008-03-10\",\n                         \"2011-11-21\",\n                         \"2015-12-21\",\n                         \"2016-10-29\",\n                         \"2018-06-01\",\n                         \"2019-04-29\",\n                         \"2023-11-14\",\n                         NA)\n\n\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\", \n                            \"PSOE\", \"PSOE\",\"PSOE\",\"PP\",\n                            \"PP\",\"PSOE\",\"PSOE\",\n                            \"PP\",\"PP\",\"PP\",\n                            \"PSOE\",\"PSOE\",\"PSOE\")\n\n\n\ndocvars(cp, \"Ideolog√≠a\") &lt;- c(\"Derecha\", \"Derecha\", \"Izquierda\", \n                            \"Izquierda\", \"Izquierda\",\"Izquierda\",\n                            \"Derecha\", \"Derecha\", \"Izquierda\",\n                            \"Izquierda\", \"Derecha\", \"Derecha\", \n                            \"Derecha\", \"Izquierda\", \"Izquierda\",\"Izquierda\")\n\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\nComo se puede observar en la tabla arriba, se han a√±adido las variables con el nombre del presidente, fecha de nombramiento, cese, el partido pol√≠tico al que pertenec√≠a y la ideolog√≠a de la mayor parte de los miembros de los partidos. Estas categor√≠as permitir√°n separar en la fase de an√°lisis diferentes perfiles de grupo como, por ejemplo, los conceptos o expresiones m√°s utilizados por l√≠deres de derecha e izquierda o por cada partido.\n\n\n\nOtra opci√≥n consiste en reorganizar el texto seg√∫n nuevas unidades de an√°lisis. A veces, algunos aspectos del discurso se desvelan de modo m√°s claro cuando las informaci√≥n se organiza desde una perspectiva distinta. Esa pseudo-alteridad se puede alcanzar a veces por mirar a un mismo texto desde otro √°ngulo. ¬øQu√© cambios se pueden observar en la importancia de los conceptos cuando organizamos los textos seg√∫n partido o ideolog√≠a y no m√°s de acuerdo con cada una de las legislaturas? ¬øAparece algo nuevo? ¬øExisten contradicciones o patrones distintos frente a lo que hab√≠amos percibido en el an√°lisis anterior?\nSe pueden adoptar dos estrategias fundamentales. La primera consiste en fragmentar los textos en unidades menores como p√°rrafos o sentencias, por ejemplo. La segunda trata de agregar los textos a partir de caracter√≠sticas comunes, como juntar todos los documentos de una misma ideolog√≠a. Adem√°s, se pueden combinar entre s√≠. Podemos juntar todos los textos por partido y luego fragmentarlos por frase. De cualquier forma, el cambio en la unidad de observaci√≥n debe tener un prop√≥sito anal√≠tico claro. ¬øQu√© se quiere aprender al estructurar los textos de una manera determinada?\nEmpecemos con la fragmentaci√≥n. Utilicemos el corpus de discursos de inauguraci√≥n de los presidentes de gobierno espa√±oles y dividamos el corpus por p√°rrafo. Esto se puede hacer con la funci√≥n corpus_reshape de quanteda.\n\n\nCode\n# Reorganiza el corpus segun frases\ncs &lt;- corpus_reshape(x = cp, to = \"sentences\")\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cs),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nPara agregar los textos, hay que dar un paso atr√°s y aunar los textos de un mismo grupo en un √∫nico documento. Para ello, podemos convertir el corpus documentado en un objeto de tipo data.frame y, luego, agregar los textos y volver a crear un corpus con la nueva unidad de observaci√≥n. Utilizaremos ahora los presidentes como unidad.\n\n\nCode\n# Convierte el corpus documentado en un data.frame\ntd &lt;- convert(cp, to=\"data.frame\")\n\n# Unifica los textos en un solo documento a partir\n# de las funciones aggregate (que agrega por grupos)\n# y paste0, que colapsa textos.\n# Hemos decidido utilizar dos separadores de linea (\\n\\n)\n# para indicar la separacion entre un texto y otro\ntd &lt;- aggregate(list(text=td$text), by=list(Presidente=td$Presidente,\n                                      Partido=td$Partido,\n                                      Ideologia=td$Ideolog√≠a),\n                                paste0,\n                                collapse=\"\\n\\n\")\n\n# vuelve a crear un corpus con el nuevo\n# objeto agregado\ncx &lt;- corpus(td)\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cx),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAhora mismo tenemos solamente siete documentos en el corpus. Corresponden a la nueva unidad de agregaci√≥n: Presidente. Con dichas informaciones, la comparaci√≥n se hace entre estilos discursivos de los l√≠deres, m√°s que un per√≠odo sobre otro. Har√≠amos lo mismo para los partidos o la ideolog√≠a. Incluso podr√≠amos utilizar un corpus para cada unidad y comparar los resultados.",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "basico.html#el-arte-de-contar-palabras",
    "href": "basico.html#el-arte-de-contar-palabras",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "El arte de contar palabras",
    "text": "El arte de contar palabras\nUna vez terminada la preparaci√≥n del corpus, toca empezar el an√°lisis. El modo m√°s sencillo consiste en identificar qu√© palabras, conceptos o t√©rminos aparecen con mayor frecuencia y averiguar si hay diferencias sustantivas en su uso entre los documentos del corpus. Se trata de un m√©todo de an√°lisis aplicable tanto a conjuntos peque√±os de textos, que pueden ser le√≠dos con antelaci√≥n por el investigador, como a grandes repositorios imposibles de leer sin un proceso previo de an√°lisis, clasificaci√≥n y muestreo.\nEste m√©todo se suele denominar bag of words o ‚Äúbolsa de palabras‚Äù y se basa en la idea de que la frecuencia de aparici√≥n de ciertos t√©rminos puede revelar algo sobre el contenido o el estilo de un texto. Aunque no es un m√©todo infalible, puede proporcionar pistas valiosas sobre los temas predominantes, las preocupaciones y las prioridades de los autores.\nEn esta parte del trabajo trataremos de cuatro temas relacionados con el recuento directo de palabras. El primero abarca las t√©cnicas de preparaci√≥n y c√°lculo de frecuencias de palabras, tanto para el corpus como un todo como para cada documento o grupo en particular. El segundo repite las operaciones, pero en lugar de palabras completas, se emplear√°n sus ra√≠ces (stemming). El tercero describe la ponderaci√≥n de las frecuencias por su ocurrencia en todos los documentos. El cuarto se dedica a visualizaciones, como las nubes de palabras.\n\nFrecuencia de palabras\nEl primer paso para contar palabras o expresiones consiste en tokenizar el corpus y, a continuaci√≥n, crear una matriz documento-atributo (dfm, en su acr√≥nimo en ingl√©s). Se trata de un procedimiento sencillo que fragmenta cada texto en palabras, n-gramas (conjunto de n-palabras que aparecen juntas como en el bigrama ‚Äúeconom√≠a pol√≠tica‚Äù, por ejemplo) o incluso frases.\n\n\nCode\n# Crea un objeto de tipo tokens por palabra\ntk &lt;- tokens(cp)\n\n# Crea un objeto dfm\nfm &lt;- dfm(tk)\n\n# Buscamos las 10 palabras m√°s frecuentes\ntopfeatures(fm)\n\n\n   de     ,    la     .     y   que    en    el     a   los \n11845 11230  7883  5781  5767  5605  4903  4422  3663  3210 \n\n\nComo podemos ver, no aprendemos nada de la pol√≠tica espa√±ola mirando hacia los 10 t√©rminos m√°s frecuentes. Todos son conectores o puntuaci√≥n que se repiten sistem√°ticamente en cualquier texto. Probablemente, ‚Äúde‚Äù, ‚Äúque‚Äù, ‚Äúy‚Äù, ‚Äúla‚Äù, as√≠ como la coma o el punto y aparte ser√°n las palabras y los s√≠mbolos m√°s comunes en cualquier texto escrito en espa√±ol. Tales palabras se conocen como stop words o ‚Äúpalabras vac√≠as‚Äù de contenido que suelen ser muy frecuentes en cualquier idioma. Para evitar que ellas supongan un problema, lo mejor es quitarlas del medio y recrear la matriz de frecuencias.\n\n\nCode\n# Crea un objeto de tipo tokens por palabra eliminando la punctuacion\ntk &lt;- tokens(cp, remove_punct = T)\n\n# Elimina las stopwords\ntk &lt;- tokens_remove(tk, \n                    stopwords(language = \"es\"))\n\n# Crea un objeto dfm\nfm &lt;- dfm(tk)\n\n# Buscamos las 10 palabras m√°s frecuentes\ntopfeatures(fm)\n\n\n gobierno    espa√±a  se√±or√≠as  pol√≠tica      pa√≠s    social  sociedad espa√±oles \n      775       652       579       495       350       344       302       288 \n     a√±os   sistema \n      282       265 \n\n\nAhora el panorama ha cambiado. Aparecen nuevos t√©rminos, como ‚Äúgobierno‚Äù, ‚ÄúEspa√±a‚Äù, ‚Äúespa√±oles‚Äù, ‚Äúpol√≠tica‚Äù, ‚Äúsocial‚Äù, ‚Äúsociedad‚Äù, ‚Äúempleo‚Äù o ‚Äúsistema‚Äù. Tambi√©n palabras espec√≠ficas de tratamiento formal en los discursos inaugurales o en intervenciones parlamentarias, como es el caso de ‚Äúse√±or√≠as‚Äù.\nUno puede visualizar la frecuencia de palabras en un corpus por medio de una nube de palabras. Aunque sea un recurso m√°s est√©tico que informativo, sirve para tener una idea somera e inicial del peso relativo de los t√©rminos en un corpus o documento espec√≠fico. El c√≥digo abajo utiliza la funci√≥n wordcloud del paquete hom√≥nimo para generar el gr√°fico:\n\n\nCode\nlibrary(quanteda.textplots)\nlibrary(tenet)\nlibrary(wordcloud)\n\nft &lt;-topfeatures(fm, 50)\n\npar(mar=rep(0,4))\nwordcloud(names(ft), \n          freq = ft, \n          colors = pal$cat.cartocolor.antique.11)\n\n\n\n\n\n\n\n\n\nAbajo, buscamos las 25 palabras m√°s comunes, calculamos su frecuencia relativa y, adem√°s, creamos dos gr√°ficos para representarlas. Utilizamos la funci√≥n dfm_weight para obtener el peso relativo de los t√©rminos en el corpus. Esta √∫ltima medida ponderada resulta especialmente importante: (a) cuando comparamos su peso en cada uno de los textos y (b) cuando la extensi√≥n de los documentos resulta muy distinta.\n\n\nCode\n# Buscamos las 25 palabras m√°s frecuentes\nft &lt;- topfeatures(fm, n = 25)\n\n# A√±adimos la frecuencia relativa \nfp &lt;- dfm_weight(fm, \"prop\")\n\n# Repite la b√∫squeda para la frecuencia relativa\nfr &lt;- topfeatures(fp, n = 25)\n\n# Convierte los resultados en un data.frame\nxx &lt;- data.frame(Palabra=names(ft), \n                 Frec.Abs=ft, \n                 Frec.Rel=fr)\n\n# Carga el paquete ggplot2\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(grid)\n\n# Genera un gr√°fico de barras para visualizar la frecuencia de las palabras\np1 &lt;- ggplot(xx, aes(x=Frec.Abs, y=reorder(Palabra, Frec.Abs)))+\n  geom_bar(stat=\"identity\", fill=\"darkgreen\")+\n  theme_classic()+\n  labs(title=\"Frecuencia ABSOLUTA\")+\n  ylab(\"\")+\n  xlab(\"Frecuencia Absoluta\")\n\np2 &lt;- ggplot(xx, aes(x=Frec.Rel, y=reorder(Palabra, Frec.Rel)))+\n  geom_bar(stat=\"identity\", fill=\"orange\")+\n  theme_classic()+\n  labs(title=\"Frecuencia RELATIVA\")+\n  ylab(\"\")+\n  xlab(\"Frecuencia Relativa\")\n\n# La funci√≥n grid.arrange permite posicionar varios gr√°ficos lado a lado o uno en cima del otro\ngrid.arrange(p1,p2, ncol=2)\n\n\n\n\n\n\n\n\n\nEl pr√≥ximo paso ser√≠a calcular las frecuencias seg√∫n un grupo o atributo del corpus, como el presidente, por ejemplo. El c√≥digo abajo utiliza las funciones dfm_group para generar una matriz de frecuencia y dfm_weight para ponderar las palabras y obtener los valores relativos.\n\n\nCode\n# Crea un objeto dfm\nfg &lt;- dfm_group(fm, groups = quanteda::docvars(cp, \"Presidente\"))\n\n\n# Buscamos las 25 palabras m√°s frecuentes para cada presidente\nft &lt;- topfeatures(fg, n = 25, \n                  groups = quanteda::docvars(fg, \"Presidente\"))\n\n# Genera una frecuencia relativa\nfgw &lt;- dfm_weight(fg, scheme=\"prop\")\n\nftg &lt;- topfeatures(fgw, n = 25, \n                  groups = quanteda::docvars(fg, \"Presidente\"))\n\n# Crea una base de datos a partir de esas informaciones\nnm &lt;- names(ft)\nxx &lt;- data.frame()\nfor(i in 1:length(nm)){\n  xx &lt;- rbind(xx, data.frame(\n                          Presidente=nm[i], \n                          Palabras=names(ft[[i]]), \n                          Freq=as.numeric(ft[[i]]),\n                          Freq.Rel=round(as.numeric(ftg[[i]]),3)))\n}\n\n# Visualiza\nlibrary(htmltools)\n\n# Render a bar chart with a label on the left\nbar_chart &lt;- function(label, width = \"100%\", height = \"1rem\", fill = \"#00bfc4\", background = NULL) {\n  bar &lt;- div(style = list(background = fill, width = width, height = height))\n  chart &lt;- div(style = list(flexGrow = 1, marginLeft = \"0.5rem\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\nlibrary(reactable)\n\nreactable(\n  xx,\n  filterable = T,\n  columns = list(\n    Presidente=colDef(name=\"Presidente\"),\n    Freq = colDef(name = \"Frecuencia\", align = \"left\", cell = function(value) {\n      width &lt;- paste0(value / max(xx$Freq) * 100, \"%\")\n      bar_chart(value, width = width)\n    }),\n    Freq.Rel = colDef(name = \"Frec. Relativa\", align = \"left\", cell = function(value) {\n      width &lt;- paste0(value / max(xx$Freq.Rel) * 100, \"%\")\n      bar_chart(value, width = width, fill=\"red\")\n    })\n  )\n)\n\n\n\n\n\n\nPodemos ver en la tabla resultante que una misma expresi√≥n puede tener pesos distintos en los discursos de diferentes presidentes. Por ejemplo, el t√©rmino pol√≠tica tiene un peso de 0.012 en el discurso de Calvo-Sotelo, pero una incidencia seis veces menor en el de Pedro S√°nchez. Algo parecido sucede con la palabra se√±or√≠as, mucho m√°s com√∫n en los documentos de S√°nchez si comparados con los de Felipe Gonz√°lez.\n\n\nRa√≠ces (stemming)\nNo obstante, muchas de las palabras que aparecen en la tabla comparten una misma ra√≠z como, por ejemplo, Espa√±a, espa√±oles, espa√±olas o econ√≥mico, econ√≥mica o econ√≥micas. Contarlas de forma separada, en realidad, puede fragmentar o ocultar un patr√≥n o un tema m√°s relevante bajo un sinf√≠n de peque√±as variantes de un mismo concepto.\nEn esos casos, una t√©cnica muy √∫til es la conversi√≥n de las palabras a sus ra√≠ces (stemming). Este procedimiento sencillo permite justamente evitar que matices entre t√©rminos impidan la identificaci√≥n de un patr√≥n claro dentro del corpus o en algunos de sus textos componentes.\nLa funci√≥n dfm_wordstem extrae la raiz de los t√©rminos de una matriz de frecuencia. Su empleo es muy sencillo, sin embargo, se debe establecer la lengua adecuada de los textos del corpus para que la transformaci√≥n funcione. La funci√≥n establece el ingl√©s por defecto. En nuestro ejemplo, definiremos el par√°metro language como ‚Äúes‚Äù para definir que se trata de espa√±ol.\n\n\nCode\n# Convierte las palabras a sus raices\nfw &lt;- dfm_wordstem(fm, language = \"es\")\n\n# Buscamos las 25 palabras m√°s frecuentes\ntopfeatures(fw, n = 25)\n\n\n    polit   gobiern     se√±or     espa√±    econom    social   espa√±ol      pais \n      797       780       766       653       593       535       535       502 \n      deb       hac    public      part     mejor      nuev     comun  ciudadan \n      460       454       434       413       408       370       368       332 \n     pued desarroll   autonom     mayor     emple    socied    derech    sistem \n      324       315       309       304       303       302       291       289 \n   reform \n      287 \n\n\nLos resultados reducen la variedad, pol√≠tico, pol√≠tica, pol√≠ticas se transforman en polit. Gobierno, gobierna, gobiernan en gobiern. Sin embargo, los resultados pueden mejorar. Espa√±a est√° de un lado como espa√±, mientras que espa√±ol, espa√±oles y espa√±olas se reducen a espa√±ol. Sin embargo, el investigador siempre puede utilizar esos resultados como punto de partida y, en seguida, agregar o corregir lo que crea necesario.\n\n\nN-gramas\nEn varias ocasiones conviene explorar la combinaci√≥n de palabras en b√∫squeda de expresiones comunes o recurrente. Algunos ejemplos claros en la pol√≠tica son ‚Äúseguridad social‚Äù, ‚Äúfuerzas armadas‚Äù, ‚Äúpol√≠ticas p√∫blicas‚Äù, ‚Äúseguridad ciudadana‚Äù, ‚Äúpartido pol√≠tico‚Äù, entre otras. Para ello, utilizamos n-gramas, que son secuencias de n-palabras seguidas. Se llaman as√≠ porque pueden ser dos (bigramas), tres (trigramas) o m√°s t√©rminos sucesivos.\nEn R, se trata de transformar los tokens en n-gramas utilizando la funci√≥n tokens_ngrams y, a continuaci√≥n, calcular las frecuencias:\n\n\nCode\n# Convierte los tokens en bigramas\ntn &lt;- tokens_ngrams(tk, n=2)\n\n# Crea una matriz de frecuencia\nfk &lt;- dfm(tn)\n\n# Extrae los 25 mas comunes\ntopfeatures(fk, 25)\n\n\n    comunidades_aut√≥nomas             uni√≥n_europea           partido_popular \n                      108                        73                        72 \n        pol√≠tica_exterior               cuatro_a√±os               punto_vista \n                       61                        60                        52 \n          se√±oras_se√±ores         se√±ores_diputados         sociedad_espa√±ola \n                       51                        49                        49 \n       pol√≠tica_econ√≥mica             pr√≥ximos_a√±os          seguridad_social \n                       45                        42                        42 \n                 cada_vez           creaci√≥n_empleo          confianza_c√°mara \n                       41                        41                        40 \n         se√±or_presidente           acci√≥n_gobierno         fuerzas_pol√≠ticas \n                       39                        39                        38 \n             √∫ltimos_a√±os              primer_lugar administraciones_p√∫blicas \n                       38                        37                        37 \n                 debe_ser     formaci√≥n_profesional                si_obtengo \n                       35                        34                        34 \n       servicios_p√∫blicos \n                       34 \n\n\nVarios bigramas interesantes saltan a la vista: comunidades aut√≥nomas, uni√≥n europea, pol√≠tica exterior, pol√≠tica econ√≥mica, seguridad social, creaci√≥n empleo, entre otros. Tambi√©n aparecen f√≥rmulas ret√≥ricas como se√±oras se√±ores, si obtengo, confianza c√°mara, por ejemplo.\nPodemos repetir el mismo procedimiento, pero ahora utilizando tres palabras en lugar de dos para ver qu√© resultados obtenemos. Este juego de ir subiendo el n√∫mero de palabras en la expresi√≥n puede seguir indefinidamente hasta que no aporte ning√∫n dato nuevo o interesante.\n\n\nCode\n# Convierte los tokens en trigramas\ntn &lt;- tokens_ngrams(tk, n=3)\n\n# Crea una matriz de frecuencia\nfk &lt;- dfm(tn)\n\n# Extrae los 25 mas comunes\ntopfeatures(fk, 25)\n\n\n          se√±oras_se√±ores_diputados                si_obtengo_confianza \n                                 48                                  23 \n               pr√≥ximos_cuatro_a√±os             producto_interior_bruto \n                                 20                                  19 \n           obtengo_confianza_c√°mara                 √∫ltimos_cuatro_a√±os \n                                 18                                  18 \n                partido_popular_vox         comunidad_econ√≥mica_europea \n                                 16                                  15 \n          sistema_p√∫blico_pensiones    legislatura_discurso_investidura \n                                 13                                  12 \n           se√±or_presidente_se√±oras          presidente_se√±oras_se√±ores \n                                 12                                  12 \n          solicito_confianza_c√°mara                    idea_espa√±a_pa√≠s \n                                 11                                  11 \n           art√≠culo_99_constituci√≥n              sistema_nacional_salud \n                                 10                                  10 \n            todas_fuerzas_pol√≠ticas           fuerzas_cuerpos_seguridad \n                                  9                                   9 \n           creaci√≥n_puestos_trabajo              si_obtengo_investidura \n                                  9                                   9 \n           espa√±a_necesita_gobierno            sistema_seguridad_social \n                                  9                                   8 \n                gobierno_si_obtengo comunidades_aut√≥nomas_ayuntamientos \n                                  8                                   8 \n         se√±ora_presidenta_se√±or√≠as \n                                  8 \n\n\nLa obtenci√≥n de la confianza del parlamento aparece en m√°s de una vez. El art√≠culo 99 de la Constituci√≥n espa√±ola (que define el proceso de voto de confianza en el Presidente) resulta la novedad m√°s clara en ese apartado. No obstante, otros t√©rminos relacionados a las pol√≠ticas p√∫blicas -como producto interior bruto, sistema p√∫blico pensiones, fuerzas cuerpos seguridad- o a la organizaci√≥n administrativa del Estado -comunidades aut√≥nomas ayuntamientos o todas administraciones p√∫blicas- tambi√©n se destacan.\nSi llegamos a aumentar el n√∫mero a 5, por ejemplo, aparece el I+D+I. En resumen, se trata de un recurso exploratorio bastante interesante para determinar qu√© expresiones compuestas o frases aparecen de forma repetitiva en los textos y que puedan incitar nuevas perspectivas sobre el contenido de los mismos.\n\n\nTF-IDF\nOtro m√©todo de selecci√≥n de t√©rminos relevantes es llamado Term Frequency-Inverse Document Frequency (TF-IDF). La f√≥rmula es intuitiva y premia aquellos casos que aparecen con mucha frecuencia, pero en relativamente pocos documentos, al mismo tiempo que penaliza los que est√°n por todas partes:\n\\[tf/idf = freq_{td} * log(\\frac{D}{d_t}) \\]\nDonde:\nfreqtd es la frecuencia absoluta (o relativa) del t√©rmino t en el cada documento d.\nD corresponde al n√∫mero total de documentos.\ndt representa el n√∫mero de documentos que contienen el t√©rmino t.\nImaginemos un corpus con 10 documentos y dos palabras ‚Äúla‚Äù y ‚Äúpobreza‚Äù, ambas con una frecuencia de 20. La √∫nica diferencia es que ‚Äúla‚Äù aparece en todos los 10 documentos con una frecuencia de 2 en cada uno, mientras que ‚Äúpobreza‚Äù se menciona en solamente dos textos, uno 14 veces y otro 6. Al calcular el tf-idf para cada una, tenemos los siguientes resultados:\n\nPara cada uno de los documentos de ‚Äúla‚Äù: 2*log(10/10) = 0\nPara el primer documento de ‚Äúpobreza‚Äù: 14*log(10/2) = 22,5\nPara el segundo documento de ‚Äúpobreza‚Äù: 6*log(10/2) = 9,7\n\nAl final, se observa que el peso de ‚Äúla‚Äù se anula completamente (tf-idf = 0) tanto por la dispersi√≥n de la frecuencia total como por su aparici√≥n en muchos documentos. Lo inverso ocurre con ‚Äúpobreza‚Äù, que tiene su frecuencia concentrada en dos textos y con mayor preponderancia en uno en concreto (un tf-idf total de 22,5 + 9,7 = 32,2).\nEn l√≠neas generales permite identificar aquellas palabras que aparecen mucho, pero que no tanto para reducir su poder informativo. Por lo tanto se trata de un indicador de frecuencia ponderada por la concentraci√≥n. Por ejemplo, ‚Äúla‚Äù, ‚Äúde‚Äù, ‚Äúel‚Äù o ‚Äúser‚Äù, ‚Äúhacer‚Äù aparecen un n√∫mero elevado de veces. Esta medida permite ponderar su peso por un factor que penaliza el hecho de aparezcan mucho en todos los documentos. El resultado son indicadores m√°s elevados para conceptos que se destacan sin ser preponderantes o muy comunes en todos los elementos del corpus.\n\n\nCode\n# Convierte las palabras a sus raices\nfw &lt;- dfm_tfidf(fm)\n\n# Buscamos las 25 palabras m√°s frecuentes\ntopfeatures(fw, n = 25)\n\n\n          vox         vamos   progresista       ustedes  consiguiente \n     26.49064      22.90486      22.22660      21.90034      20.02053 \n      derecha     coalici√≥n       digital  ultraderecha       popular \n     19.86798      18.31666      17.68025      17.44797      17.35020 \n      ejemplo         euros            ss         vista        g√©nero \n     16.87403      16.51501      16.16480      15.99216      14.90891 \n   presidenta             `    transici√≥n        se√±ora   l√≥gicamente \n     14.64935      14.44944      14.36088      14.14420      13.63905 \n         solo      amnist√≠a     ecol√≥gica         pacto reaccionarias \n     13.28381      13.24532      13.08598      12.20455      12.04120 \n\n\nVemos que otros t√©rminos aparecen: digital, progresista, coalici√≥n, euros, g√©nero, transici√≥n, pacto, comunitaria, ecol√≥gica, pobreza y 2030. Tales t√©rminos sugieren contenido program√°tico de la pol√≠tica y despiertan mayor inter√©s que el lenguaje m√°s formal que hemos visto hasta ahora. Por otra parte, verbos y expresiones muy peculiares de cada presidente, como el vamos de Pedro S√°nchez o el consiguiente de Felipe Gonz√°lez, saltan a la vista. Tales ejemplos evidencian c√≥mo la medida TF-IDF puede ayudar a singularizar el discurso de un presidente o de un partido pol√≠tico tanto por el contenido pol√≠tico que por las f√≥rmulas ling√º√≠sticas empleadas para dirigirse a los miembros del Poder Legislativo. Adem√°s, como en los ejemplos anteriores, se pueden detallar los resultados por grupo (Presidente, partido, ideolog√≠a, entre otras variables de contexto disponibles).\n\n\nKeyness\nEl keyness es otro m√©todo que compara la distribuci√≥n desigual de t√©rminos entre textos. A partir de un texto de referencia, utiliza m√©todos estad√≠sticos como el chi-cuadrado o la likelihood ratio para determinar cu√°les palabras se acercan m√°s a un documento y las que menos. A partir de esas informaciones podemos encontrar elementos √∫tiles para caracterizar un discurso concreto.\nla funci√≥n textstat_keyness del paquete quanteda.textstats permite calcular el keyness de los t√©rminos de un corpus con relaci√≥n a un documento de referencia concreto. Utilicemos, por ejemplo, los discursos de Pedro S√°nchez como referencia:\n\n\nCode\n# Nueva matriz de fecuencia\npfm &lt;- dfm(tk)\n\n# Atribuimos el nombre del presidente como grupo\npfm &lt;- dfm_group(pfm, groups = quanteda::docvars(pfm, \"Presidente\"))\n\n# Calcula el keyness\nkn &lt;- textstat_keyness(pfm, target = \"Pedro S√°nchez\")\n\n# Visualiza los resultados en una tabla\nreactable(kn, \n          columns = list(\n                    chi2=colDef(\n                            format=colFormat(\n                            digits=2)),\n                    p=colDef(\n                            format=colFormat(\n                            digits=2))))\n\n\n\n\n\n\nVemos que las palabras que m√°s se asocian al discurso de S√°nchez son vamos, se√±or√≠as, progresista, digital, avanzar y g√©nero. Las que menos son pol√≠tica, econ√≥mica, esfuerzo, ciudadanos, proceso, exterior y cooperaci√≥n. Podemos tambi√©n compararlas visualmente utilizando la funci√≥n textplot_keyness del paquete quanteda.textplots. En el gr√°fico abajo, se seleccionan las 20 palabras que m√°s y menos caracterizan los textos de Pedro S√°nchez.\n\n\nCode\nlibrary(quanteda.textplots)\n\ntextplot_keyness(kn, color = c(\"red3\",\"blue\"))\n\n\n\n\n\n\n\n\n\nComo se trata de un gr√°fico basado en la arquitectura ggplot2, se pueden a√±adir elementos como t√≠tulos, temas, nuevos colores y otros elementos visuales.\n\n\nRatio de probabilidades\nEl ratio de probabilidades representa otra forma de visualizar la importancia de determinadas palabras para un texto concreto en el corpus. Este m√©todo compara la frecuencia relativa de una palabra en un texto con su incidencia en todos los documentos. Una odds ratio nos informa la especificidad de cada palabra comparando un texto (o conjunto de textos) frente a los dem√°s. Como suelen existir casos extremos (frecuencias o ratio muy elevadas), se emplean el logaritmo de las ratios para representar los valores. Los valores resultantes pueden ser tanto positivos como negativos. Un valor positivo indica que la palabra es m√°s frecuente en el texto de referencia que en el resto del corpus. Un valor negativo indica justo lo contrario.\nPor ejemplo, en el gr√°fico abajo comparamos los discursos de Pedro S√°nchez con los dem√°s presidentes de gobierno espa√±oles. Vemos que las palabras que m√°s se asocian a S√°nchez son vamos, se√±or√≠as, progresista, digital, avanzar y g√©nero. Las que menos son pol√≠tica, econ√≥mica, esfuerzo, ciudadanos, proceso, exterior y cooperaci√≥n.\n\n\nCode\ncp &lt;- corpus(spa.inaugural)\n\nci &lt;- corpus_group(cp, groups = President)\n\nplotLogOddsRatio(ci, ref.cat=\"Rajoy\")\n\n\n\n\n\n\n\n\nLa estructura del gr√°fico sigue el mismo formato de un diagrama de dispersi√≥n tradicional con dos variables continuas. En el eje horizontal (x) se representa el logaritmo de la frecuencia de las palabras. En el vertical (y), el logaritmo de la odds ratio. Las palabras que m√°s se asocian al discurso de S√°nchez son las que se encuentran m√°s a la arriba del gr√°fico. Las que menos, m√°s abajo. Por otra parte, los t√©rminos situados m√°s a la izquierda son menos frecuentes que los de la derecha. El tama√±o de los puntos representa la frecuencia de las palabras en el corpus.",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "basico.html#asociaci√≥n-entre-palabras",
    "href": "basico.html#asociaci√≥n-entre-palabras",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "Asociaci√≥n entre palabras",
    "text": "Asociaci√≥n entre palabras\n\nCo-ocurrencias\nEl primer m√©todo de an√°lisis de la asociaci√≥n entre palabras explora la cantidad de veces en que dos palabras aparecen juntas en un corpus. Este fen√≥meno se denomina co-ocurrencia. Se calcula a partir de la funci√≥n fcm de quanteda que genera una matriz de co-ocurrencia. B√°sicamente, se trata de una matriz NxN, donde N corresponde al n√∫mero de palabras en el corpus.\n\n\nCode\n# Crea una matriz de co-ocurrencia\nfc &lt;- fcm(tk)\n\n# Selecciona las 50 co-ocurrencias mas frecuentes\nfeat &lt;- colSums(fc) |&gt;\n  sort(decreasing = TRUE) |&gt;\n  head(50) |&gt;\n  names()\n\nfc &lt;- fcm_select(fc, pattern = feat) \n\n# carga el paquete\nlibrary(quanteda.textplots)\n\n# genera la red\nlibrary(ggplot2)\nset.seed(pi)\ntextplot_network(fc, \n                 edge_color = \"red\", \n                 edge_alpha = 0.05, omit_isolated = T)\n\n\n\n\n\n\n\n\n\nEl c√≥digo m√°s arriba calcula la matriz de co-ocurrencia para el corpus de los discursos de investidura de los presidentes de gobierno de Espa√±a y selecciona los 50 pares de t√©rminos m√°s frecuentes. Con esos datos, genera un sociograma que representa las asociaciones m√°s comunes entre palabras. El grosor de los v√≠nculos revela la intensidad de su asociaci√≥n y la centralidad de los nodos su peso o importancia en el conjunto de elementos seleccionados.\nComo podemos observar, Espa√±a aparece en el centro, seguida por se√±or√≠as, ley, vamos, partido y sistema. La red posee un n√∫cleo m√°s denso de palabras interconectadas entre s√≠ y otro conjunto de t√©rminos perif√©ricos, con escasos v√≠nculos con este centro de la red.\n\n\nCo-localizaciones\nUn m√©todo adicional para el an√°lisis de los v√≠nculos entre t√©rminos es la co-localizaci√≥n. A diferencia de las co-ocurrencias, que se basan exclusivamente en las frecuencias, la funci√≥n textstat_collocations del paquete quanteda.textstats utiliza un modelo log-linear para comparar la incidencia de un grupo de palabras y definir su grado de asociaci√≥n. El coeficiente lambda (\\(\\lambda\\)) representa dicha estimaci√≥n.\nLa co-localizaci√≥n define el grado de asociaci√≥n de otra palabra cerca. As√≠ se puede precedir qu√© palabra viene despu√©s a partir del conjunto que viene antes.\n\n\nCode\n# Genera una lista de 2 palabras que aparecen en secuencia\ncc &lt;- textstat_collocations(tk, size = 2)\n\nreactable(cc, \n          resizable=T, \n          rownames = F, \n          columns = list(\n                        lambda=colDef(format=colFormat(digits=2)),\n                        z=colDef(format=colFormat(digits = 2))))\n\n\n\n\n\n\n\n\nCorrelaci√≥n\nLa correlaci√≥n corresponde a un m√©todo cl√°sico de medir la asociaci√≥n entre dos variables. Cuando se trata de la correlaci√≥n entre t√©rminos podemos utilizar diferentes algoritmos. Silge y Robinson (2017), por ejemplo, emplean el coeficiente phi (\\(\\phi\\)) de Yule, un m√©todo de asociaci√≥n a partir de la coincidencia binaria (1/0, S√≠/No) entre palabras en un mismo documento. Como el rho (\\(\\rho\\)) de Pearson, posee un intervalo entre -1 y 1 y se interpreta del mismo modo.\nEsta medida puede resultar √∫til para textos cortos, como tweets o un corpus organizado seg√∫n sentencias. En esos casos, importa menos la cantidad de las palabras que el hecho de que ambas aparezcan en un mismo documento. Se valora la coincidencia de dos conceptos o ideas y tiene poco sentido evaluar su intensidad. La probabilidad de encontrar una palabra con frecuencia superior a 1 en una misma frase u oraci√≥n es peque√±a.\nNo obstante, ese razonamiento tambi√©n revela la principal limitaci√≥n del coeficiente phi. Al tratarse de un test binario, no lleva en cuenta diferencias cuantitativas que pueden observarse en documentos m√°s extensos como libros, cap√≠tulos, entrevistas, leyes, manifiestos o discursos. En estos casos, se requieren m√©todos m√°s precisos que, adem√°s de la presencia o ausencia de los t√©rminos, ponderen la intensidad de asociaci√≥n seg√∫n su frecuencia o rango.\nLa correlaci√≥n de orden de rango (rank-order correlation) o rho (\\(\\rho\\)) de Spearman resulta m√°s indicada para esos casos. Se trata de una medida que ordena los documentos seg√∫n el rango de frecuencia de cada palabra en concreto y compara el grado de similitud o diferencia entre los rangos. La f√≥rmula es la siguiente:\n\\[S\\rho = 1-\\frac{6 \\sum R(X_i)-R(Yi)}{n(n^2-1)}\\]\nDonde:\n\nR(Xi) indica el rango (ranking) de Xi en los valores de X.\nR(Yi) indica el rango (ranking) de Yi en los valores de Y.\nn corresponde al n√∫mero de observaciones. En nuestro caso, indica el n√∫mero de documentos en el corpus.\n\n\nImportantePuesto que la distribuci√≥n de las frecuencias de palabras no es normal, no se recomienda el empleo del rho (\\(\\rho\\)) de Pearson para avaliar su asociaci√≥n. Tampoco resulta indicable el c√°lculo de la correlaci√≥n para un corpus con un n√∫mero muy reducido de documentos (menos de 10, por ejemplo). En tales escenarios, quiz√°s ser√≠a mejor reestructurar el corpus seg√∫n unidades menores -como sentencias o p√°rrafos, por ejemplo- y emplear el phi (\\(\\phi\\)) como alternativa.\n\n\nEl c√≥digo abajo crea una lista con nodos correspondientes a las palabras del corpus y v√≠nculos que expresan la intensidad de su asociaci√≥n y un sociograma representando la asociaci√≥n entre las palabras. Por defecto, el m√©todo empleado es el rho de Spearman.\n\n\nCode\n# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea una lista de correlaciones\nll &lt;- corTerms(cp, \n               min.freq = 100, \n               n.terms = 100)\n\n# Genera el sociograma\ncorNet(ll)\n\n\n\n\n\n\n\n\n\nCuanto m√°s grandes los puntos, mayor la frecuencia de la palabra en el corpus. El grosor del v√≠nculo revela la intensidad de asociaci√≥n y el color su direcci√≥n. En el ejemplo arriba, correlaciones negativas se representan en rojo y positivas en azul. De ese modo, vemos que pa√≠ses y compromiso se relacionan de forma negativa. Fuerzas y social presentan una correlaci√≥n positiva.",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "basico.html#consideraciones-finales",
    "href": "basico.html#consideraciones-finales",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "Consideraciones finales",
    "text": "Consideraciones finales\nEn este documento hemos visto c√≥mo abrir los textos en R y trabajar con distintas t√©cnicas de an√°lisis exploratorio. Nos hemos concentrado en m√©todos inductivos, sin una lectura anterior y profunda que orientara el an√°lisis. Los ejemplos se han concentrado fundamentalmente en entender qu√© t√©rminos ocurren con mayor frecuencia y c√≥mo se asocian entre ellos.\nEste tipo de an√°lisis dista mucho de ser m√≠nimamente aceptable dentro de una perspectiva cualitativista pura. Contar palabras constituye una aproximaci√≥n somera al an√°lisis de textos. No obstante, no tiene el prop√≥sito de reemplazar nada. Su utilidad reside en suministrar recursos y una primera aproximaci√≥n a t√©cnicas m√°s profundas y sofisticadas. Los t√©rminos encontrados aqu√≠ sirven para la creaci√≥n de diccionarios y la codificaci√≥n tem√°tica. Tambi√©n ayudan a desvelar patrones no completamente observables desde una perspectiva cualitativa. Adem√°s, su poder reside en permitir extraer patrones de amplios vol√∫menes de texto, imposibles de leer uno a uno.\nEn la pr√≥xima sesi√≥n utilizaremos t√©cnicas deductivas y otros m√©todos exploratorios para profundizar en el conocimiento de los textos. Trataremos de la codificaci√≥n tem√°tica, la selecci√≥n de textos seg√∫n t√©rminos o atributos para un an√°lisis m√°s detallado y la creaci√≥n de diccionarios como base para tareas de clasificaci√≥n y descubierta.",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "basico.html#ejercicios",
    "href": "basico.html#ejercicios",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "Ejercicios",
    "text": "Ejercicios\nEjercicio 1. Utilice el data.frame cis.corrupt del paquete tenet para crear un nuevo corpus. Realice el an√°lisis del nuevo corpus utilizando la funci√≥n summary. Guarde el resultado de summary en un data.frame llamado d.\n\n\nSoluci√≥n\n# Carga los paquetes tenet y quanteda\nlibrary(tenet)\nlibrary(quanteda)\n\n# Convierte cis.corrupt en un corpus\ncx &lt;- corpus(cis.corrupt)\n\n# Resume los resultados\nd &lt;- summary(cx, n = nrow(cis.corrupt))\n\n\nEjercicio 2. A√±ada una variable de documentaci√≥n (docvars) al corpus reci√©n creado con la densidad o la diversidad l√©xica de cada texto dividiendo el n√∫mero de types por tokens y multiplicando por 100. Formalmente, este t√©rmino se denomina Type-Token Ratio (TTR). Tambi√©n a√±ada otras dos variables a la documentaci√≥n del corpus: (a) el n√∫mero de palabras por sentencia y (b) el n√∫mero de types por sentencia.\n\n\nSoluci√≥n\n# Calcula el TTR\nd$TTR &lt;- round((d$Types/d$Tokens)*100,1)\n\n# Calcula el n√∫mero de palabras por frase\nd$tokens_sentence &lt;- round(d$Tokens/d$Sentences,1)\n\n# Calcula el n√∫mero de tipos por frase\nd$types_sentence &lt;- round(d$Types/d$Sentences,1)\n\n# A√±ade las tres variables como documentaci√≥n del corpus\ndocvars(cx,\"TTR\") &lt;- d$TTR\ndocvars(cx,\"tokens_sentence\") &lt;- d$tokens_sentence\ndocvars(cx,\"types_sentence\") &lt;- d$types_sentence\n\n\nEjercicio 3. Divida el corpus en tokens bajo la forma de palabras. Excluyas puntuaci√≥n, s√≠mbolos y palabras vac√≠as (stop words). Cree un nuevo objeto con bi-gramas en lugar de solo palabras como tokens. Finalmente, genere una nueva lista de t√©rminos, pero ahora con solamente las ra√≠ces.\n\n\nSoluci√≥n\n# Crea los tokens removiento puntuaci√≥n y s√≠mbolos\ntk &lt;- tokens(cx,\n             remove_punct = T,\n             remove_symbols = T)\n\n# Elimina los stop words\ntk &lt;- tokens_remove(tk, \n                    stopwords(\"es\"))\n\n# Convierte en bi-gramas\ntb &lt;- tokens_ngrams(tk, 2)\n\n# Convierte en ra√≠ces\ntr &lt;- tokens_wordstem(tk, language = \"es\")\n\n\nEjercicio 4. Crea dos matrices de frecuencia: (a) una con las palabras y (b) otra con sus ra√≠ces. Selecciona las 30 palabras m√°s frecuentes en cada una de ellas.\n\n\nSoluci√≥n\n# Crea la matriz de frecuencia para las palabras\nfm &lt;- dfm(tk, tolower = T)\n\n# Crea la matriz de frecuencia para las ra√≠ces\nfr &lt;- dfm(tr, tolower = T)\n\n# Selecciona las 30 palabras m√°s frecuentes\ntopfeatures(fm, 30)\n\n# Selecciona las 30 ra√≠ces m√°s frecuentes\ntopfeatures(fr, 30)\n\n\nEjercicio 5. Crea una matriz de frecuencia para cada grupo demogr√°fico (variable ‚ÄúGrupo.Demografico‚Äù en docvars) y selecciona las 10 palabras m√°s comunes para cada uno de ellos.\n\n\nSoluci√≥n\n# Agrupa la matriz por grupo demogr√°fico\nfg &lt;- dfm_group(fm, groups = docvars(cx, \"Grupo.Demografico\"))\n\n# Selecciona los 10 m√°s frecuentes\ntopfeatures(fg, groups = docvars(fg, \"Grupo.Demografico\"))\n\n\nEjercicio 6. Ahora, utilice la matriz agrupada del ejercicio 5, calcule los TF-IDF de cada palabra y seleccione las 20 palabras m√°s importantes para cada grupo.\n\n\nSoluci√≥n\n# Calcula el TF-IDF\nfi &lt;- dfm_tfidf(fg)\n\n# Selecciona las 20 m√°s frecuentes\ntopfeatures(fi,\n            n = 20,\n            groups = docvars(fg, \"Grupo.Demografico\"))\n\n\nEjercicio 7. Utilice la matriz de frecuencia de los grupos demogr√°ficos para calcular el keyness del corpus (con la funci√≥n textstat_keyness) y crea el gr√°fico utilizando la funci√≥n texplot_keyness. Utilice como referencia el grupo ‚ÄúObreros‚Äù.\n\n\nSoluci√≥n\n# Calcula el keyness\nkn &lt;- textstat_keyness(fg, target = \"Obreros\")\n\n# Carga el paquete\nlibrary(quanteda.textplots)\n\n# Genera el gr√°fico\ntextplot_keyness(kn, color = c(\"red3\",\"blue\"))\n\n\nEjercicio 8. Cree una matriz de co-ocurrencia y, luego, genere el sociograma de la asociaci√≥n de las 25 co-ocurrencias m√°s frecuentes.\n\n\nSoluci√≥n\n# Crea una matriz de co-ocurrencia\nfc &lt;- fcm(tk)\n\n# Selecciona las 25 co-ocurrencias mas frecuentes\nfeat &lt;- names(topfeatures(fc, 25))\n\nfc &lt;- fcm_select(fc, pattern = feat) \n\n# carga el paquete\nlibrary(quanteda.textplots)\n\n# genera la red\nlibrary(ggplot2)\nset.seed(pi)\ntextplot_network(fc, \n                 edge_color = \"blue\", \n                 edge_alpha = 0.05)\n\n\nEjercicio 9. Crea el gr√°fico de correlaciones para el corpus a partir de 100 palabras con frecuencia superior a 50 en el corpus.\n\n\nSoluci√≥n\n# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea una lista de correlaciones\nll &lt;- corTerms(cx,\n               min.freq = 50, \n               n.terms = 100)\n\n# Genera el sociograma\ncorNet(ll)",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "basico.html#lecturas-adicionales",
    "href": "basico.html#lecturas-adicionales",
    "title": "Fundamentos de an√°lisis de textos",
    "section": "Lecturas adicionales",
    "text": "Lecturas adicionales\n\n\nIgnatow G, Mihalcea R (2018). ‚ÄúThe Philosophy and Logic of Text Mining.‚Äù In An Introduction to Text Mining: Research Design, Data Collection, and Analysis, chapter 4. SAGE, Los Angeles.\n\n\nEste breve cap√≠tulo del manual de Ignatow y Mihalcea trata de las distintas perspectivas de exploraci√≥n de textos en las ciencias sociales. Trata de las principales corrientes epistemol√≥gicas, las dos culturas (‚Äúcuantitativa‚Äù y ‚Äúcualitativa‚Äù) y describe las caracter√≠sticas y principales diferencias entre las l√≥gicas deductiva, inductiva y abductiva.\n\n\nBenoit K (2020). ‚ÄúText as data: An overview.‚Äù In The SAGE Handbook of Research Methods in Political Science and International Relations, chapter 26, 461. SAGE, Thousand Oaks. Publisher: SAGE Publishing Ltd Thousand Oaks.\n\n\nSe trata de otro excelente texto para clasificar las perspectivas (ahora desde la ciencia pol√≠tica) sobre el an√°lisis de texto. Propone una aclaraci√≥n conceptual importante: un texto considerado como un conjunto de datos no puede ser confundido con un texto en su integridad. La conversi√≥n a datos supone transformaciones, abstracci√≥n y cierto grado de reducci√≥n. Sobre todo, deja claro qu√© NO debemos esperar de esa perspectiva de an√°lisis para evitar frustraciones o inferencias inadecuadas.\n\n\nGrimmer J, Roberts ME, Stewart BM (2022). ‚ÄúBag of Words.‚Äù In Text as Data: A New Framework for Machine Learning and the Social Sciences, chapter 5, 94-111. Princeton University Press, New Haven.\n\n\nEl texto de Grimmer, Roberts y Stewart representa una buena introducci√≥n a los principales conceptos tratados en esta secci√≥n. Introducen el an√°lisis de tipo ‚Äúsaco de palabras‚Äù (bag of words) y explican los procesos de su implementaci√≥n como la tokenizaci√≥n, la remoci√≥n de palabras vac√≠as (stop words), la reducci√≥n a lemas o ra√≠ces y la creaci√≥n de matrices de frecuencias.\n\n\nSilge J, Robinson D (2017). ‚Äú‚ÄòAnalyzing Word & Document Frequency - tf-idf‚Äô y ‚ÄòRelationships between words: n-grams and correlations‚Äô.‚Äù In Text Mining with R: A Tidy Approach, chapter 4 and 5. O‚ÄôReilly Media, London. https://www.tidytextmining.com/.\n\n\nFinalmente, los dos cap√≠tulos de Silge y Robinson explican m√°s detenidamente y de forma pr√°ctica varios an√°lisis realizados en esta secci√≥n. Frecuencia de palabras, TF-IDF, la ley de Zipf (muy importante para comprender las frecuencias de palabras), as√≠ como los N-gramas y las correlaciones entre palabras.",
    "crumbs": [
      "An√°lisis b√°sicos"
    ]
  },
  {
    "objectID": "codificacion.html#introducci√≥n",
    "href": "codificacion.html#introducci√≥n",
    "title": "Codificaci√≥n tem√°tica",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\n¬øCu√°l es la posici√≥n de los partidos con relaci√≥n a la reducci√≥n de impuestos? ¬øCu√°les mencionan la pobreza o el aumento de la desigualdad en sus manifiestos? ¬øEn qu√© textos aparece el tema de la inmigraci√≥n como amenaza a la integridad social o pol√≠tica del pa√≠s? ¬øCu√°les diputados intervienen durante los plenos en favor de pol√≠ticas que reduzcan las emisiones y frenen el cambio clim√°tico? Cada una de esas preguntas encierra un conjunto de opciones claras en t√©rminos emp√≠ricos, te√≥ricos y metodol√≥gicos. Primero, se basan en el contenido textual como fuente de informaci√≥n emp√≠rica. Segundo, establecen la comparaci√≥n como m√©todo, buscando diferencias significativas seg√∫n partido o ideolog√≠a. Finalmente, los partidos o diputados se dividen en grupos que hipot√©ticamente se antagonizan ante ciertas pol√≠ticas consideradas clave. Existe un comportamiento esperado que se puede o no confirmar a partir de an√°lisis del material seleccionado.\nA diferencia de lo que se hizo en el an√°lisis inductivo, ya no se trata de ver aqu√≠ qu√© palabras o t√©rminos aparecen en un conjunto de documentos, sino de buscar c√≥mo temas concretos se manifiestan, por qui√©nes, d√≥nde y en qu√© contexto. El an√°lisis exploratorio deductivo nos permite evaluar la prevalencia de un tema en los textos, verificar si su distribuci√≥n resulta uniforme o se concentra de acuerdo con la ideolog√≠a, el partido pol√≠tico o un momento hist√≥rico concreto.\nEn esta parte del trabajo revisaremos los instrumentos y estrategias disponibles para el desarrollo de temas. Examinaremos diferentes herramientas para determinar el contexto en el que se inscriben, as√≠ como determinaremos la prevalencia de distintas categor√≠as anal√≠ticas. Haremos especial hincapi√© en el concepto de diccionario o l√©xico como el resultado de un proceso de codificaci√≥n tem√°tica y construcci√≥n te√≥rica a partir del an√°lisis abductivo resultante de la consulta e iteraci√≥n constante entre texto (como material emp√≠rico fundamental), teor√≠a e interpretaci√≥n (Thompson 2022; B. L. Kennedy and Thornberg 2018).\nEl texto siguiente se divide en tres sesiones. El segundo apartado se dedica a las t√©cnicas de b√∫squeda de palabras en los textos y que resultan muy √∫tiles para situar las ideas en su contexto. La tercera parte trata de la codificaci√≥n tem√°tica a partir de diccionarios. C√≥mo la construcci√≥n y refinamiento de l√©xicos puede resultar de un proceso iteractivo que da lugar al desarrollo de nuevas categor√≠as anal√≠ticas. Finalmente, el cuarto apartado explora los temas para identificar patrones, el grado de asociaci√≥n entre ellos y su distribuci√≥n seg√∫n distintas categor√≠as anal√≠ticas (como partidos o ideolog√≠a, por ejemplo).",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "codificacion.html#b√∫squeda-de-palabras",
    "href": "codificacion.html#b√∫squeda-de-palabras",
    "title": "Codificaci√≥n tem√°tica",
    "section": "B√∫squeda de palabras",
    "text": "B√∫squeda de palabras\nEn el apartado sobre el m√©todo inductivo aprendimos a contar palabras. No obstante, en muchas ocasiones, el significado de un t√©rmino puede variar seg√∫n el contexto. Seg√∫n la Real Academia Espa√±ola (RAE), la palabra ‚Äúestrella‚Äù puede significar tanto un cuerpo celeste como una persona que sobresale en su profesi√≥n, por ejemplo. En los estudios pol√≠ticos palabras como pueblo, democracia o libertad exigen que el analista establezca siempre el contenido concreto asociado a tales expresiones abstractas. El prop√≥sito de esta secci√≥n consiste en introducir algunas herramientas que permitan determinar el significado de una expresi√≥n de forma clara y con menos ambig√ºedad.\n\nKeyword in Context (Kwic)\nUna forma sencilla de contextualizar t√©rminos consiste en visualizarlos directamente en los pasajes del texto en que aparecen. El m√©todo kwic (keyword in context) extrae de un texto o corpus todos los trechos en los que aparece una palabra y los muestra dentro de un contexto o ventana que puede ser compuesta por una o m√°s expresiones antecedentes y posteriores. En el ejemplo abajo, buscamos la palabra ‚Äúlibertad‚Äù en los discursos de investidura de los presidentes espa√±oles, con una ventana de 5 palabras alrededor del t√©rmino.\n\n\nCode\n# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea un corpus (discursos inaugurales Espana)\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un data.frame a partir de\n# la funcion Keyword in Context de\n# Quanteda \nd &lt;- kwic(x = tokens(cp),\n          pattern= \"libertad\",\n          window = 5)\n\n# Visualiza los resultados\n reactable::reactable(d,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo podemos observar, el uso del t√©rmino libertad var√≠a significativamente seg√∫n el momento y el presidente en cuesti√≥n. Adolfo Su√°rez lo utiliza en un contexto de transici√≥n hacia la democracia, como superaci√≥n de una etapa anterior autoritaria. Por esa raz√≥n, la palabra aparece junto a derechos, instituciones y democracia. Felipe Gonz√°lez la utiliza junto a las ideas de igualdad y solidaridad, mientras que Aznar las asocia a la expresi√≥n, ense√±anza y seguridad. Zapatero introduce el concepto de libertad sexual. Rajoy la asocia a prosperidad e igualdad, mientras que Pedro S√°nchez se centra en dos ejes: valores postmateriales (sexual, aborto, eutanasia) y territorial (autonom√≠a de las comunidades aut√≥nomas).\nPor otra parte, si hacemos un ejercicio y utilizamos el t√©rmino ‚Äúempleo‚Äù, se puede averiguar que este se refiere casi exclusivamente al mundo laboral y pol√≠ticas activas de acceso o creaci√≥n de puestos de trabajo. Solo en dos ocasiones espec√≠ficas se trata del verbo ‚Äúemplear‚Äù con el significado ‚Äúutilizar‚Äù, como las referencias ‚Äúemplear una pol√≠tica monetaria‚Äù o el ‚Äúempleo de los caudales p√∫blicos‚Äù, ambas en el primer discurso de investidura de Felipe Gonz√°lez. Por lo tanto, al examinar los resultados, vemos que esos dos casos constituyen una excepci√≥n al significado principal de empleo a que hacen referencia todos los discursos.\n\n\n√Årbol de palabras\nEl √°rbol de palabras (wordtree) nos brinda una visi√≥n semejante al kwic con una diferencia fundamental: cada palabra que compone la frase se dimensiona de acuerdo con la frecuencia con que aparecen en los textos. Este recurso resulta √∫til para discriminar los usos m√°s comunes de los t√©rminos en sus contextos predominantes. De acuerdo con Wattenberg y Vi√©gas (2008, 2‚Äì3), corresponde a una alternativa gr√°fica y exploratoria de visualizaci√≥n de los kwic. Se trata, adem√°s, de un recurso interactivo que permite al usuario jugar con los contextos, direccionando su mirada hacia frases concretas o subiendo a patrones m√°s generales. Posee tres caracter√≠sticas distintivas. Primero, facilita la identificaci√≥n de repeticiones de palabras. Segundo, tiene una estructura de √°rbol claramente identificable. Finalmente, facilita la exploraci√≥n del contexto.\nPor lo tanto, representa una herramienta que sirve tanto para la exploraci√≥n de patrones en los textos durante una primera fase exploratoria de un estudio como de instrumento de comunicaci√≥n de patrones o temas recurrentes encontrados en los datos. Su interactividad invita tanto a la descubierta como a una mayor atenci√≥n a los argumentos que se desean transmitir.\nEl c√≥digo abajo utiliza la funci√≥n wordtree del paquete tenet para crear el √°rbol de palabras alrededor del t√©rmino libertad. Como podemos ver, el resultado es muy semejante al producido por el kwic. No obstante, ahora nuestra atenci√≥n se ve atra√≠da por las palabras de mayor tama√±o. Las rutas m√°s comunes se evidencian, como es el caso de ‚Äúde la libertad de expresi√≥n‚Äù, por ejemplo.\n\n\nCode\n# Crea un arbol de palabras en tenet\nwordtree(corpus = cp,\n         keyword = \"libertad\",\n         height = 800)\n\n\n\n\n\n  \n  \n  \n                   \n                   \n                   \n                   \n                   \n\n\nPor otra parte, si filtramos el corpus para que incluya solamente textos de un presidente o partido pol√≠tico, podemos identificar los usos espec√≠ficos que hacen de los t√©rminos y, as√≠, trazar variantes y revelar patrones √∫tiles te√≥ricamente. Seguramente veremos diferencias sustantivas entre Adolfo Su√°rez y Pedro S√°nchez, como hemos podido contrastar en el apartado anterior. Adem√°s, ser√° posible identificar de forma m√°s sencilla las expresiones m√°s recurrentes o t√≠picas de cada uno. En el caso de Jos√© Mar√≠a Aznar, por ejemplo, la libertad se asocia de forma muy evidente al progreso econ√≥mico.\n\n\nPrevalencia en el tiempo\nGoogle Libros, consiste en uno de los servicios m√°s interesantes de Google. Se trata de un proyecto ambicioso que ha llevado a cabo la digitalizaci√≥n y el pre-procesamiento ling√º√≠stico masivo de un n√∫mero enorme de libros en diferentes lenguas. Adem√°s de disponibilizar muchos documentos en l√≠nea (en su gran mayor√≠a de dominio p√∫blico), la aplicaci√≥n realiza el c√°lculo de una serie de m√©tricas entre las cuales se encuentra la densidad de palabras o expresiones (los n-gramas como ya hemos visto en el cap√≠tulo anterior). De ah√≠ surgi√≥ el Google N-Gram Viewer, una aplicaci√≥n que permite mapear la evoluci√≥n de un t√©rmino o palabra en una lengua en largos per√≠odos de tiempo.\nEl paquete ngramr en R permite acceder a los datos de Google N-Gram Viewer y obtener las frecuencias relativas. La funci√≥n ngrami, por ejemplo, busca una o m√°s palabras sin considerar si est√° en may√∫sculas o min√∫sculas y retorna un data.frame con los resultados. Estos datos se pueden emplear luego para generar un gr√°fico.\nLa gran ventaja de N-Gram Viewer est√° en su car√°cter hist√≥rico. Revela c√≥mo una palabra o expresi√≥n evoluciona en una misma lengua a lo largo de amplios per√≠odos de tiempo. El c√≥digo abajo realiza la b√∫squeda de tres infraestructuras clave: los tel√©grafos, los ferrocarriles y el Internet. A partir del examen de su prevalencia en los libros en espa√±ol de 1800 a 2019, podemos observar sus distintos ciclos temporales:\n\n\nCode\n# Carga el paquete ngramr\nlibrary(ngramr)\n\n# Busca los t√©rminos (case-insensitive)\nnm &lt;- ngrami(\n          c(\"tel√©grafo\",\n            \"ferrocarril\",\n            \"internet\"), \n          corpus = \"es-2019\")\n\n\n# Ngram data table\n# Phrases:      \n# Case-sensitive:   \n# Corpuses:     \n# Smoothing:        \n# Years:        1800-2022\n\n  Year          Phrase    Frequency  Corpus Parent             type\n1 1800 tel√©grafo (All) 7.850821e-07 es-2019        CASE_INSENSITIVE\n2 1801 tel√©grafo (All) 6.980287e-07 es-2019        CASE_INSENSITIVE\n3 1802 tel√©grafo (All) 6.175318e-07 es-2019        CASE_INSENSITIVE\n4 1803 tel√©grafo (All) 5.662840e-07 es-2019        CASE_INSENSITIVE\n5 1804 tel√©grafo (All) 4.264378e-07 es-2019        CASE_INSENSITIVE\n6 1805 tel√©grafo (All) 3.531117e-07 es-2019        CASE_INSENSITIVE\n\n\nCode\n# Genera un gr√°fico con los resultados\nlibrary(ggplot2)\n\np &lt;- ggplot(nm, \n            aes(\n              x=Year, \n              y=Frequency, \n              color=Phrase))+\n  geom_line()+\n  theme_classic()+\n  scale_color_discrete(name=\"T√©rmino\")+\n  labs(\n    title=\"Google N-Gramas (1800-2019)\", \n    subtitle = \"Densidad de palabras en libros \n                en espa√±ol de 1800 a 2019.\")+\n  xlab(\"A√±o\")+\n  ylab(\"Frecuencia relativa\")\n\np\n\n\n\n\n\n\n\n\n\nLas curvas no pueden ser m√°s ilustrativas. Los tel√©grafos presentan una ascensi√≥n entre 1850 y 1900 para, luego, entrar en declive. Los ferrocarriles se han comportado de forma similar, aunque aparezcan de forma mucho m√°s frecuente en los libros del per√≠odo. Internet, una infraestructura mucho m√°s reciente, se ve reflejada a partir de una curva exponencial a partir de los 2000.\nNo resulta nada dif√≠cil replicar la misma l√≥gica en textos que posean alguna secuencia temporal o l√≥gica. Para reproducir el an√°lisis realizado por el NGram Viewer con un corpus no previamente preparado necesitamos llevar a cabo dos tareas centrales. La primera consiste en crear una funci√≥n que busque algunos t√©rminos o expresiones en el corpus y, luego, calcule su densidad. En el segundo paso se organiza la base de datos de modo que cada unidad textual refleje una unidad de tiempo. Por ejemplo, podemos aglutinar los documentos por d√≠a, mes, a√±o o cualquier medida que convenga al investigador. Lo importante es que las unidades sean homog√©neas y comparables entre s√≠.\nUna de las bases de datos de ejemplo incluidas en el paquete tenet se conforma por todas las intervenciones parlamentarias durante la XIV Legislatura del Congreso de Diputados de Espa√±a, vigente entre diciembre de 2019 y junio de 2023. La base original contiene la intervenci√≥n de cada diputado y la fecha en la que se ha realizado. Por esa raz√≥n, resulta relativamente sencillo agregar los textos por fecha. No se recomienda el uso de d√≠as, pues los intervalos entre sesiones no resulta uniforme. Algunas semanas contienen dos o m√°s sesiones, mientras que otras apenas se re√∫nen. Por esa raz√≥n, emplearemos en nuestro ejemplo el mes como unidad de comparaci√≥n de tiempo.\nEl panel abajo contiene la descripci√≥n detallada de cada paso:\n\nPaso 1: Funci√≥nPaso 2: TablaResultado: Gr√°fico\n\n\nEmpezaremos por crear una funci√≥n llamada countNgram con cinco par√°metros o argumentos. El primero, keywords, informa cu√°les son los t√©rminos que deseamos buscar. El segundo, corpus, se refiere al conjunto de textos que ser√°n utilizados como base para el recuento. En tercer lugar, time, suministra la referencia secuencial de tiempo (o de identificaci√≥n) para cada uno de los textos contenidos en corpus (en nuestro caso los meses). Por esa raz√≥n, el n√∫mero de elementos de esos dos √∫ltimos argumentos debe ser siempre igual. Cuarto, rel.freq establece si se calcular√° la frecuencia absoluta o la relativa. Se calcula por defecto a menos que se defina rel.freq=FALSE. Finalmente, remove.accent permite eliminar las tildes y signos de acentuaci√≥n de los textos y de las palabras claves. Por ejemplo, si no eliminamos los acentos, la palabra clave ‚Äúreligio‚Äù regresar√° solo ‚Äúreligioso‚Äù, ‚Äúreligiosidad‚Äù, ‚Äúreligiosa‚Äù, ‚Äúreligiosamente‚Äù y excluir√° ‚Äúreligi√≥n‚Äù. Est√° definidaa como verdadero por defecto, remove.accent=TRUE.\n\n\nCode\n# Funci√≥n countNgram, que cuenta el \n# n√∫mero de veces un conjunto de palabras\n# aparece en un corpus.\ncountNgram &lt;- function(keywords, \n                       corpus=NULL, \n                       time=NULL, \n                       rel.freq=TRUE,\n                       remove.accent=TRUE){\n\n  # En el caso de que sea un diccinario,\n  # permite identificar la categor√≠a o dimension\n  # a que pertenece la palabra clave.\n  \n  dicio &lt;- quanteda::is.dictionary(keywords)\n  \n  if(dicio==TRUE){\n    \n    dct &lt;- keywords\n  \n    kw &lt;- unlist(dct)\n    nm &lt;- stringi::stri_replace_all_regex(names(kw),\"[0-9]+\",\"\")\n    dt &lt;- data.frame(dimension=nm, Keyword=kw)\n  \n    keywords &lt;- kw\n    \n  }\n  \n  # Elimina tildes y signos de acentuaci√≥n  \n  if(remove.accent==TRUE){\n    corpus &lt;- stringi::stri_trans_general(corpus, \"Latin-ASCII\")\n    keywords &lt;- stringi::stri_trans_general(keywords, \"Latin-ASCII\")\n  }\n  \n  # Cuenta todos los t√©rminos de la lista\n  tt &lt;- outer(corpus, \n              keywords, \n              stringi::stri_count_regex)\n  \n  # Nombre cada columna de la matriz de resultados \n  # con los t√©rminos\n  colnames(tt) &lt;- keywords\n  \n  # Transforma los resultados en data.frame\n  tt &lt;- data.frame(tt)\n  \n  # Si se desea la frecuencia relativa\n  if(rel.freq==TRUE){\n    \n    # Cuenta todas las palabras de todas\n    # las intervenciones de cada mes\n    count &lt;- stringi::stri_count_words(corpus)\n    \n    # Calcula la frecuencia relativa\n    for(i in 1:ncol(tt)){\n      tt[,i] &lt;- tt[,i]/count\n    }\n    \n  }\n  \n  # A√±ade la identificaci√≥n del tiempo\n  # (en nuestro caso el mes)\n  tt$Time &lt;- time\n  \n  # Cambia el formato de los datos\n  # (importante para generar el gr√°fico\n  # en un formato ggplot2)\n  tt &lt;- reshape2::melt(tt, id.vars = \"Time\")\n  \n  # Asigna los nombres de las columnas\n  names(tt) &lt;- c(\"Time\",\"Keyword\",\"Density\")\n  \n  # Si es un diccionario, adiciona la \n  # una variable identificando la dimension\n  if(dicio==TRUE){\n    tt &lt;- merge(tt, dt, by=\"Keyword\")\n  }\n  \n  # Devuelve el resultado final\n  return(tt)\n  \n}\n\n\n\n\nEn el paso siguiente, creamos una copia de la base de datos de intervenciones legislativas, identificamos el mes en el cual se ha realizado y aglutinamos todas las intervenciones en una sola unidad de textos seg√∫n la nueva unidad de tiempo. Tales ‚Äúintervenciones del mes‚Äù ser√°n empleadas en la reci√©n creada countNgram para calcular la densidad de los t√©rminos ‚Äúsexual‚Äù e ‚Äúinmigra‚Äù que incluyen todo relativo a los derechos sexuales e inmigraci√≥n. El c√≥digo describe c√≥mo hacerlo paso a paso y la tabla exhibe los resultados.\n\n\nC√≥digo\n# Crea una copia de los diarios de sesiones del \n# Congreso de Diputados de Espa√±a\nss &lt;- spa.sessions\n\n# Genera una variable con el mes de cada\n# intervenci√≥n de cada diario de sesiones\nss$mes &lt;- as.Date(\n            paste0(\n              substr(\n                ss$session.date, \n                1,\n                7),\n              \"-01\"))\n\n# Junta todas las intervenciones de un \n# mismo mes en un largo texto\nss &lt;- aggregate(list(texto=ss$speech.text), \n                by=list(mes=ss$mes), \n                paste0, \n                collapse=\"\\n\")\n\n# Ejecuta la funci√≥n que cuenta los t√©rminos\n# por unidad de tiempo (en el caso cada mes)\nres &lt;- countNgram(\n            keywords = c(\"sexual\",\"inmigra\"), \n            corpus = ss$texto,\n            time = ss$mes)\n\n# Multiplica la frecuencia relativa por 10 mil \n# para facilitar la lectura de los datos y\n# reduce el n√∫mero de d√≠gitos decimales a 2.\nres$Density &lt;- round(res$Density*10000,2)\n\n# Ense√±a los resultados\nreactable(res,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\nFinalmente, el √∫ltimo paso para reproducir el ejemplo anterior con base en datos de Google NGram Viewer consiste en crear un gr√°fico. El c√≥digo abajo hace justamente eso:\n\n\nC√≥digo\n# Carga el paquete ggplot2\nlibrary(ggplot2)\nlibrary(ggiraph)\n\n# Renombra las variables para\n# que aparezcan mejor en el gr√°fico\nnames(res) &lt;- c(\"Mes\",\"Keyword\",\"Densidad\")\n\n# Genera un gr√°fico de l√≠nea con\n# la evoluci√≥n por mes de cada \n# t√©rmino\np &lt;- ggplot(res, \n            aes(\n              x=Mes, \n              y=Densidad,\n              color=Keyword))+\n  geom_line()+\n  theme_classic()+\n  theme(legend.position=\"bottom\")+\n  scale_color_discrete(name=\"T√©rmino\")+\n  labs(\n    title=\"Sesiones legislativas del Congreso \n           de Diputados (2019-2023)\", \n    subtitle = 'Densidad de palabras conteniendo \n               \"inmigra\" y \"sexual\" en los diarios \n               de sesiones del Congreso\\nde Diputados \n               de Espa√±a entre diciembre de 2019 y \n               junio de 2023.')+\n  xlab(\"A√±o\")+\n  ylab(\"Frecuencia (a cada 10 mil)\")\n\n# Visualiza los resultados\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl examen de los resultados de la tabla y del gr√°fico revela dos picos, uno para cada palabra. En el caso de expresiones relacionadas a ‚Äúsexual‚Äù este se encuentra en octubre de 2021, cuando se ha llevado a cabo el debate sobre la propuesta de la Ley Org√°nica Garant√≠a de la Libertad Sexual (la ley del ‚Äús√≠ es s√≠‚Äù). Para ‚Äúinmigra‚Äù, se trata de agosto de 2020, mes en el cual tuvo lugar una sesi√≥n de la diputaci√≥n permanente en la que se aborda expl√≠citamente el tema de la inmigraci√≥n irregular ocurrida en ese verano.\n\n\nCode\nlibrary(tenet)\n\n# Selecciona los principales diputados de \n# Vox en la XIV legislatura\nag &lt;- spa.sessions[\n  spa.sessions$rep.name%in%\n    c(\"Abascal Conde, Santiago\",\n      \"Espinosa de los Monteros de Sim√≥n, Iv√°n\",\n      \"Olona Chocl√°n, Macarena\",                \n      \"Ortega Smith-Molina, Francisco Javier\"),]\n\n# Reduce los nombres\nag$rep.name[ag$rep.name==\"Abascal Conde, Santiago\"] &lt;- \"Abascal\"\nag$rep.name[ag$rep.name==\"Espinosa de los Monteros de Sim√≥n, Iv√°n\"] &lt;- \"Espinosa \"\nag$rep.name[ag$rep.name==\"Olona Chocl√°n, Macarena\"] &lt;- \"Olona\"\nag$rep.name[ag$rep.name==\"Ortega Smith-Molina, Francisco Javier\"] &lt;- \"Ortega Smith\"\n\n# Create a variable of month for smoothing the data\nag$month &lt;- substr(ag$session.date,3,7)\n\n\n# Aggregate words by representative and month\nag &lt;- aggregate(\n  list(words=ag$speech.tokens), \n  by=list(\n    month=ag$month, \n    rep=ag$rep.name, \n    party=ag$rep.party), \n  sum, \n  na.rm=T)\n\n# Order the data by month\nag &lt;- ag[order(ag$month),]\n\nplotStream(ag,\n           x=\"month\",\n           y=\"words\",\n           group=\"rep\", elementId = \"elem223\")\n\n\n\n\n\n\nCode\nplotGrid(ag, \n         x=\"month\", \n         y=\"rep\", \n         size=\"words\", \n         color=\"rep\", \n         width_svg = 9, height_svg = 4, \n         leg.size=\"Palabras\", \n         leg.color=\"Diputado\",  \n         grid.color=\"grey99\")\n\n\n\n\n\n\n\n\nEtiquetado autom√°tico de textos\nUna tarea central de cualquier an√°lisis cualitativo de texto consiste en la lectura atenta y la selecci√≥n de pasajes o t√©rminos para la codificaci√≥n tem√°tica. Se trata de un proceso iterativo -con muchas idas y venidas, revisiones y cambios- en el cual el investigador marca o subraya trechos o palabras del documento y les asigna un c√≥digo o categor√≠a anal√≠tica. En ese sentido, la lectura orienta el proceso de codificaci√≥n y clasificaci√≥n, dando lugar al surgimiento de conceptos m√°s abstractos. Aunque la investigaci√≥n parta de algunas nociones previas, esta fase exploratoria consiste en la clave para la vinculaci√≥n de elementos concretos presentes en el texto a temas te√≥ricos m√°s amplios. Existe cierto consenso en la literatura sobre el tema que considera la clasificaci√≥n tem√°tica como el resultado (y no el punto de partida) del an√°lisis cualitativo (Saldana 2015, 13; Krippendorff 2004; Neuendorf 2017; Guest, MacQueen, and Namey 2011; Miles, Huberman, and Saldana 2019; Auerbach and Silverstein 2003). Es el producto de la interacci√≥n entre el analista y el texto.\nPor esa raz√≥n, se necesitan instrumentos que faciliten tal interacci√≥n entre el investigador y el texto, que les permitan partir de algunas semillas y profundizar en las sucesivas capas de an√°lisis hasta encontrar los temas centrales de su investigaci√≥n. En este apartado introduciremos dos opciones de etiquetado y visualizaci√≥n de textos. Constituyen recursos que permiten lo que algunos autores (Saldana 2015, 13) denominan como pre-codificaci√≥n, como el sublineado o coloreado de pasajes del texto con el prop√≥sito de identificar pasajes √∫tiles para la generaci√≥n de hip√≥tesis o la creaci√≥n de conceptos.\nEl etiquetado toma un texto √∫nico y examina la presencia de un conjunto de palabras-clave o expresiones definidas por el investigador. Si se busca, por ejemplo, saber c√≥mo los presidentes mencionan las pol√≠ticas sociales en sus discursos, no ser√≠a raro que se empezara buscando referencias a la seguridad social, sanidad, educaci√≥n, pobreza o desigualdad. Estudiosos del populismo, por otra parte, buscar√≠an referencias a patria, pueblo, naci√≥n, o √©lites.\nLa funci√≥n tagText del paquete tenet permite realizar dicha tarea. Su utilidad consiste en subrayar un texto concreto seg√∫n una lista de palabras-clave. Su prop√≥sito resulta muy sencillo: ayudar en la creaci√≥n de c√≥digos. El usuario puede leer el texto y al mismo tiempo, crear un diccionario o una lista de palabras que resultan particularmente expresivas de una idea o concepto que se desea capturar.\nEl c√≥digo abajo selecciona el discurso de investidura de Adolfo Su√°rez y subraya todas las palabras que contengan las ra√≠ces ‚Äúpolitic‚Äù, ‚Äúacci‚Äù, ‚Äúconflict‚Äù, ‚Äúpartid‚Äù, ‚Äúdefensa‚Äù, y la expresi√≥n ‚Äúfuerzas armadas‚Äù. A cada palabra asocia un color para que sea m√°s f√°cil su identificaci√≥n en el texto. De ese modo, se puede ver d√≥nde aparecen.\n\n\nCode\n# Examina un conjunto de palabras en el\n# discurso de investidura de Adolfo Su√°rez\ntagText(as.character(spa.inaugural$text[1]), \n        keywords = c(\"politic\", \n                     \"acci\", \n                     \"conflict\", \n                     \"partid\", \n                     \"defensa\",\n                     \"fuerzas armadas\"), \n        palette = pal$cat.wesanderson.AsteroidCity2.6,\n        font.size = 18, \n        title = \"Adolfo Suarez (1979)\",\n        margin = 100)\n\n\n\nComo se puede observar, a cada ra√≠z, palabra-clave o expresi√≥n le corresponde un color distinto. Este atributo favorece no solo la identificaci√≥n de su posici√≥n en el texto, sino que tambi√©n los distingue y subraya cuando coinciden en un mismo p√°rrafo o sentencia. Sit√∫a, contextualiza, compara y, sobre todo, mantiene el lector anclado al texto.\nUna queja o desconfianza de ciertos investigadores cualitativos frente a m√©todos asistidos por ordenador tiene que ver con el distanciamiento que √©stos √∫ltimos provocan con relaci√≥n a las fuentes textuales. No obstante, el proceso no tiene que implicar una tal dicotom√≠a si se emplean herramientas adecuadas para el examen de patrones textuales.\nLa funci√≥n tagText tambi√©n se puede utilizar en conjunci√≥n con diccionarios. Resulta √∫til para averiguar la consistencia de la codificaci√≥n desarrollada, as√≠ como permite perfeccionar los c√≥digos existentes. La estructura de un diccionario resulta sencilla. A cada c√≥digo corresponde un conjunto de t√©rminos, ra√≠ces o expresiones. Al emplear un diccionario en tagText todos los elementos de un c√≥digo se representan bajo un mismo color. De ese modo, resulta m√°s f√°cil no solo ubicar las categor√≠as, sino tambi√©n ver hasta qu√© punto aparecen en un mismo pasaje del documento. M√°s adelante, cuando tratemos del proceso de codificaci√≥n por medio de diccionarios emplearemos algunos ejemplos.\n\n\nDispersi√≥n l√©xica\nEn ciertas ocasiones interesa saber no solo cu√°ntas veces una palabra aparece en un texto o corpus, sino d√≥nde. El lugar en que se manifiesta una idea puede ser muy significativo para determinar su importancia en un discurso. ¬øEst√° por todo el texto o aparece solamente en algunas partes? ¬øQu√© documentos poseen mayor o menor concentraci√≥n?\nEl gr√°fico de dispersi√≥n l√©xica (lexical dispersion plot) representa una excelente visualizaci√≥n para determinar la localizaci√≥n de un conjunto de palabras-clave en distintos pasajes o sentencias de documentos de componen un corpus y, por lo tanto, su grado de dispersi√≥n (o concentraci√≥n). La funci√≥n plotLexDiv del paquete tenet permite crear el gr√°fico a partir de un conjunto de palabras clave (o incluso de un diccionario). El paquete quanteda.textplots tambi√©n dispone de una funci√≥n semejante, pero limitada a una sola palabra de cada vez y que no permite la agregaci√≥n de t√©rminos bajo una misma categor√≠a de un diccionario.\nEl c√≥digo abajo explora dos maneras en las que los presidentes de gobierno espa√±oles se refieren a los ciudadanos del pa√≠s. La primera, con √©nfasis m√°s patri√≥tico, se centra en las expresiones ‚Äúespa√±oles‚Äù y ‚Äúespa√±olas‚Äù. La segunda, con un car√°cter m√°s republicano, enfoca la membres√≠a a la comunidad pol√≠tica bajo los t√©rminos ‚Äúciudadanos‚Äù y ‚Äúciudadanas‚Äù. ¬øExisten diferencias en el uso de esos dos modos de representar a los miembros de la sociedad espa√±ola? ¬øSu presencia resulta concentrada o dispersa, es decir, se trata de algo muy concreto en una parte de los discurso o aparece en todo el texto como una expresi√≥n recurrente y articuladora de las dem√°s partes?\n\n\nCode\n# Genera el primer gr√°fico con las\n# palabras \"espa√±oles\" y \"espa√±olas\"\np1 &lt;- plotLexDiv(cp,\n           title = \"Espa√±oles\", \n           keywords = c(\"espanoles\",\"espanolas\"), \n           custom.color = \"red3\")\n\n# Genera el segundo gr√°fico con las\n# palabras \"ciudadanos\" y \"ciudadanas\"\np2 &lt;- plotLexDiv(cp,\n           title = \"Ciudadanos\", \n           keywords = c(\"ciudadanos\",\"ciudadanas\"), \n           custom.color = \"blue\")\n            \n\n# Carga los paquetes necesarios para\n# pegar los dos gr√°ficos como una √∫nica\n# imagen\nlibrary(grid)\nlibrary(gridExtra)\n\n# Genera la visualizaci√≥n final\ngrid.arrange(p1,p2, nrow=1)\n\n\n\n\n\n\n\n\n\nUna breve inspecci√≥n visual revela que hay diferencias entre los presidentes en el uso de una u otra forma. Mariano Rajoy aparece como el m√°s enf√°tico usuario de ‚Äúespa√±oles‚Äù, mientras que Felipe Gonz√°lez se decanta por ‚Äúciudadanos‚Äù. Otros se refieren a ambos t√©rminos, lo que exige un escrutinio m√°s detenido para entender el contexto del uso de cada. En algunos documentos las expresiones aparecen a lo largo de toda la extensi√≥n, mientras que en otros, como espa√±oles en Zapatero II o ciudadanos en Calvo Sotelo, solo en pasajes concretos.\n\n\nCode\n# Crea un corpus con los discursos\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un diccionario con las dos categor√≠as\ndic &lt;- dictionary(\n  list(espa√±oles=c(\"espa√±oles\",\"espa√±olas\"),\n       ciudadanos=c(\"ciudadanos\",\"ciudadanas\")))\n\n# Genera la tabla de posiciones\nter &lt;- filterWords(cp, dic)\n\n# Define el orden\nor &lt;- c(\"Su√°rez\",\"Calvo Sotelo\",\"Gonz√°lez I\",\"Gonz√°lez II\",\"Gonz√°lez III\",\"Gonz√°lez IV\",\"Aznar I\",\"Aznar II\",\"Zapatero I\",\"Zapatero II\",\"Rajoy I\",\"Rajoy II\",\"Rajoy III\",\"S√°nchez I\",\"S√°nchez II\",\"S√°nchez III\")\n\n# Convierte la variable de nombres en factor\nter$name &lt;- factor(ter$name, \n                   levels = or, \n                   labels = or)\n\n# Genera el gr√°fico\nplotSpike(ter, \n          label.size = 4, \n          ring.width = 0.5, \n          line.width = 0.4,\n          title=\"Espa√±oles y ciudadanos en los discursos de inauguraci√≥n\",\n          legend.title = \"T√©rmino: \")\n\n\n\n\n\n\nCode\n# Genera el gr√°fico\nplotSpike(ter, \n          polar = F,\n          label.size = 4, \n          ring.width = 0.5, \n          line.width = 0.4,\n          title=\"Espa√±oles y ciudadanos en los discursos de inauguraci√≥n\",\n          legend.title = \"T√©rmino: \")\n\n\n\n\n\n\n\n\nFiltrado de textos: la ratio Documento/Corpus\nCuando se trabaja con muchos textos, uno de los mayores retos consiste en separar el material √∫til del mont√≥n de informaci√≥n que no se desea emplear, al menos en un principio. Por eso, hacen falta estrategias de selecci√≥n o filtrado de documentos que permitan arrojar luz sobre aquellos documentos que contienen material sustantivo para la investigaci√≥n.\nEn muchos casos, basta con buscar aquellos documentos que contienen una expresi√≥n o palabra. No obstante, ¬øqu√© pasa cuando esta palabra aparece en varios documentos, pero ni todos ellos son relevantes? En estos casos, hace falta alg√∫n criterio que permita discriminar de forma ponderada su peso relativo en un corpus.\nLa ratio Documento - Corpus de una palabra constituye una soluci√≥n posible para este problema. Se trata de una medida que eval√∫a cu√°n marcada resulta la aparici√≥n de un t√©rmino concreto en cada documento frente a su frecuencia relativa media en todo el corpus. Una cifra superior a uno en un documento indica una prevalencia m√°s alta que la media y, a la inversa, un valor inferior a uno se√±ala un aparecimiento menos frecuente.\n\\[fd/fc = \\frac{Fd_{i}}{F_i}\\]\nDonde:\nFdi corresponde a la frecuencia relativa del t√©rmino i en del documento d.\nFi corresponde a la frecuencia relativa del t√©rmino i en todo el corpus.\nPuesto que los textos que componen un mismo corpus pueden presentar distinto tama√±o, se utiliza la frecuencia relativa para evitar distorsiones y garantizar la comparabilidad de los resultados.\nHemos creado la funci√≥n tfRatio en el paquete tenet con el objetivo de calcular la ratio de una palabra-clave, ra√≠z o expresi√≥n en todos los documentos de un corpus. Genera una lista con la ratio del t√©rmino para cada documento o una lista de los documentos que superan un cierto umbral (por medio de los par√°metros threshold y return.selected).\nEl c√≥digo abajo calcula la ratio de la ra√≠z ‚Äúmachis‚Äù (machismo, machista) para todos los discursos de investidura de los presidentes de gobierno espa√±oles:\n\n\nCode\n# Calcula la ratio de las palabras que\n# contienen \"machis\" en el corpus de \n# los discursos de investidura\ntfRatio(cp, \"machis\")\n\n\n [1] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.72 0.00 0.00 0.00 3.07 2.15\n[16] 4.67\n\n\nComo podemos observar, solamente tres de los 15 textos contienen alguna referencia al machismo. Estos son los dos discursos de Pedro S√°nchez y el primero de Jos√© Luis Rodr√≠guez Zapatero.\nSi queremos seleccionar los textos, el par√°metro threshold permite establecer un m√≠nimo para que un documento se considere relevante y el argumento return.selected retorna el √≠ndice del documento en lugar el la ratio. El c√≥digo abajo utiliza las ratios para seleccionar los textos en los que palabras como machismo o machista aparezcan m√°s del doble de veces que en la media de todo el corpus (threshold=2):\n\n\nCode\n# Convierte los textos en un data.frame\ntx &lt;- convert(cp, \"data.frame\")\n\n# Selecciona con relacion a la ratio\nsx &lt;- tx[tfRatio(cp, \n                 \"machis\", \n                 threshold = 2, \n                 return.selected = T),]\n\n# Visualiza los resultados\nreactable(sx, \n          resizable = T,  \n          wrap = F)\n\n\n\n\n\n\n\nAhora el investigador puede utilizar otras herramientas como el tagText, los √°rboles de palabra, la dispersi√≥n l√©xica o el kwic para analizar los textos y entender c√≥mo cada uno de ellos trata el tema del machismo. Al reducir de forma clara el tiempo de selecci√≥n de los textos relevantes, esta medida posibilita concentrar esfuerzos en otras tareas de an√°lisis.",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "codificacion.html#codificaci√≥n-tem√°tica-con-diccionarios",
    "href": "codificacion.html#codificaci√≥n-tem√°tica-con-diccionarios",
    "title": "Codificaci√≥n tem√°tica",
    "section": "Codificaci√≥n tem√°tica con diccionarios",
    "text": "Codificaci√≥n tem√°tica con diccionarios\nLa codificaci√≥n tem√°tica consiste en una de las estrategias m√°s comunes en las ciencias sociales para tratar datos en forma de texto. De forma muy resumida, representa un proceso fundamentado en la lectura profundizada de una cantidad limitada de textos y acompa√±ada de anotaciones que permitan extraer temas o conceptos que ayuden en el desarrollo o teste de hip√≥tesis.\nDe acuerdo con Krippendorff (2004, 18) el an√°lisis de contenido constituye una t√©cnica que permite la realizaci√≥n de inferencias a partir de textos en el contexto en que se emplean. Neuendorf (2017, 1), por otra parte, define el m√©todo como el ‚Äúan√°lisis sistem√°tico, objetivo y cuantitativo de las caracter√≠sticas de un mensaje‚Äù. Otros autores (Saldana 2015; Miles, Huberman, and Saldana 2019) extienden el proceso de codificaci√≥n m√°s all√° del an√°lisis de contenido y mencionan un n√∫mero m√°s amplio de m√©todos que se basan en el examen detenido de informaci√≥n textual, como el an√°lisis de discurso, la teor√≠a fundamentada (grounded theory) o el an√°lisis narrativo.\nLa codificaci√≥n de temas y la extracci√≥n de insights te√≥ricos es una constante en la literatura sobre los m√©todos cualitativos en las ciencias sociales. En algunos casos, se considera un proceso casi artesanal basado en la lectura profunda y un conocimiento detallado de los textos y sus contextos. En otros, la existencia de vol√∫menes masivos de documentos invita a la combinaci√≥n entre: (a) el escrutinio atento de una muestra de los documentos y (b) m√©todos cuantitativos que permitan sintetizar y clasificar un masa de material imposible de examinar de forma tradicional.\n\nExpresiones, c√≥digos, temas\nResulta √∫til iniciar este apartado con una breve distinci√≥n entre expresiones (t√©rminos, palabras o n-gramas), c√≥digos (etiquetas que sintetizan el contenido) y temas (constructos te√≥ricos derivados del an√°lisis). De forma resumida, se puede argumentar que las primeras indexan los textos (conectan palabras a ideas) y ayudan a seleccionar atributos o pasajes que pueden ser particularmente significativas o te√≥ricamente relevantes. Los siguientes (c√≥digos) posibilitan aunar los t√©rminos en grupos o colecciones y llevar a cabo un esfuerzo de agregaci√≥n o s√≠ntesis conceptual previo al desarrollo de conceptos m√°s abstractos. Finalmente, un tema emerge del an√°lisis de los distintos c√≥digos, sus contextos y relaciones rec√≠procas.\nImaginemos que deseamos saber la posici√≥n de distintos partidos sobre las pol√≠ticas sociales. Una forma de encontrar informaciones consiste en buscar palabras como pobreza, desigualdad, ayudas, sanidad, educaci√≥n, seguridad social, asistencia social, transferencias, entre otros t√©rminos. En su conjunto, tales expresiones permiten ubicar en los diferentes documentos de un corpus cu√°nto y d√≥nde se ha mencionado alguna de esas medidas o asuntos relacionados. Por lo tanto, act√∫an como instrumentos de b√∫squeda e indexaci√≥n de categor√≠as abarcadoras, que pueden estar reunidas en c√≥digos que ayuden a identificar cada aspecto mencionado con la noci√≥n general de pol√≠tica social. Sin este paso, resulta muy complicado filtrar aquellos pasajes m√°s destacados sobre ese tema en corpus con muchos documentos. Tampoco se podr√≠a calcular su peso relativo: ¬øcu√°l pol√≠tica recibe un n√∫mero mayor de menciones: la sanidad o la educaci√≥n?\nLos c√≥digos son, a su vez, ‚Äúetiquetas que atribuyen significado simb√≥lico a informaci√≥n descriptiva o inferencial compilada durante un estudio‚Äù (Miles, Huberman, and Saldana 2019, 78‚Äì79). Tales ‚Äúetiquetas‚Äù pueden ser palabras o expresiones que permitan atribuir un sentido m√°s amplio, sea descriptivo o conceptual, a la palabra o trecho al que se le asocia. Tambi√©n pueden considerarse como dispositivos de segmentaci√≥n y aglomeraci√≥n de significados. Al asociar un conjunto diverso de palabras a los t√©rminos izquierda y derecha, por ejemplo, establecemos qu√© partes se a√∫nan dentro de esas categor√≠as y, al mismo tiempo, las separamos del resto del texto (diagrama 1).\n\n\n\nDiagrama 1. Ejemplos de t√©rminos y expresiones, as√≠ como de c√≥digos de primer y segundo nivel en un diccionario con las categor√≠as izquierda y derecha.\n\nAdem√°s, los c√≥digos facilitan la b√∫squeda, clasificaci√≥n y ordenamiento de la informaci√≥n seg√∫n los intereses del investigador. De cierto modo, sintetizan la diversidad de t√©rminos y expresiones en un n√∫mero reducido de categor√≠as o marcadores. Corresponden a una heur√≠stica, un m√©todo de descubierta (Miles, Huberman, and Saldana 2019, 80). Al reestructurar los textos de acuerdo a categor√≠as anal√≠ticamente significativas, permite identificar patrones, establecer la relaci√≥n entre c√≥digos y desarrollar interpretaciones anal√≠ticas de m√°s alto nivel. En cierto sentido, constituyen una herramienta para estructurar el texto y permitir su recomposici√≥n bajo l√≥gicas anal√≠ticas distintas.\nFinalmente, los temas constituyen construcciones te√≥ricas derivadas del an√°lisis de los c√≥digos. Pueden surgir del examen de un, diez o cien mil documentos. Todo va a depender del dise√±o de la investigaci√≥n y c√≥mo cada investigador construye sus categor√≠as anal√≠ticas, abstrayendo temas m√°s generales a partir de c√≥digos encontrados en los textos. El tipo y el grado de abstracci√≥n depender√° de los objetivos definidos para cada estudio. No obstante, el m√©todo suele ser siempre el mismo: la definici√≥n de c√≥digos, su an√°lisis y la posterior s√≠ntesis en un conjunto de temas clave y te√≥ricamente significativos.\nAsociar c√≥digos a los textos, por lo tanto, permite extraer informaci√≥n valiosa para la investigaci√≥n. Como hemos se√±alado, un c√≥digo puede estar asociado tanto a una palabra en concreto como a pasajes enteros. Por esa raz√≥n, cabe subrayar que el m√©todo propuesto en este trabajo resulta un poco distinto de la codificaci√≥n tradicional y representa una alternativa h√≠brida entre m√©todos puramente cuantitativos y la construcci√≥n de un sistema cualitativo tradicional. Aqu√≠, la b√∫squeda de ra√≠ces, palabras o expresiones se utiliza para identificar e indexar d√≥nde una determinada idea aparece en el documento.\nTales elementos emp√≠ricos fundamentales pueden estar aislados y servir para un examen preliminar del corpus o agregados seg√∫n categor√≠as anal√≠ticas. Los c√≥digos tambi√©n pueden anidarse en una estructura jer√°rquica, como ramas e un √°rbol. Como vimos en el diagrama 1 arriba, los t√©rminos y expresiones ‚Äúderecho aborto‚Äù, ‚Äúcambio clim√°tico‚Äù y ‚Äútransex‚Äù conforman la categor√≠a ‚Äúprogresismo‚Äù. Ya ‚Äúeduc‚Äù, ‚Äúpobreza‚Äù y ‚Äúsocial‚Äù se a√∫nan en ‚Äúigualdad social‚Äù. Finalmente, los dos c√≥digos (progresismo e igualdad social) componen la categor√≠a m√°s abstracta ‚Äúizquierda‚Äù.\nEn resumen, un c√≥digo agrupa t√©rminos b√°sicos en categor√≠as anal√≠ticas m√°s o menos homog√©neas internamente y distintas entre s√≠. De ese modo, un c√≥digo se constituye a partir de: (a) una recolecci√≥n de esas unidades textuales b√°sicas (m√°s que de frases o trechos enteros) o (b) de otros c√≥digos con menores niveles de abstracci√≥n.\nTal estrategia puede considerarse a primera vista como limitante. No obstante, presenta dos grandes puntos fuertes. En primer lugar, palabras o expresiones cortas sirven como elementos √∫tiles para la selecci√≥n de pasajes m√°s amplios de forma autom√°tica y el filtrado de contenido relevante. Al emplearse en conjunto con las herramientas de visualizaci√≥n disponibles, como el kwic y el etiquetado de textos resulta m√°s facil situar las ideas en su contexto sin tener que examinar de antemano largos pasajes de textos.\nSegunda ventaja: el uso de palabras, bigramas o trigramas es indudablemente m√°s f√°cil de cuantificar y comparar en grandes corpus de texto. Resulta mucho m√°s sencillo (y r√°pido) comparar ‚Äúmachis‚Äù en 10 mil documentos que frases concretas que solo indirectamente mencionan el tema del machismo. Para estos √∫ltimos casos, estrategias complementarias, como la inclusi√≥n de met√°foras u otras alusiones indirectas relevantes puede ayudar en su identificaci√≥n. Cuando el n√∫mero de documentos es inabarcable desde una metodolog√≠a tradicional, una alternativa √∫til consiste en combinar la lectura profundizada de una muestra cuidadosamente seleccionada de textos con algoritmos que permitan rastrear la presencia de c√≥digos en todo el material disponible.\nEste m√©todo implica un cambio en la forma de construcci√≥n de los c√≥digos. A√∫n m√°s cuando empleamos diccionarios (o l√©xicos) tem√°ticos como instrumento. La inclusi√≥n de una palabra o n-grama en los diccionarios hace con que nuevos pasajes salgan a la luz. La clave se encuentra en analizar las frases o p√°rrafos que contienen tales expresiones y, a partir de ah√≠, refinar el mismo l√©xico, desarrollar c√≥digos m√°s abstractos y generar temas te√≥ricamente relevantes para el estudio. Como se ha se√±alado m√°s arriba, no se trata de abandonar un enfoque cualitativo, sino adaptarlo a situaciones en las que la lectura exhaustiva y en profundidad del corpus resulta imposible o poco pr√°ctica.\n\n\nDiccionarios como colecciones de c√≥digos\nComo se ha mencionado m√°s arriba, los diccionarios pueden considerarse como dispositivos de ensamblaje de expresiones de inter√©s te√≥rico o como instrumentos de organizaci√≥n de ideas. Permiten la aglutinaci√≥n de t√©rminos en categor√≠as anal√≠ticas m√°s amplias y su posterior organizaci√≥n en estructuras conceptuales jer√°rquicas.\nEl diagrama 1 m√°s arriba nos muestra como ciertas ra√≠ces, palabras o expresiones se agrupan en categor√≠as como progresismo, conservadurismo, liberalismo econ√≥mico o igualdad social y √©stas en c√≥digos m√°s abstractos como izquierda o derecha. Este breve diccionario nos ayuda a asociar t√©rminos que se pueden encontrar en los textos a conceptos o ideas m√°s abstractas, sin una manifestaci√≥n emp√≠rica directa. De ese modo, la izquierda se compone por dos dimensiones -igualdad social y progresismo-, mientras que la derecha se caracteriza como la combinaci√≥n entre liberalismo econ√≥mico y conservadurismo social.\nAunque est√© muy lejos de brindar una definici√≥n exhaustiva de los conceptos de derecha e izquierda, este peque√±o ejemplo ilustra c√≥mo un diccionario permite asociar elementos emp√≠ricos concretos encontrados en los textos a dimensiones te√≥rico-conceptuales m√°s abstractas.\nPor esa misma raz√≥n, se debe subrayar la naturaleza iterativa del proceso. Las categor√≠as m√°s abarcadoras (como progresismo, por ejemplo) se suelen construir a partir de un proceso reiterado de examen de los textos, la inclusi√≥n de nuevos t√©rminos y la revisi√≥n de los resultados. Incluso en los casos en que se dispone de un diccionario previo hecho por otros, resulta fundamental la adaptaci√≥n a los prop√≥sitos de investigaci√≥n para alcanzar los mejores resultados.\nLa creaci√≥n de un diccionario representa una t√©cnica de medici√≥n y b√∫squeda (Neuendorf 2017, 126‚Äì27) en la el uso de palabras y otros elementos textuales permiten identificar la presencia de ciertas ideas o conceptos en un corpus determinado. Tambi√©n se le podr√≠a considerar como un libro de c√≥digos, una compilaci√≥n de categor√≠as y los elementos que le componen. La formalizaci√≥n expl√≠cita de los grupos y su documentaci√≥n facilita el trabajo en grupo y aumenta la transparencia y reproducibilidad de al menos parte del an√°lisis realizado (Saldana 2015, 21).\nEl ejemplo abajo crea un diccionario que clasifica 39 t√©rminos seg√∫n los c√≥digos ‚Äúeconom√≠a‚Äù, ‚Äúfiscal‚Äù, ‚Äúeducaci√≥n‚Äù, ‚Äúsanidad‚Äù y ‚Äúmedioambiente‚Äù. Luego, lo emplea en conjunci√≥n con la funci√≥n tagText para resaltar las categor√≠as en el discurso de investidura de Adolfo Su√°rez. Al analizar los resultados, vemos que a cada categor√≠a (o c√≥digo) corresponde un color cuyo nombre se revela al mover el cursos sobre una palabra subrayada.\n\n\nCode\n# Crea un diccionario de algunos t√©rminos pol√≠ticos\ndic &lt;- dictionary(\n    list(\n    economica=c(\"econom\",\n               \"inversion\",\n               \"empresa\",\n               \"desarroll\",\n               \"monetari\",\n               \"industri\",\n               \"agric\",\n               \"agrari\"),\n    fiscal=c(\"hacienda\",\n               \"gasto\",\n               \"impuest\",\n               \"presupuest\",\n               \"tribut\",\n               \"tasa\",\n               \"fiscal\"),\n    educacion=c(\"educa\",\n             \"profesor\",\n             \"docent\",\n             \"escuel\",\n             \"colegio\",\n             \"universi\",\n             \"formaci√≥n\"),\n    sanidad=c(\"sanidad\",\n               \"salud\",\n               \"hospital\",\n               \"sanitari\",\n               \"m√©dic\",\n               \"enfermer\",\n               \"salud\"),\n    medioambiente=c(\"sostenible\",\n                 \"cambio clima\",\n                 \"medioambient\",\n                 \"reciclaje\",\n                 \"ecol√≥gico\",\n                 \"l√≠mpia\",\n                 \"invernadero\",\n                 \"emisiones\",\n                 \"carbono\",\n                 \"pl√°stico\",\n                 \"f√≥siles\")))\n\n# genera un texto para ser le√≠do en el panel\n# Viewer de RStudio\ntagText(spa.inaugural$text[1], \n        keywords = dic, \n        palette = pal$cat.cartocolor.prism.11,\n        font.size = 18, \n        title = \"Adolfo Suarez (1979)\",\n        margin = 100)\n\n\n\nEl resultado nos sit√∫a en un espacio entre una b√∫squeda automatizada de t√©rminos y la codificaci√≥n manual. Al aplicar el diccionario a un texto espec√≠fico, se puede observar no solo d√≥nde las categor√≠as aparecen m√°s o menos, sino tambi√©n c√≥mo estas se asocian entre ellas en un mismo p√°rrafo, por ejemplo. Algunos pasajes son monotem√°ticos, inciden sobre una idea clave. Otros interesan por la asociaci√≥n entre conceptos distintos. En muchas ocasiones es justamente la asociaci√≥n entre temas lo que permite el surgimiento de nuevas hip√≥tesis. Adem√°s, el an√°lisis de la incidencia de los c√≥digos en el texto invita a la revisi√≥n del diccionario para incorporar nuevos t√©rminos o categor√≠as y, de ese modo, completar el an√°lisis.\nMiles et al. (2019, 86) sugieren que un m√©todo para utilizar diccionarios como herramientas para la codificaci√≥n consiste en crear una lista provisional de c√≥digos por medio de un proceso deductivo a partir de las referencias te√≥ricas que sirven de marco para el estudio. Una vez elaborada, puede servir de semilla para el examen de los textos y pasar por procesos sucesivos de adaptaci√≥n, refinamiento y elaboraci√≥n con el empleo de una codificaci√≥n inductiva complementaria.\nEste proceso de revisi√≥n constante requiere instrumentos que permitan explorar, ordenar, filtrar y sintetizar la informaci√≥n. La funci√≥n tagCorpus de tenet emplea una tabla interactiva que permite a los usuarios llevar a cabo una serie de tareas de exploraci√≥n de los t√©rminos y c√≥digos de un diccionario en todo un corpus. Por lo tanto, se centra en posibilitar la identificaci√≥n tanto de aspectos compartidos como de se√±as distintivas entre documentos, categor√≠as o actores. Adem√°s, permite examinar de forma sumaria la coocurrencia de c√≥digos en frases o p√°rrafos.\nEl paquete tenet tambi√©n incluye un diccionario de ejemplo llamado dic.pol.es. Se trata de un conjunto de c√≥digos que analizan diferentes dimensiones de los discursos pol√≠ticos. Contiene tres niveles: (1) palabras o expresiones, (2) c√≥digos de primer nivel y (3) c√≥digos de segundo nivel. Por ejemplo, ‚Äúizquierda unida‚Äù pertenece al c√≥digo nivel-1 ‚Äúpartidos‚Äù y al c√≥digo nivel-2 ‚Äúactores‚Äù. Por su parte, ‚Äúilustres‚Äù pertence a ‚Äúret√≥rica‚Äù (nivel-1) y a ‚Äúdiscurso‚Äù (nivel-2).\nEl c√≥digo abajo utiliza la funci√≥n tagCorpus y el diccionario dic.pol.es para identificar la incidencia de las categor√≠as en cada sentencia del corpus de los discursos de investidura espa√±oles. Como se podr√° ver, abajo, el resultado es una tabla con siete columnas. La primera Order corresponde al orden de la frase en el documento X (columna Doc.). De ese modo, Order igual a 1 y Doc. igual a Su√°rez corresponde a la primera frase del discurso de investidura de Adolfo Su√°rez. Paragraph corresponde a la unidad textural, que puede ser el documento completo (documents), p√°rrafos (paragraphs) o oraciones (sentences). Las categor√≠as m√°s frecuentes aparecen en la columna siguiente (Main Category) y todas las categor√≠as encontradas en la columna Paragraph aparecen en All Categories, inclu√≠das, por supuestos, las m√°s frecuentes. Matches informa el n√∫mero de veces una palabra o t√©rmino del diccionario se ha encontrado en el texto. Finalmente, Cat. No. informa el n√∫mero total de categor√≠as encontradas.\nAdicionalmente, debajo del nombre de cada columna, se pueden encontrar campos de filtro. Basta digitar cualquier valor o texto para seleccionar los resultados. Por ejemplo, si uno desea saber c√≥mo Calvo Sotelo trataba temas sociales, puede seleccionar solo los documentos que se inicien por ‚ÄúCalvo‚Äù y que, en All Categories, incluya el c√≥digo ‚Äúsocial‚Äù. Al hacer clic sobre el nombre de cada columna tambi√©n se pueden ordenar los valores de forma ascendente o descendente.\n\n\nCode\n# tagCorpus, hace algo parecido para un corpus\ntagCorpus(cp,\n          defaultPageSize = 4,\n          dic.pol.es, \n          palette = pal$cat.ggthemes.tableau.20, \n          reshape.to = \"sentences\", \n          show.details = T)\n\n\n\n\n\n\n\nConsideremos otro ejemplo. Si queremos identificar cu√°les actores sociales y pol√≠ticos mencionados en los discursos de investidura de los presidentes espa√±oles que est√©n vinculados al tema tecnol√≥gico, podemos filtrar las sentencias del corpus en las que el tema principal (Main Category) son los ‚Äúactores‚Äù y en que tambi√©n aparezcan (All Categories) ‚Äútecnologia‚Äù.\nVemos que la concepci√≥n tecnol√≥gica de los presidentes pasa por una actuaci√≥n clave de empresas y del mercado. Poco se menciona sobre el rol de la inversi√≥n en ciencia. Mucho inversor y poco investigador. La innovaci√≥n, por lo tanto, se da por la atracci√≥n de capital y de tecnolog√≠as desarrolladas por otros m√°s que por un proceso aut√≥nomo de construcci√≥n tecnol√≥gica a partir de la inversi√≥n en ciencia. No resulta para nada casual que casi la mitad del presupesto asignado a investigaci√≥n suela estar constituida por cr√©ditos destinados al I+D+I de empresas (en colaboraci√≥n con la universidad).",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "codificacion.html#an√°lisis-tem√°tico",
    "href": "codificacion.html#an√°lisis-tem√°tico",
    "title": "Codificaci√≥n tem√°tica",
    "section": "An√°lisis tem√°tico",
    "text": "An√°lisis tem√°tico\n¬øCu√°les c√≥digos tienen m√°s peso? ¬øQu√© categor√≠as se asocian de forma m√°s estrecha? Una vez creados los c√≥digos y los diccionarios, cabe dar un paso adelante y buscar patrones, identificar las caracter√≠sticas de los conjuntos de t√©rminos y sus relaciones con otros atributos en los textos.\nEl an√°lisis de la incidencia de los c√≥digos en un corpus y su interrelaci√≥n permiten explicitar patrones y definir el peso relativo de cada idea en los textos. Este apartado emplea tres estrategias para explorar la importancia de los temas. La primera consiste en averiguar el peso de las categor√≠as, es decir, emplear estad√≠sticas sumarias, como la frecuencia relativa de c√≥digos o expresiones, para establecer su prevalencia. La segunda se basa en la desagregaci√≥n y el filtro para comparar grupos o atributos o para seleccionar aspectos concretos que se desean examinar con m√°s detenimiento. La tercera investiga su asociaci√≥n por medio de las redes de coocurrencia.\nCombinadas, tales estrategias permiten identificar los temas centrales presentes en un texto o corpus y c√≥mo se relacionan entre s√≠. Tambi√©n revelan su variaci√≥n de acuerdo con variables contextuales, como puede ser un partido, el presidente o un per√≠odo de tiempo determinado. Se tratan de herramientas sencillas, pero muy √∫tiles, a la hora de contextualizar ideas e identificar variaciones importantes en el uso de conceptos o t√©rminos.\n\nEstad√≠sticas tem√°ticas\nDenominamos estad√≠sticas tem√°ticas el conjunto de t√©cnicas cuantitativas que permiten representar la importancia de categor√≠as o expresiones en un corpus. ¬øCu√°ntas veces los presidentes de gobierno han mencionado la ciencia en sus discursos de investidura? ¬øCu√°ntas han mencionado al terrorismo? ¬øQui√©nes han sido los que m√°s uso han hecho de la expresi√≥n ‚Äúg√©nero‚Äù o ‚Äúfuerzas de seguridad‚Äù?\nPor lo tanto, aunque sencillas, tales herramientas permiten delinear diferencias program√°ticas e ideol√≥gicas entre distintos actores pol√≠ticos. Sobre todo, se√±ala aquellas categor√≠as m√°s frecuentes, tanto por el n√∫mero de veces que aparecen como por la cantidad de documentos en los que aparecen. Por ejemplo, solo algunos de los presidentes mencionan el tema de g√©nero en sus discursos (en especial Jos√© Luis Rodr√≠guez Zapatero y Pedro S√°nchez). No obstante, temas como el mercado laboral o la fiscalidad del Estado, como esperado, aparecen en todos ellos (aunque acompa√±ados de distintos calificativos).\nEl panel abajo contiene un conjunto de recursos para el an√°lisis de los c√≥digos del diccionario dic.pol.es aplicado a los discursos de investidura de los presidentes de gobierno de Espa√±a. La primera pesta√±a (tabla) contiene la frecuencia relativa de los c√≥digos y t√©rminos del diccionario en el corpus. Las dem√°s corresponden a visualizaciones que permiten hacer una s√≠ntesis de los pesos relativos de palabras-clave y categor√≠as en los textos. Force Directed Tree genera un diagrama de √°rbol que representa la jerarqu√≠a de los t√©rminos como una red. Las dos alternativas siguientes (voronoi tree) generan una imagen parecida, pero con otros instrumentos de interacci√≥n y niveles de zoom. Esto permite mirar hacia los resultados de una forma ligeramente distinta.\n\nTablaForce-Directed TreeVoronoi (C√≥digos)Voronoi (keywords)\n\n\nLa funci√≥n countKeywords de tenet produce un data.frame con un conjunto de campos que auxilian en el an√°lisis del peso relativo de cada categor√≠a en un corpus determinado. El primero es groups, que indica el grupo (como partido o presidente, por ejemplo) que detalla los resultados. Si no se ha informado ninguna variable de grupo, aparecer√° ‚ÄúAll‚Äù (todos). El segundo, level1, se√±ala el c√≥digo de m√°s alto nivel en un diccionario (la funci√≥n admite hasta dos niveles de jerarqu√≠a, en esos casos aparecer√≠a tambi√©n level2). El tercero, keyword, indica la palabra-clave que conforma el diccionario y frequency muestra la frecuencia (absoluta o relativa del t√©rmino en el corpus).\nLa tabla abajo muestra la frecuencia relativa (por cada mil palabras) de cada t√©rmino del diccionario dic.pol.es en el corpus de discursos de investidura de los presidentes espa√±oles.\n\n\nC√≥digo\n# Calcula la frecuencia relativa en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario\nxy &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    quietly = TRUE)\n\n# Elimina los t√©rminos no encontrados\nxy &lt;- xy[xy$frequency&gt;0,]\n\n# Puesto que es una frecuencia relativa\n# multiplicamos por 10 mil para tener la\n# ratio de ocurrencia a cada 10 mil palabras\nxy$frequency &lt;- round(xy$frequency*10000, 1)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T)\n\n\n\n\n\n\n\n\nSi ordenamos por frecuencia, vemos que la categor√≠a m√°s general de discurso y, dentro de esta, Espa√±a, se destaca. A ella le sigue la figura ret√≥rica de ‚ÄúSe√±or‚Äù, que incluye todas las formas derivadas: se√±or, se√±ora, se√±or√≠a, se√±ores, y dem√°s. Se trata tambi√©n de una forma de tratamiento com√∫n en este tipo de texto en los que los candidatos a presidente de gobierno se refieren de forma respetuosa a los dem√°s representantes parlamentarios.\n\n\nAunque la tabla nos brinde los detalles de la frecuencia de cada t√©rmino, una visualizaci√≥n de todos los c√≥digos a la vez posibilita entender su peso relativo de forma instant√°nea y comparada. Un diagrama de √°rbol (force directed tree) representa cada categor√≠a en el diccionario como un √°rbol en el que cada c√≥digo es una rama y cada elemento una hoja que se atraen o repulsan de acuerdo con su peso relativo (Holten 2006). El tama√±o de cada c√≠rculo (rama o hoja) se define de acuerdo con su frecuencia y el color seg√∫n el nivel m√°s alto en el diccionario. A mayor peso, m√°s centralidad en el gr√°fico. De acuerdo con el ejemplo que se emplea aqu√≠, discurso tendr√° un color, social otro, exterior el suyo y as√≠ sucesivamente. Cada una de esas categor√≠as abarcadoras mantendr√° la misma estructura de c√≥digos secundarios y expresiones (o keywords) como figuran en el diccionario.\n\n\nC√≥digo\n# Genera el gr√°fico de √°rbol\nforceDirectedTree(xy,\n                  value_col = \"frequency\",\n                  attraction = -8,\n                  palette = pal$cat.awtools.bpalette.16, \n                  max.radius = 18, \n                  height = 500)\n\n\n\n\n\n\n\n\nComo se puede ver, la categor√≠a discurso es la que m√°s peso tiene en el corpus. Est√° conformada por expresiones de tratamiento como ‚Äúse√±or√≠as‚Äù o ‚Äúinvestidura‚Äù y relacionadas a Espa√±a, como ‚Äúespa√±oles‚Äù, ‚Äúpatria‚Äù o ‚Äúpueblo‚Äù. Viene seguida de temas sociales, de pol√≠tica exterior y fiscales.\n\n\nUn treemap corresponde a otra forma de visualizaci√≥n de datos jer√°rquicos. En este caso, todas las categor√≠as y sus subcategor√≠as se dividen en un c√≠rculo fragmentado en partes que se dimensionan seg√∫n la frecuencia de cada c√≥digo (Balzer and Deussen 2005).\n\n\nC√≥digo\n# Agrega las frecuencias seg√∫n los dos niveles\n# de c√≥digo contenidos en el diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency),\n                by=list(level1=xy$level1,\n                        level2=xy$level2),\n                sum, na.rm=TRUE)\n\n# Genera el gr√°fico\nplotVoronoiTree(data = xx,\n                value_col = \"frequency\")\n\n\n\n\n\n\nAl hacer clic sobre una categor√≠a, el gr√°fico hace un zoom y redistribuye el espacio solo con las subcategor√≠as del c√≥digo principal seleccionado.\n\n\nEl ejemplo abajo repite el gr√°fico anterior, pero ahora con las palabras-clave como unidades de divisi√≥n de las √°reas del c√≠rculo. Aqu√≠ se pueden observar el peso relativo de cada expresi√≥n en la configuraci√≥n de cada c√≥digo.\n\n\nC√≥digo\n# Agrega las frecuencias seg√∫n el primer\n# nivel de c√≥digo y las palabras clave \n# contenidas en el diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency),\n                by=list(level1=xy$level1,\n                        keyword=xy$keyword),\n                sum, na.rm=TRUE)\n\n# Genera el gr√°fico\nplotVoronoiTree(data = xx,\n                value_col= \"frequency\")\n\n\n\n\n\n\n\n\n\n\n\nUn an√°lisis r√°pido de los resultados subraya la importancia de formas ret√≥ricas y puramente discursivas en el corpus. El uso de expresiones de tratamiento, como se√±or√≠as, o relativos a Espa√±a o los espa√±oles predomina por su reiterada aparici√≥n. El tama√±o de la categor√≠a ‚Äúdiscurso‚Äù en todas las visualizaciones manifiesta claramente su predominio sobre los dem√°s temas.\nLa segunda categor√≠a de mayor importancia se encuentra relacionada con temas sociales. No sorprende que las cuestiones laborales, y en particular el empleo, constituyan elementos centrales de los discursos de todos los presidentes. Con relaci√≥n a las dem√°s √°reas de pol√≠tica social, se percibe un destaque muy particular a la educaci√≥n. Se trata de un tema alrededor del que los distintos partidos siempre han marcado sus diferencias program√°ticas. Tal protagonismo se ve reflejado en los discursos de investidura.\nEn pol√≠tica exterior, pesa mucho m√°s Europa frente a otros temas y al resto del mundo. Se observa una clara orientaci√≥n hacia el contexto regional frente a otros v√≠nculos pol√≠ticos m√°s tradicionales. Este patr√≥n se puede verificar claramente en la falta casi absoluta de protagonismo de Am√©rica Latina en los textos.\nLa menci√≥n a distintos actores sociales tambi√©n resulta √∫til para entender la relaci√≥n de los presidentes con diferentes sectores de la sociedad civil. En el corpus analizado, queda claro el destaque atribuido a las empresas y empresarios, vistos como promotores de crecimiento econ√≥mico. En segundo lugar, hay muchas referencias al propio partido o a aquellos que forman parte de la coalici√≥n de gobierno. Los trabajadores ocupan el √∫ltimo lugar.\nEl tema territorial aparece, principalmente, bajo la forma de acci√≥n administrativa del Estado hacia comunidades aut√≥nomas y la administraci√≥n local. No obstante, otros temas vinculados con la dimensi√≥n territorial de la organizaci√≥n del Estado espa√±ol, como el regionalismo y el terrorismo, tambi√©n presentan cierto destaque.\nLa tecnolog√≠a es vista como un motor de desarrollo. No obstante, la ciencia ocupa un rol marginal. En varios discursos, se trata de dar incentivos a empresas y atraer tecnolog√≠as desarrolladas en otros pa√≠ses m√°s que crear un sistema de investigaci√≥n robusto que permita la innovaci√≥n desde Espa√±a.\nEn relaci√≥n a las categor√≠as postmaterialistas, se observan dos patrones. El medioambiente conforma el tema con m√°s peso y con un car√°cter m√°s transversal. De una forma o de otra, todos los presidentes lo consideran un problema a atajar. No obstante, la diversidad sexual constituye un divisor de aguas. Aparecen con una frecuencia significativamente mayor en los discursos de los dos √∫ltimos presidentes socialistas y constituyen, de cierto modo, una marca de sus programas de gobierno.\n\n\nTemas por atributo\n¬øC√≥mo distintos partidos mencionan un tema? ¬øY los presidentes? En muchas ocasiones, el punto central del an√°lisis consiste en comparar c√≥mo los temas var√≠an seg√∫n un atributo cualquiera como, por ejemplo, la ideolog√≠a, el tiempo, o distintas regiones o pa√≠ses.\nEn algunos casos, interesa desagregar los datos generales por atributo y examinar c√≥mo los patrones var√≠an seg√∫n cada valor o grupo. En otros, el objetivo consiste el filtrar o seleccionar algunos valores para explorarlos en profundidad. La capacidad de manipulaci√≥n de datos representa uno de los puntos fuertes de R. Resulta muy sencillo realizar b√∫squedas y selecciones de datos a partir de criterios l√≥gicos. Por esa raz√≥n, emplear tales capacidades en favor de un an√°lisis de datos m√°s detallado consiste en algo sencillo.\nEl panel abajo desagrega los datos presentados anteriormente por partidos y por presidente, as√≠ como filtra los resultados solo para el c√≥digo ‚Äúpostmaterialismo‚Äù. En las pesta√±as tabla, se presentan las frecuencias desagregadas por ambas variables y, en las pesta√±as Sankey, se presentan los datos en un diagrama aluvial conocido como diagrama de Sankey (A. B. W. Kennedy and Sankey 1898; Riehmann, Hanfler, and Froehlich 2005).\n\nTabla: PartidosSankey: PartidosTabla: Presid.Sankey: Presid.Filtro\n\n\nLa tabla abajo presenta las frecuencias relativas desagregadas por partido de cada c√≥digo y expresi√≥n contenida en el diccionario dic.pol.es. Como en los ejemplos anteriores, se ha empleado el corpus de los discursos de investidura de los presidentes de gobierno de Espa√±a. La √∫nica diferencia con el ejemplo anterior est√° en el uso del par√°metro group.var=‚ÄúPartido‚Äù en la funci√≥n countKeywords, que establece que los resultados ahora deben ser desagrupados por el partido pol√≠tico del presidente.\n\n\nC√≥digo\n# Carga el paquete quanteda\nlibrary(quanteda)\n\n# A√±ade la variable partido al corpus cp\n# que hemos creado anteriormente\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\",\n                            \"PSOE\", \"PSOE\", \"PSOE\",\n                            \"PP\", \"PP\", \"PSOE\",\n                            \"PSOE\", \"PP\", \"PP\",\n                            \"PP\", \"PSOE\", \"PSOE\")\n\n# Obtiene la frecuencia relativa de los\n# t√©rminos contenidos en el diccionario\n# dic.pol.es desagregados por la variable\n# partido.\nxp &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    group.var = \"Partido\",\n                    quietly = TRUE)\n\n# Agrega los resultados por los dos niveles\n# de c√≥digo del diccionario\nxx &lt;- aggregate(list(frequency=xp$frequency), \n                 by=list(groups=xp$groups, \n                         level1=xp$level1,\n                         level2=xp$level2), \n                 sum, na.rm=T)\n \n# Elimina los t√©rminos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualizaci√≥n de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Visualiza los resultados\nreactable(xx, \n          filterable = T,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\nEl diagrama de Sankey abajo representa los resultados de la tabla anterior. Cada barra de la izquierda corresponde a un partido y de la derecha a un c√≥digo del diccionario. Los v√≠nculos de cada partido a cada categor√≠a se hacen visible cuando se pasa el cursor sobre una de las barras. Si el cursor est√° sobre un partido, se muestran sus v√≠nculos con todas las categor√≠as. Si, por otra parte, se pone sobre una categor√≠a, se se√±alan todos los partidos y la intensidad con la que se vinculan.\n\n\nC√≥digo\n# Agrega las frecuencias relativas seg√∫n\n# el grupo (partido) y el segundo nivel \n# de c√≥digos (m√°s detallado)\nxx &lt;- aggregate(list(frequency=xp$frequency), \n                 by=list(groups=xp$groups,\n                         level2=xp$level2), \n                 sum, na.rm=T)\n\n# Elimina los t√©rminos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualizaci√≥n de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Genera el gr√°fico\nplotSankey(xx, \n           from = \"groups\", \n           to=\"level2\", \n           value = \"frequency\", \n           opacity = 0.05)\n\n\n\n\n\n\n\n\nSi pasamos el cursor sobre el c√≥digo ‚Äúdemocracia‚Äù, por ejemplo, vemos que la UCD contiene el mayor n√∫mero de menciones. Se trata de algo absolutamente esperado, puesto que los presidentes de este partido (Adolfo Su√°rez y Leopoldo Calvo-Sotelo) han sido los primeros a ocupar el cargo durante la transici√≥n a la democracia. Si consideramos los t√©rminos ‚ÄúEspa√±a‚Äù y ‚Äúempresas‚Äù vemos que el PP, a su vez, contiene un mayor protagonismo, aunque en el √∫ltimo caso, su preponderancia resulta modesta frente a los dem√°s grupos pol√≠ticos.\n\n\nEn este caso, los datos se desagregan por presidente.\n\n\nC√≥digo\n# Obtiene la frecuencia relativa de los\n# t√©rminos contenidos en el diccionario\n# dic.pol.es desagregados por la variable\n# President.\nxz &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    group.var = \"President\",\n                    quietly = TRUE)\n\n# Agrega los resultados por: los grupos\n# (cada uno de los presidentes) y los dos \n# niveles de c√≥digo del diccionario\nxx &lt;- aggregate(list(frequency=xz$frequency), \n                 by=list(groups=xz$groups, \n                         level1=xz$level1,\n                         level2=xz$level2), \n                 sum, na.rm=T)\n\n# Elimina los t√©rminos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualizaci√≥n de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Visualiza los resultados\nreactable(xx, \n          filterable = T,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\nEl mismo diagrama, pero ahora desagregado por presidente.\n\n\nC√≥digo\n# Agrega las frecuencias relativas seg√∫n\n# el grupo (presidentes) y el segundo nivel \n# de c√≥digos (m√°s detallado)\nxx &lt;- aggregate(list(frequency=xz$frequency), \n                 by=list(groups=xz$groups,\n                         level2=xz$level2), \n                 sum, na.rm=T)\n \n# Elimina los t√©rminos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualizaci√≥n de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Genera el gr√°fico\nplotSankey(xx, \n           from = \"groups\", \n           to=\"level2\", \n           value = \"frequency\", \n           opacity = 0.05)\n\n\n\n\n\n\n\n\nAqu√≠ vemos c√≥mo cada presidente utiliza los t√©rminos. Resulta muy llamativo el uso de expresiones relacionadas a ‚ÄúEspa√±a‚Äù por Mariano Rajoy, ‚Äúgenero‚Äù por Pedro S√°nchez o ‚Äúfiscal‚Äù por Aznar. Tales c√≥digos les destacan frente a los dem√°s y nos permiten identificar las caracter√≠sticas de sus discursos que les singularizan.\n\n\nFiltrado de valores\nTambi√©n podemos filtrar los valores para centrar la atenci√≥n a una categor√≠a o c√≥digo espec√≠fico. En algunos casos, como g√©nero o medioambiente, por ejemplo, resulta dif√≠cil ver las diferencias en un gr√°fico dada su peque√±o peso frente a otras categor√≠as m√°s frecuentes. En el ejemplo abajo, seleccionamos solamente los c√≥digos de segundo nivel relacionados al ‚Äúpostmaterialismo‚Äù, es decir, cuestiones de g√©nero, medioambiente y memoria hist√≥rica.\n\n\nC√≥digo\n# Filta el resultado de los presidentes\n# para mantener solo los valores relativos\n# a la categor√≠a \"postmaterialismo\"\nx1 &lt;- xz[xz$level1==\"postmaterialismo\",]\n\n# Agrega las frecuencias relativas seg√∫n\n# el grupo (presidentes) y el segundo nivel \n# de c√≥digos (m√°s detallado)\nxx &lt;- aggregate(list(frequency=x1$frequency), \n                 by=list(groups=x1$groups,\n                         level2=x1$level2), \n                 sum, na.rm=T)\n \n# Elimina los t√©rminos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualizaci√≥n de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Genera el gr√°fico\nplotSankey(xx, \n           from = \"groups\", \n           to=\"level2\", \n           value = \"frequency\", \n           opacity = 0.05)\n\n\n\n\n\n\n\n\nComo vemos, en los temas postmateriales hay un predominio de presidentes de gobierno del PSOE, en especial Zapatero y S√°nchez. No obstante, en algunos temas como la memoria hist√≥rica y el medioambiente, presidentes de otros partidos tambi√©n aparecen con menciones, aunque en menor grado.\n\n\n\n\n\nRedes tem√°ticas\n¬øQu√© c√≥digos siempre se mencionan juntos? ¬øQu√© otros nunca aparecen en una misma frase, p√°rrafo o documento? El an√°lisis de la asociaci√≥n entre categor√≠as constituye otro recurso muy √∫til para identificar patrones en los textos y facilitar el an√°lisis del contenido de los mismos. Dicha tarea constituye el n√∫cleo del desarrollo de redes tem√°ticas, construidas a partir de la abstracci√≥n de c√≥digos hacia conjuntos interrelacionados de temas (Attride-Stirling 2001). En los discursos pol√≠ticos t√©rminos como democracia o libertad tienden a estar asociados a otras expresiones que les califican y permiten asignar una posici√≥n ideol√≥gica concreta. Por ese motivo, el an√°lisis de las redes de asociaci√≥n tem√°tica permiten avanzar a√∫n m√°s en la comprensi√≥n de los patrones existentes en el contenido de los documentos que componen un corpus.\nEl panel abajo trabaja con dos niveles. El primero examina la relaci√≥n entre c√≥digos de m√°s alto nivel como actores, instituciones, pol√≠tica exterior o fiscal. El segundo baja un escal√≥n y trata de los c√≥digos menos abstractos como laboral, europa, retorica, genero. Para cada nivel los datos se muestran tanto bajo la forma de una tabla con los t√©rminos y el n√∫mero de veces que aparecen juntos como en un diagrama de cuerdas (chord diagram) que permite la visualizaci√≥n de redes cuyos nodos se encuentran densamente asociados entre s√≠ (Bremer and Wu 2012).\n\nTabla (nivel 1)Cuerdas (nivel 1)Tabla (nivel 2)Cuerdas (nivel 2)Grid\n\n\nLa tabla abajo ha sido producida a partir de la funci√≥n matchCodes que examina la coocurrencia de los c√≥digos de un diccionario determinado en un corpus. Aqu√≠ se emplean los discursos de investidura organizados seg√∫n sentencias y se busca mapear la asociaci√≥n entre los c√≥digos de m√°s alto nivel del diccionario dic.pol.es. El resultado es un data.frame con tres columnas: term1, correspondiente al primer t√©rmino, term2, representando el segundo c√≥digo, y value, que contiene el n√∫mero de veces en que esas dos categor√≠as aparecen en una misma unidad textual del corpus (sentencia, p√°rrafo o documento entero).\n\n\nC√≥digo\n# Reorganiza el corpus seg√∫n\n# sentencias o frases\ncs &lt;- corpus_reshape(cp, \"sentences\")\n\n# Calcula la frecuencia en la\n# que dos codigos del mismo \n# diccionario aparecen juntos\n# en cada frase\nd1 &lt;- matchCodes(cs, \n                dic.pol.es, \n                level = 1, \n                quietly=TRUE)\n\n# Ordena los resultados de mayor a menor\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\n# Visualiza los resultados\n# Obs.: En esta versi√≥n el c√≥digo resulta\n# m√°s largo porque inclu√≠mos un gr√°fico de\n# barras en la tabla. Si no quisi√©ramos ver\n# el gr√°fico bastar√≠a con el c√≥digo:\n# &gt; reactable(d1)\n\nlibrary(htmltools)\n\n# Crea una funci√≥n que transformar√°\n# los valores de frecuencia en \n# barras a ser representadas en una\n# o m√°s columnas de la tabla.\nbar_chart &lt;- function(label, \n                      width = \"100%\", \n                      height = \"1rem\", \n                      fill = \"purple\", \n                      background = NULL) {\n  \n  bar &lt;- div(\n          style = list(\n                    background = fill, \n                    width = width, \n                    height = height)\n          )\n  \n  chart &lt;- div(\n          style = list(\n                    flexGrow = 1, \n                    marginLeft = \"0.5rem\", \n                    background = background), \n          bar)\n  \n  div(\n    style = list(\n            display = \"flex\", \n            alignItems = \"center\"), \n    label, \n    chart)\n}\n\n# Visualiza los resultados\nreactable::reactable(\n            d1, \n            resizable = T,\n            filterable = T,            \n            columns = list(\n                        value = colDef(\n                        name = \"value\", \n                        align = \"left\", \n                        cell = function(value) {\n                            width &lt;- paste0(\n                                      value / max(d1$value) * 100, \n                                      \"%\")\n                            bar_chart(value, width = width)\n                            }\n                          )\n                        )\n            )\n\n\n\n\n\n\n\n\nAl examinar los resultados, la d√≠ada m√°s frecuente corresponde a discurso-instituciones, con 746 ocurrencias, seguida de discurso-exterior, con 489, y discurso-social, con 358. Las menos frecuentes son instituciones-postmaterialismo, con 4 ocurrencias, y defensa-postmaterialismo, con 5.\n\n\nEl diagrama de cuerdas abajo revela el patr√≥n en su conjunto, algo m√°s dif√≠cil de observar solo por el examen de la tabla anterior. Adem√°s de discurso, temas sociales, e instituciones son los que m√°s se asocian entre s√≠ y con las dem√°s categor√≠as. Postmaterialismo, defensa y tecnolog√≠a los que menos.\n\n\nC√≥digo\n# Genera el gr√°fico\nplotChord(d1, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")\n\n\n\n\n\n\n\n\nLa tabla abajo repite la operaci√≥n, pero ahora para las categor√≠as de segundo nivel. Ahora, la d√≠ada parlamento-retorica predomina, con 344 apariciones. Espa√±a-retorica viene en segundo lugar, con 299 ocurrencias. Se tratan claramente de referencias al mismo Congreso de los Diputados y a los espa√±oles y a Espa√±a. Tales asociaciones corresponden a lo que ya hemos visto en an√°lisis anteriores.\n\n\nC√≥digo\n# Reordena el corpus seg√∫n sentencia\n# o frase.\ncs &lt;- corpus_reshape(cp, \"sentences\")\n\n# Calcula las coocurrencias, pero ahora\n# para el segundo nivel del diccionario\nd2 &lt;- matchCodes(cs, \n                dic.pol.es, \n                level = 2, \n                quietly=TRUE)\n\n# Ordena de los mayores a menores valores\nd2 &lt;- d2[order(d2$value, decreasing = T),]\n\n\n# Visualiza los resultados\n# Obs.: En esta versi√≥n el c√≥digo resulta\n# m√°s largo porque inclu√≠mos un gr√°fico de\n# barras en la tabla. Si no quisi√©ramos ver\n# el gr√°fico bastar√≠a con el c√≥digo:\n# &gt; reactable(d1)\n\nlibrary(htmltools)\n\n# Crea una funci√≥n que transformar√°\n# los valores de frecuencia en \n# barras a ser representadas en una\n# o m√°s columnas de la tabla.\nbar_chart &lt;- function(label, \n                      width = \"100%\", \n                      height = \"1rem\", \n                      fill = \"purple\", \n                      background = NULL) {\n  \n  bar &lt;- div(\n          style = list(\n                    background = fill, \n                    width = width, \n                    height = height)\n          )\n  \n  chart &lt;- div(\n          style = list(\n                    flexGrow = 1, \n                    marginLeft = \"0.5rem\", \n                    background = background), \n          bar)\n  \n  div(\n    style = list(\n            display = \"flex\", \n            alignItems = \"center\"), \n    label, \n    chart)\n}\n\n# Visualiza los resultados\nreactable::reactable(\n            d2, \n            resizable = T,\n            filterable = T,\n            columns = list(\n                        value = colDef(\n                        name = \"value\", \n                        align = \"left\", \n                        cell = function(value) {\n                            width &lt;- paste0(\n                                      value / max(d2$value) * 100, \n                                      \"%\")\n                            bar_chart(value, width = width)\n                            }\n                          )\n                        )\n            )\n\n\n\n\n\n\n\n\n\n\nEl diagrama de cuerdas abajo se√±ala las relaciones entre las categor√≠as de segundo nivel. Las categor√≠as que m√°s se vinculan a otras son ret√≥rica, Espa√±a, administraci√≥n y otros temas de car√°cter social. Las menos asociadas son g√©nero, polic√≠a, memoria hist√≥rica y medioambiente.\n\n\nC√≥digo\n# Genera el gr√°fico\nplotChord(d2, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplotGrid(d2, standardize = T,\n         palette = pal$cat.ggsci.ucscgb.26,\n         x=\"term1\", \n         y=\"term2\", \n         size=\"value\", \n         color=\"term1\", \n         width_svg = 12, height_svg = 8, \n         leg.size=\"Menciones\", \n         leg.color=\"C√≥digo/Tema\",  \n         grid.color=\"grey99\")",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "codificacion.html#consideraciones-finales",
    "href": "codificacion.html#consideraciones-finales",
    "title": "Codificaci√≥n tem√°tica",
    "section": "Consideraciones finales",
    "text": "Consideraciones finales\nEn esta parte hemos explorado el uso de estrategias de an√°lisis que facilitan la selecci√≥n, la codificaci√≥n y el an√°lisis tem√°tico de textos. Como ya ha sido mencionado de forma extensiva, no se trata de reemplazar m√©todos cualitativos tradicionales, sino de ofrecer entradas alternativas para el an√°lisis de contenido. La mayor√≠a de las t√©cnicas presentadas aqu√≠ tienen en mente un contexto h√≠brido de investigaci√≥n. De un lado, se desea mantener al m√°ximo el rigor de un examen en profundidad del corpus. De otro, se quiere expandir el abanico de opciones disponibles para la identificaci√≥n de patrones, su comunicaci√≥n y el desarrollo de nuevas ideas.\nCabe subrayar que, aunque las estrategias desarrolladas aqu√≠ tengan en mente corpus con un n√∫mero elevado de textos, las herramientas que se han presentado pueden emplearse tambi√©n para una cantidad peque√±a de documentos. Entrevistas, libros, cuadernos de campo, todos pueden ser objeto de la aplicaci√≥n de los m√©todos discutidos aqu√≠. Para un investigador acostumbrado a estudios cuantitativos, representa una oportunidad para examinar nuevas fuentes de informaci√≥n con mayor profundidad. Para los que se basan en m√©todos cualitativos, se abre la posibilidad de expandir sus instrumentos de trabajo con nuevas formas de visualizaci√≥n y exploraci√≥n del material emp√≠rico con el que trabajan.",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "codificacion.html#ejercicios",
    "href": "codificacion.html#ejercicios",
    "title": "Codificaci√≥n tem√°tica",
    "section": "Ejercicios",
    "text": "Ejercicios\nEjercicio 1. Keyword in Context (kwic). Utilice la funci√≥n kwic para buscar el t√©rmino ‚Äúigualdad‚Äù en el corpus de los discursos de investidura de los presidentes de gobierno espa√±oles. Utilice una ventana de 7 palabras antes y despu√©s.\n\n\nSoluci√≥n\n# Carga los paquetes tenet y quanteda\nlibrary(tenet)\nlibrary(quanteda)\n\n# Crea un corpus (discursos inaugurales Espana)\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un data.frame a partir de\n# la funcion Keyword in Context de\n# Quanteda \nd &lt;- kwic(x = tokens(cp),\n          pattern= \"igualdad\",\n          window = 7)\n\n# Visualiza los resultados\nView(d)\n\n\nEjercicio 2. √Årbol de palabras. Crea un √°rbol de palabras empleando el mismo corpus y la misma palabra del ejercicio anterior. Puedes intentarlo tambi√©n con otras palabras de tu inter√©s.\n\n\nSoluci√≥n\n# Crea un arbol de palabras en tenet\nwordtree(corpus = cp,\n         keyword = \"igualdad\",\n         height = 800)\n\n# Palabra alternativa: empleo\n# (puede ser cualquiera que elijas)\nwordtree(corpus = cp,\n         keyword = \"empleo\",\n         height = 800)\n\n\nEjercicio 3. Etiquetado de textos. Seleccione el √∫ltimo discurso inaugural de Pedro S√°nchez (texto 15) de la base de datos spa.inaugural y emplee las palabras clave ‚Äútecnol‚Äù, ‚Äúciencia‚Äù, ‚Äúcient√≠f‚Äù, y ‚Äúdigital‚Äù.\n\n\nSoluci√≥n\n# Examina un conjunto de palabras en el\n# discurso de investidura de Adolfo Su√°rez\ntagText(as.character(spa.inaugural$text[15]), \n        keywords = c(\"tecnol\", \n                     \"digital\", \n                     \"ciencia\", \n                     \"cient√≠f\"),\n        title = \"Pedro Sanchez (2019)\")\n\n\nEjercicio 4. Etiqueta do textos con diccionario. Repita el ejercicio anterior. No obstante, ahora, utilice el diccionario de la secci√≥n ‚ÄúDiccionarios como colecciones de c√≥digos‚Äù para etiquetar el texto de Pedro S√°nchez. Utilice la paleta de colors ‚ÄúFantasticFox1‚Äù para colorear las categor√≠as, un tama√±o de fuente de 20 y unos m√°rgines de 120 pixeles.\n\n\nSoluci√≥n\n# Crea el diccionario\ndic &lt;- dictionary(\n  list(\n    economica=c(\"econom\",\n               \"inversion\",\n               \"empresa\",\n               \"desarroll\",\n               \"monetari\",\n               \"industri\",\n               \"agric\",\n               \"agrari\"),\n    fiscal=c(\"hacienda\",\n               \"gasto\",\n               \"impuest\",\n               \"presupuest\",\n               \"tribut\",\n               \"tasa\",\n               \"fiscal\"),\n    educacion=c(\"educa\",\n             \"profesor\",\n             \"docent\",\n             \"escuel\",\n             \"colegio\",\n             \"universi\",\n             \"formaci√≥n\"),\n    sanidad=c(\"sanidad\",\n               \"salud\",\n               \"hospital\",\n               \"sanitari\",\n               \"m√©dic\",\n               \"enfermer\",\n               \"salud\"),\n    medioambiente=c(\"sostenible\",\n                 \"cambio clima\",\n                 \"medioambient\",\n                 \"reciclaje\",\n                 \"ecol√≥gico\",\n                 \"l√≠mpia\",\n                 \"invernadero\",\n                 \"emisiones\",\n                 \"carbono\",\n                 \"pl√°stico\",\n                 \"f√≥siles\")))\n\n# genera un texto para ser le√≠do en el panel\n# Viewer de RStudio\ntagText(spa.inaugural$text[15], \n        keywords = dic, \n        palette = pal$cat.wesanderson.FantasticFox1.5,\n        font.size = 20, \n        title = \"Pedro Sanchez (2019)\",\n        margin = 120)\n\n\nEjercicio 5. Etiquetado de corpus. Ahora emplee el mismo diccionario para etiquetar todo el corpus de discursos de investidura. Reorganice los documentos en p√°rrafos (‚Äúparagraph‚Äù) en lugar de emplear documentos enteros.\n\n\nSoluci√≥n\n# La funci√≥n tagCorpus etiqueta\n# todo un corpus de acuerdo con un\n# diccionario\ntagCorpus(cp,\n          dic, \n          reshape.to = \"paragraph\")\n\n\nEjercicio 6. Etiquetado de corpus. Ahora repita el √∫ltimo ejercicio. Pero ahora crea un nuevo diccionario con los t√©rminos que m√°s te interesen.\n\n\nSoluci√≥n\n# Crea un nuevo diccionario, he elegido\n# algunas pol√≠ticas p√∫blicas concretas\ndic.nuevo &lt;- dictionary(\n  list(\n    gobierno=c(\"gobierno\",\n               \"instituciones\",\n               \"comunidades\",\n               \"autonom\",\n               \"administracion\",\n               \"entidades\"),\n    politica=c(\"politica\",\n               \"partido\",\n               \"accion\",\n               \"cortes\"),\n    patria=c(\"patria\",\n             \"nacion\",\n             \"espanoles\",\n             \"pueblo\"),\n    justicia=c(\"constituci\",\n               \"justicia\",\n               \"judic\",\n               \"ley\",\n               \"legal\"),\n    financiero=c(\"financ\",\n                 \"hacienda\",\n                 \"presupuesto\",\n                 \"tributa\",\n                 \"fiscal\"),\n    equidad=c(\"desigual\", \n              \"igualdad\")))\n\n# Etiqueta los textos seg√∫n el nuevo\n# diccionario\ntagCorpus(cp,\n          dic.nuevo, \n          reshape.to = \"paragraph\")\n\n\nEjercicio 7. Estad√≠sticas tem√°ticas. Utilice el diccionario empleado en el ejercicio 4 para contar la frecuencia de cada c√≥digo en el corpus de discursos de investidura. Recuerda que debes utilizar la funci√≥n countKeywords. No elimine los c√≥digos con frecuencia igual a cero y utilice la frecuencia absoluta.\n\n\nSoluci√≥n\n# Calcula la frecuencia ABSOLUTA en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario dic\nxy &lt;- countKeywords(cp, \n                    dic, \n                    quietly = TRUE)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T,\n                     filterable = T)\n\n\nEjercicio 8. Diagrama de √°rbol. Utilice los resultados del ejercicio anterior para crear un diagrama de √°rbol (Force Directed Tree). No obstante, ahora elimine las palabras clave con frecuencia igual a cero. Utilice ‚Äúlevel1‚Äù y ‚Äúkeywords‚Äù como grupos y ‚Äúkeywords‚Äù como elementos.\n\n\nSoluci√≥n\n# Elimina los valores con frecuencia\n# igual a cero\nxy &lt;- xy[xy$frequency&gt;0,]\n\n# Formatea los datos para que puedan \n# ser representados en el gr√°fico\njs &lt;- jsonTree(data = xy, \n               groups=c(\"level1\",\"keyword\"), \n               elements = \"keyword\", \n               value=\"frequency\")\n\n# Genera el gr√°fico\nforceDirectedTree(js)\n\n\nEjercicio 9. √Årbol de Voronoi. Utilice los resultados del ejercicio 7 para crear ahora un diagrama de Voronoi (Voronoi Treemap). No obstante, ahora elimine las palabras clave con frecuencia igual a cero. Utilice ‚Äúlevel1‚Äù y ‚Äúkeywords‚Äù como grupos y ‚Äúkeywords‚Äù como elementos.\n\n\nSoluci√≥n\n# Genera el gr√°fico\nplotVoronoiTree(data = xy,\n                groups = \"level1\",\n                elements = \"keyword\",\n                value = \"frequency\")\n\n\nEjercicio 10. Estad√≠sticas tem√°ticas: filtro. Repita la operaci√≥n del ejercicio 7, pero ahora desagrega los resultados por presidente y emplea la frecuencia relativa.\n\n\nSoluci√≥n\n# Calcula la frecuencia ABSOLUTA en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario dic\nxy &lt;- countKeywords(cp, \n                    dic,\n                    group.var = \"President\",\n                    quietly = TRUE,\n                    rel.freq = T)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T,\n                     filterable = T)\n\n\nEjercicio 11. Diagrama aluvial de Sankey. Genere un diagrama de Sankey con los resultados del ejercicio anterior. No olvides de agregar los valores por grupo (‚Äúgroups‚Äù) y por c√≥digo (‚Äúlevel1‚Äù) y eliminar los c√≥digos con frecuencia igual a cero.\n\n\nSoluci√≥n\n# Agrega los resultados por presidente y\n# por c√≥digo del diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency), \n                 by=list(groups=xy$groups,\n                         level1=xy$level1), \n                 sum, na.rm=T)\n\n# Elimina los t√©rminos sin correspondencia\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# General el gr√°fico \nplotSankey(xx, \n           from = \"groups\", \n           to=\"level1\", \n           value = \"frequency\")\n\n\nEjercicio 12. Coocurrencia de c√≥digos. Reorganice el corpus en el nivel de p√°rrafos. Calcule las coocurrencias de los c√≥digos del diccionario dic.\n\n\nSoluci√≥n\n# Reorganiza el corpus seg√∫n p√°rrafos\ncs &lt;- corpus_reshape(cp, \"paragraph\")\n\n# Calcula las coocurrencias\nd1 &lt;- matchCodes(cs, \n                dic, \n                level = 1, \n                quietly=TRUE)\n\n# Ordena las frecuencias de mayor a menor\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\n# Visualiza los resultados\nView(d1)\n\n\nEjercicio 13. Diagrama de cuerdas. Utilice los resultados del ejercicio anterior para crear un diagrama de cuerdas que permita visualiza las conexiones entre los distintos t√©rminos del diccionario dic aplicado al corpus de discursos de investidura.\n\n\nSoluci√≥n\n# Visualiza los resultados\nplotChord(d1, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "codificacion.html#lecturas-adicionales",
    "href": "codificacion.html#lecturas-adicionales",
    "title": "Codificaci√≥n tem√°tica",
    "section": "Lecturas adicionales",
    "text": "Lecturas adicionales\n\n\nAttride-Stirling J (2001). ‚ÄúThematic networks: an analytic tool for qualitative research.‚Äù Qualitative research, 1(3), 385-405.\n\n\nCon un n√∫mero de 8.178 citas en Google Acad√©mico, el art√≠culo de Attride-Stirling se ha convertido en referencia casi obligatoria en el proceso de codificaci√≥n tem√°tica. Se destaca por presentar los temas no como ideas sueltas creadas a partir de c√≥digos, sino como redes jer√°rquicas que van desde los referentes emp√≠ricos (en nuestro caso los textos) hasta el tema central del estudio.\n\n\nKennedy BL, Thornberg R (2018). ‚ÄúDeduction, induction, and abduction.‚Äù In The SAGE handbook of qualitative data collection, chapter 4, 49-64. SAGE Publications, London.\n\n\nEl texto de Kennedy y Tornberg representa una excelente introducci√≥n a las l√≥gicas inductiva, deductiva y abductiva. Su consulta resulta muy recomendable, si deseas conocer m√°s sobre las principales caracter√≠sticas y diferencias entre esos tipos de razonamiento o m√©todos de construcci√≥n de conocimiento.\n\n\nSaldana J (2015). The Coding Manual for Qualitative Researchers Third Edition. SAGE, Los Angeles ; London.\n\n\nEl libro de Salda√±a corresponde a una gu√≠a detallada del proceso de codificaci√≥n tem√°tica. No solo detalla los distintos tipos de c√≥digo como sugiere algunas heur√≠sticas para aplicarlos a los textos. Se trata de una referencia b√°sica para el proceso de creaci√≥n de c√≥digos.\n\n\nThompson J (2022). ‚ÄúA guide to abductive thematic analysis.‚Äù The Qualitative Report, 5(27), 1410-1421.\n\n\nEl breve art√≠culo de Thompson suministra una gu√≠a paso a paso de c√≥mo emplear un razonamiento abductivo en el an√°lisis tem√°tico en las ciencias sociales. Constituye una lectura interesante para aquellos que desean una orientaci√≥n m√°s detallada de c√≥mo llevar a cabo el proceso de an√°lisis de texto.",
    "crumbs": [
      "Codificaci√≥n"
    ]
  },
  {
    "objectID": "escalonado.html#introducci√≥n",
    "href": "escalonado.html#introducci√≥n",
    "title": "Escalonado de textos",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\nEn esta sesi√≥n vamos a explorar distintas t√©cnicas de comparaci√≥n y escalonado de textos. Tales herramientas permiten establecer qu√© textos se acercan entre s√≠ y medir las distancias entre ellos, algo que resulta especialmente √∫til cuando deseamos identificar patrones o desarrollar tipolog√≠as.",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#wordscores",
    "href": "escalonado.html#wordscores",
    "title": "Escalonado de textos",
    "section": "Wordscores",
    "text": "Wordscores\nEl m√©todo de Wordscores (Laver, Benoit, and Garry 2003) es una t√©cnica supervisada de escalonado de textos que asigna puntuaciones a documentos bas√°ndose en un conjunto de textos de referencia previamente clasificados. Utiliza la frecuencia de las palabras para calcular la proximidad de cada documento a los textos de referencia o anclas que definen las posiciones en una escala. Por ejemplo, si tenemos discursos pol√≠ticos clasificados como ‚Äúizquierda‚Äù, ‚Äúcentro‚Äù y ‚Äúderecha‚Äù, podemos usar Wordscores para evaluar nuevos discursos y determinar su posici√≥n en el espectro pol√≠tico bas√°ndonos en su similitud con los textos de referencia. Sin embargo, contiene una serie de limitaciones de car√°cter estad√≠stico que dificultan la comparaci√≥n de textos distintos y problemas de compatibilidad entre escalas, extensamente discutido por Lowe (2008).\nSe trata de un m√©todo ‚Äúcuqui‚Äù, ‚Äúsencillito‚Äù, muy novedoso y hermoso por su simplicidad. Para 2003, cuando no exist√≠a pr√°cticamente nada, representaba una innovaci√≥n importante en lo que se pod√≠a hacer con textos. No obstante, hoy no es m√°s que un trofeo a la obsolescencia cient√≠fica en la era de la IA. Por eso, rendimos homenaje a la t√©cnica y no perderemos el tiempo ni siquiera en intentar implementarla.",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#escalonado-de-textos",
    "href": "escalonado.html#escalonado-de-textos",
    "title": "Escalonado de textos",
    "section": "Escalonado de textos",
    "text": "Escalonado de textos\nEl escalonado de textos es una t√©cnica utilizada en el an√°lisis de textos para posicionar documentos en un espacio multidimensional basado en sus caracter√≠sticas ling√º√≠sticas. A trav√©s de algoritmos espec√≠ficos, se pueden identificar patrones y relaciones entre los textos, permitiendo compararlos y clasificarlos seg√∫n sus similitudes y diferencias.",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#wordfish",
    "href": "escalonado.html#wordfish",
    "title": "Escalonado de textos",
    "section": "Wordfish",
    "text": "Wordfish\nPreparaos, que ahora empezamos la secci√≥n de m√©todos relacionados con peces. Wordfish es una t√©cnica bastante buena que emplea estad√≠stica bayesiana para determinar la posici√≥n de textos en una escala unidimensional (Slapin and Proksch 2008; Proksch and Slapin 2010). A partir de la distribuci√≥n de las palabras, determina la proximidad entre m√∫ltiples textos y los ubica en una escala com√∫n.\nA diferencia de Wordscores, Wordfish no requiere textos de referencia preclasificados, lo que lo hace m√°s flexible para analizar conjuntos de textos diversos. Adem√°s, se basa en un modelo estad√≠stico de distribuci√≥n (poisson), algo que faltaba a la t√©cnica anterior. El gran tal√≥n de Aquiles de la herramienta viene de su propia fortaleza: la suposici√≥n de que los textos pueden ser representados adecuadamente en una sola dimensi√≥n (Slapin and Proksch 2008).\nComo veremos, el tema de la dimensionalidad es crucial para el escalonado de textos. Las t√©cnicas que consideramos aqu√≠ comprimen la variedad y riqueza de los textos a una escala unidimensional. Cuando trabajamos con textos extensos y que contienen m√°s de un tema o dimensi√≥n sustantiva eso puede resultar en la dificultad de identificar a qu√© exactamente se refiere la escala producida por el an√°lisis. A veces tenemos una escala ideol√≥gica clara, pero en muchas otras la interpretaci√≥n no resulta tan sencilla.\nAqu√≠, la selecci√≥n de los textos para el an√°lisis resulta fundamental. Si empleamos debates o textos que se centran en un tema o que claramente permiten el mapeo de posiciones en una escala ideol√≥gica, los resultados son m√°s potentes. Por esa raz√≥n, y en mi experiencia aplicando el m√©todo, el wordfish resulta particularmente interesante para ubicar actores o entidades en un espacio tem√°tico concreto.\n\n\nCode\n# carga los paquetes\nrequire(quanteda)\nrequire(quanteda.textmodels)\nrequire(quanteda.textplots)\n\n# crea el corpus y los tokens\ncp &lt;- corpus(spa.inaugural)\n\ntk &lt;- tokens(cp, \n            remove_punct = TRUE)\n\ntk &lt;- tokens_remove(tk, \n                       pattern = stopwords(\"es\"))\n\n# Crea la matriz de frecuencias\ndf &lt;- dfm(tk)\n\nwf &lt;- textmodel_wordfish(df)\n\n# Muestra los resultados\ntextplot_scale1d(wf)\n\n\n\n\n\n\n\n\n\nEn el ejemplo anterior hemos utilizado los discursos de investidura de los presidentes del gobierno espa√±ol. El resultado es una escala que ubica a los presidentes en funci√≥n de las palabras que emplean en sus discursos. En ese caso, todo el corpus ha sido empleado para ubicar a los textos.\nNo obstante, podemos sacar m√°s provecho de la t√©cnica al aplicarla a temas o debates concretos. El c√≥digo abajo aplica el algoritmo al debate sobre el proyecto de Ley Org√°nica del Sistema Universitario (LOSU). He listado tambi√©n los n√∫meros de expediente de otros proyectos de ley para que podamos ver c√≥mo los distintos partidos se posicionan frente a diferentes propuestas legislativas:\n\n\nCode\n# Expedientes de temas concretos\n\n# Libertad sexual: 121/000062\n# Memoria democr√°tica: 121/000064\n# Creacion de empresas: 121/000075\n# Transformaci√≥n energetica: 121/000019\n# Deporte: 121/000082\n# Derecho a la vivienda: 121/000089\n# LOSU: 121/000111\n\n# Crea una base de datos con las sesiones\nsp &lt;- spa.sessions\n\n# Selecciona el debate deseado\nlibrary(stringi)\n\nsx &lt;- sp[which(stri_detect_fixed(sp$issue.details,\"121/000111\")==TRUE),]\nsx &lt;- sx[sx$rep.condition!=\"Miembro de la mesa\",]\n\n# Agrega los textos por partido\nag &lt;- aggregate(\n        list(text=sx$speech.text), \n        by=list(party=sx$rep.party), \n        FUN=paste, \n        collapse=\"\\n\")\n\n# Crea el corpus\ncp &lt;- corpus(ag, \n             docid_field = \"party\")\n\n# Crea los tokens y los limpia\ntk &lt;- tokens(cp, \n            remove_punct = TRUE)\n\ntk &lt;- tokens_remove(tk, \n                       pattern = stopwords(\"es\"))\n\n# Crea la matriz de frecuencias\ndf &lt;- dfm(tk)\n\n# Ejecuta el wordfish\nwf &lt;- textmodel_wordfish(df)\n\n# Muestra los resultados\ntextplot_scale1d(wf)",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#escalonado-unidimensional",
    "href": "escalonado.html#escalonado-unidimensional",
    "title": "Escalonado de textos",
    "section": "Escalonado unidimensional",
    "text": "Escalonado unidimensional\n\nWordscores\nEl m√©todo de Wordscores (Laver, Benoit, and Garry 2003) es una t√©cnica de escalonado de textos que asigna puntuaciones a documentos bas√°ndose en un conjunto de textos de referencia previamente clasificados. Utiliza la frecuencia de las palabras para calcular la proximidad de cada documento a los textos de referencia o anclas que definen las posiciones en una escala. Por ejemplo, si tenemos discursos pol√≠ticos clasificados como ‚Äúizquierda‚Äù, ‚Äúcentro‚Äù y ‚Äúderecha‚Äù, podemos usar Wordscores para evaluar nuevos discursos y determinar su posici√≥n en el espectro pol√≠tico bas√°ndonos en su similitud con los textos de referencia. Sin embargo, contiene una serie de limitaciones de car√°cter estad√≠stico que dificultan la comparaci√≥n de textos distintos y problemas de compatibilidad entre escalas, extensamente discutido por Lowe (2008).\nSe trata de un m√©todo ‚Äúcuqui‚Äù, ‚Äúsencillito‚Äù, muy novedoso y hermoso por su simplicidad. Para 2003, cuando no exist√≠a pr√°cticamente nada, representaba una innovaci√≥n importante en lo que se pod√≠a hacer con textos. No obstante, hoy no es m√°s que un trofeo a la obsolescencia cient√≠fica en la era de la IA. Por eso, rendimos homenaje a la t√©cnica y no perderemos el tiempo ni siquiera en intentar implementarla.\n\n\nWordfish\nPreparaos, que ahora empezamos la secci√≥n de m√©todos relacionados a peces. Wordfish es una t√©cnica bastante buena que emplea estad√≠stica bayesiana para determinar la posici√≥n de textos en una escala unidimensional (Slapin and Proksch 2008; Proksch and Slapin 2010; Lauderdale and Herzog 2016). A partir de la distribuci√≥n de las palabras, determina la proximidad entre m√∫ltiples textos y los ubica en una escala com√∫n.\nA diferencia de Wordscores, Wordfish no requiere textos de referencia preclasificados, lo que lo hace m√°s flexible para analizar conjuntos de textos diversos. Adem√°s, se basa en un modelo estad√≠stico de distribuci√≥n (poisson), algo que faltaba a la t√©cnica anterior. El gran tal√≥n de Aquiles de la herramienta viene de su propia fortaleza: la suposici√≥n de que los textos pueden ser representados adecuadamente en una sola dimensi√≥n (Slapin and Proksch 2008).\nComo veremos, el tema de la dimensionalidad es crucial para el escalonado de textos. Las t√©cnicas que consideramos aqu√≠ comprimen la variedad y riqueza de los textos a una escala unidimensional. Cuando trabajamos con textos extensos y que contienen m√°s de un tema o dimensi√≥n sustantiva eso puede resultar en la dificultad de identificar a qu√© exactamente se refiere la escala producida por el an√°lisis. A veces tenemos una escala ideol√≥gica clara, pero en muchas otras la interpretaci√≥n no resulta tan sencilla.\nAqu√≠, la selecci√≥n de los textos para el an√°lisis resulta fundamental. Si empleamos debates o textos que se centran en un tema o que claramente permiten el mapeo de posiciones en una escala ideol√≥gica, los resultados son m√°s potentes. Por esa raz√≥n, y en mi experiencia aplicando el m√©todo, el wordfish resulta particularmente interesante para ubicar actores o entidades en un espacio tem√°tico concreto.\n\n\nCode\nrequire(quanteda)\nrequire(quanteda.textmodels)\nrequire(quanteda.textplots)\n\ncp &lt;- corpus(spa.inaugural)\n\ntk &lt;- tokens(cp, \n            remove_punct = TRUE)\n\n\ntk &lt;- tokens_remove(tk, \n                       pattern = stopwords(\"es\"))\n\ndf &lt;- dfm(tk)\n\nwf &lt;- textmodel_wordfish(df)\n\ntextplot_scale1d(wf)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsp &lt;- spa.sessions\n\nlibrary(stringi)\n\n\n# Luego temas concretos\n\n# Libertad sexual: 121/000062\n# Memoria democr√°tica: 121/000064\n# Creacion de empresas: 121/000075\n# Transformaci√≥n energetica: 121/000019\n# Deporte: 121/000082\n# Derecho a la vivienda: 121/000089\n# LOSU: 121/000111\nsx &lt;- sp[which(stri_detect_fixed(sp$issue.details,\"121/000111\")==TRUE),]\nsx &lt;- sx[sx$rep.condition!=\"Miembro de la mesa\",]\n\nag &lt;- aggregate(\n        list(text=sx$speech.text), \n        by=list(party=sx$rep.party), \n        FUN=paste, \n        collapse=\"\\n\")\n\n\ncp &lt;- corpus(ag, docid_field = \"party\")\n\ntk &lt;- tokens(cp, \n            remove_punct = TRUE)\n\n\ntk &lt;- tokens_remove(tk, \n                       pattern = stopwords(\"es\"))\n\ndf &lt;- dfm(tk)\n\nwf &lt;- textmodel_wordfish(df)\n\ntextplot_scale1d(wf)\n\n\n\n\n\n\n\n\n\nCode\nsx &lt;- sp[which(stri_detect_fixed(sp$issue.details,\"121/000064\")==TRUE),]\nsx &lt;- sx[sx$rep.condition!=\"Miembro de la mesa\",]\n\nag &lt;- aggregate(\n        list(text=sx$speech.text), \n        by=list(party=sx$rep.party), \n        FUN=paste, \n        collapse=\"\\n\")\n\n\ncp &lt;- corpus(ag, docid_field = \"party\")\n\ntk &lt;- tokens(cp, \n            remove_punct = TRUE)\n\n\ntk &lt;- tokens_remove(tk, \n                       pattern = stopwords(\"es\"))\n\ndf &lt;- dfm(tk)\n\nwf &lt;- textmodel_wordfish(df)\n\ntextplot_scale1d(wf)\n\n\n\n\n\n\n\n\n\n\n\nWordshoal\nWordshoal es una extensi√≥n del m√©todo Wordfish que permite el escalonado de textos en m√∫ltiples dimensiones (Proksch and Slapin 2010). A diferencia de Wordfish, que asume una sola dimensi√≥n, Wordshoal utiliza un modelo estad√≠stico que puede capturar varias dimensiones tem√°ticas simult√°neamente. Esto es especialmente √∫til cuando los textos analizados abordan m√∫ltiples temas o cuando se desea explorar diferentes aspectos de los textos.",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#escalonado-multidimensional",
    "href": "escalonado.html#escalonado-multidimensional",
    "title": "Escalonado de textos",
    "section": "Escalonado multidimensional",
    "text": "Escalonado multidimensional\n\nAn√°lisis de correspondencias\nEl an√°lisis de correspondencias es una t√©cnica estad√≠stica que permite representar datos categ√≥ricos en un espacio multidimensional. En el contexto del an√°lisis de textos, se utiliza para identificar relaciones entre palabras y documentos, facilitando la visualizaci√≥n de similitudes y diferencias entre ellos. A trav√©s de esta t√©cnica, es posible mapear textos en un espacio donde la proximidad entre ellos refleja su similitud tem√°tica o ling√º√≠stica (greenacre2017?).",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#wordshoal",
    "href": "escalonado.html#wordshoal",
    "title": "Escalonado de textos",
    "section": "Wordshoal",
    "text": "Wordshoal\nWordshoal es una t√©cnica de escalonado de textos que permite medir las variaciones en la posici√≥n de los actores en diferentes debates o subconjuntos de texto (Lauderdale and Herzog 2016). Se trata de la herramienta idea para identificar polarizaci√≥n en discursos pol√≠ticos. T√≠pico chiste friki de usuarios de R (rvest, para harvest, dplyr, por deeply R, o muchos otros paquetes similares en R), el nombre mismo hace referencia a la t√©cnica anterior: mientras que el wordfish miraba los peces de palabras (word - fish), el wordshoal mira los card√∫menes o bancos de palabras (word - shoal).\nA diferencia de Wordfish, que generaba una estad√≠stica √∫nica para cada unidad de texto, esta t√©cnica permite medir la variaci√≥n en el posicionamiento de partidos o diputados en distintos debates. Eso permite no solo identificar aquellos actores m√°s coherentes en todo el corpus, sino tambi√©n identificar los debates m√°s polarizados. Trabaja en dos pasos. Primero, calcula el wordfish para cada debate por separado. Luego, lleva a cabo un an√°lisis factorial bayesiano para integrar todos los debates.\nAlgunos de los par√°metros estimados:\n\ntheta - posici√≥n estimada de cada documento (en nuestro caso, la posici√≥n estimada de cada partido en los debates como un todo)\npsi - posiciones estimadas a nivel de debate (en nuestro caso, la posici√≥n de cada partido en cada debate)\nbeta - Efectos marginales del debate\nalpha - Efectos fijos del debate\n\n\n\nCode\n# Carga los datos de las sesiones\nsp &lt;- spa.sessions\n\n# Crea un identificador √∫nico para cada debate\nsp$debate_id &lt;- paste0(sp$session.number,\"_\",sp$issue.details)\n\n# Elimina las intervenciones de los\n# miembros de la mesa\nsp &lt;- sp[sp$rep.condition!=\"Miembro de la mesa\",]\n\n# Selecciona solo aquellos textos con m√°s\n# de 15 palabras\nlibrary(stringi)\nsp$nwords &lt;- stri_count_words(sp$speech.text)\nsp &lt;- sp[sp$nwords&gt;15,]\n\n# Selecciona solo aquellos debates con m√°s\n# de 4 intervinientes distintos\nag &lt;- aggregate(sp$rep.name,\n                by=list(debate_id=sp$debate_id),\n                FUN=function(x) length(unique(x)))\n\nag &lt;- ag[ag$x&gt;4,]\nsp &lt;- sp[sp$debate_id%in%ag$debate_id,]\n\n\n# Prepara los datos para el an√°lisis\nlibrary(quanteda)\nlibrary(wordshoal)\n\n# Agrega los textos por debate y partido\nag &lt;- aggregate(\n        list(text=sp$speech.text), \n        by=list(debate_id=sp$debate_id,\n                partido=sp$rep.party), \n        FUN=paste, \n        collapse=\"\\n\")\n\n# Crea el corpus\ncp &lt;- corpus(ag, \n             text_field = \"text\")\n\n# Crea los tokens\ndf &lt;- dfm(tokens(cp, remove_punct=T))\n\n\n# Corre el wordshoal\nws &lt;- textmodel_wordshoal(df,\n                          groups=docvars(cp,\"debate_id\"), \n                          authors=docvars(cp, \"partido\"))\n\n\n\nScaling 785 document groups...................20 ............\n\n\n.......40 ...................60 ...................80 ...................100 ......\n\n\n.............120 ...................140 ...................160 ...................180 ...................200 ...................220 ...................240 ...................260 \n\n\n...................280 ...................300 ...................320 ...................340 ...................360 ...............\n\n\n..\n\n\n..380 ...................400 ...................420 ...............\n\n\n....440 ...................460 ...................480 ...................500 ...................520 ...................540 ...................\n\n\n560 ...................580 ...................600 ...................620 ...................640 ...................660 ...................680 ...................700 ...................720 .......\n\n\n............740 ...................760 ...................780 .....\nFactor Analysis on Debate-Level Scales.................................\nElapsed time: 157.554 seconds.\n\n\nCode\n# Extrae las posiciones por debate y partido\ndd &lt;- data.frame(debate=ws$groups, \n                 partido=ws$authors, \n                 psi=round(ws$psi,2))\n\n# Muestra los resultados\nlibrary(reactable)\n\nreactable(dd,\n         columns = list(\n           debate = colDef(name = \"Debate\"),\n           partido = colDef(name = \"Partido\"),\n           psi = colDef(name = \"Posici√≥n\")\n         ),\n         searchable = TRUE,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)\n\n\n\n\n\n\nPodemos ver la distribuci√≥n de posiciones por partido en los debates mediante un gr√°fico de densidad:\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggridges)\n\nggplot(dd, aes(x=psi, y=partido, fill=partido))+\n  geom_density_ridges()+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title.position = \"plot\",\n        plot.title=ggtext::element_markdown(size=18))+\n  labs(title = \"**Distribuci√≥n de posiciones por partido&lt;br&gt;**\",\n       x = \"Posici√≥n (psi)\",\n       y = \"Partido\")\n\n\n\n\n\n\n\n\n\nAhora, identificamos los debates m√°s polarizados, es decir, aquellos en los que la diferencia entre la posici√≥n m√°xima y m√≠nima de los partidos es mayor:\n\n\nCode\n# Agrega las posiciones por debate\nag &lt;- aggregate(list(range=dd$psi),\n                by=list(debate=dd$debate),\n                FUN=range)\n\n# Calcula el rango de posiciones\nag$min &lt;- round(ag$range[,1],2) \nag$max &lt;- round(ag$range[,2],2) \nag$dif &lt;- round(ag$range[,2] - ag$range[,1],2)\nag$range &lt;- NULL\n\n# Ordena los debates por polarizaci√≥n\nag &lt;- ag[order(ag$dif, decreasing=TRUE),]\n\n# Muestra los resultados\nlibrary(reactable)\n\nreactable(ag,\n         columns = list(\n           debate = colDef(name = \"Debate\"),\n           min = colDef(name = \"M√≠nimo\"),\n           max = colDef(name = \"M√°ximo\"),\n           dif = colDef(name = \"Diferencia\")\n         ),\n         searchable = TRUE,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#an√°lisis-de-correspondencias",
    "href": "escalonado.html#an√°lisis-de-correspondencias",
    "title": "Escalonado de textos",
    "section": "An√°lisis de correspondencias",
    "text": "An√°lisis de correspondencias\nEl an√°lisis de correspondencias es una t√©cnica estad√≠stica que permite representar datos categ√≥ricos en un espacio multidimensional. En el contexto del an√°lisis de textos, se utiliza para identificar relaciones entre palabras y documentos, facilitando la visualizaci√≥n de similitudes y diferencias entre ellos. A trav√©s de esta t√©cnica, es posible mapear textos en un espacio donde la proximidad entre ellos refleja su similitud tem√°tica o ling√º√≠stica (Greenacre 2017). Tambi√©n podemos ver c√≥mo los componentes son capaces de resumir o sintetizar la varianza entre las posiciones pol√≠ticas.\n\n\nCode\n# Carga los paquetes\nlibrary(quanteda.textmodels)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n# Selecciona el debate deseado\n# En nuestro caso ser√° el de \n# la memooria democr√°tica: 121/000064\nsp &lt;- spa.sessions\nsx &lt;- sp[which(stri_detect_fixed(sp$issue.details,\"121/000064\")==TRUE),]\nsx &lt;- sx[sx$rep.condition!=\"Miembro de la mesa\",]\n\n# Agrega los textos por partido\nag &lt;- aggregate(\n        list(text=sx$speech.text), \n        by=list(party=sx$rep.party), \n        FUN=paste, \n        collapse=\"\\n\")\n\n# Crea el corpus\ncp &lt;- corpus(ag, \n             docid_field = \"party\")\n\n# Crea los tokens y los limpia\ntk &lt;- tokens(cp, \n            remove_punct = TRUE)\n\ntk &lt;- tokens_remove(tk, \n                       pattern = stopwords(\"es\"))\n\n# Crea la matriz de frecuencias\ndf &lt;- dfm(tk)\n\n# Ejecuta el an√°lisis de correspondencias\ncca &lt;- textmodel_ca(df)\n\n# Extrae las coordenadas de los cap√≠tulos\ndd &lt;- data.frame(cca$rowcoord)\ndd$name &lt;- as.character(rownames(dd))\ndd &lt;- dd[, c(\"name\", \"Dim1\", \"Dim2\")]\n\n# crea un gr√°fico de dispersi√≥n \n# para examinar los resultados\nggplot(dd, aes(x = Dim1, y = Dim2)) +\n  geom_point() +\n  geom_text_repel(aes(label = name)) +\n  theme_classic() +\n  theme(legend.position = \"none\")+\n  geom_hline(yintercept = 0, \n             color = \"red3\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 0, \n             color = \"red3\", \n             linetype = \"dashed\")+\n  xlab(\"Dimensi√≥n 1\") +\n  ylab(\"Dimensi√≥n 2\") +\n  labs(title = \"An√°lisis de correspondencias (Ley de Memoria Hist√≥rica)\")\n\n\n\n\n\n\n\n\n\nAdem√°s, podemos emplear componentes con menos peso (componente 3, 4, 5) para entender, por ejemplo, subtemas o matices en las posiciones de los partidos que no se captan por medio de un an√°lisis unidimensional. Podemos tambi√©n aplicar otras t√©cnicas de escalonado multidimensonal, como PCA o MDS.",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#an√°lisis-de-cluster",
    "href": "escalonado.html#an√°lisis-de-cluster",
    "title": "Escalonado de textos",
    "section": "An√°lisis de cluster",
    "text": "An√°lisis de cluster\nEl an√°lisis de conglomerados o clustering es una t√©cnica estad√≠stica que agrupa objetos similares en conjuntos o ‚Äúclusters‚Äù bas√°ndose en sus caracter√≠sticas compartidas. En el contexto del an√°lisis de textos, se utiliza para identificar grupos de documentos que comparten temas, estilos o vocabularios similares. Al aplicar esta t√©cnica, podemos descubrir patrones ocultos en grandes conjuntos de textos, facilitando la comprensi√≥n de su estructura y contenido. No es nada de otro mundo. Lo que hacemos es solamente aplicar la t√©cnica ya conocida a una matriz de frecuencia de palabras.\nUtilicemos el debate sobre la Ley de Memoria Democr√°tica para ilustrar c√≥mo funciona el an√°lisis de conglomerados en textos:\n\n\nCode\n# Crea los tokens\ntk &lt;- tokens(cp, \n             remove_punct = T, \n             remove_numbers = T, \n             remove_separators = T, \n             remove_symbols = T)\n\n# Remueve las palabras vac√≠as\ntk &lt;- tokens_remove(tk, \n                    pattern=stopwords(\"es\"))\n\n# Crea la matriz de frecuencias\ndtm &lt;- dfm(tk)\n\n# Carga el paquete necesario\nlibrary(quanteda.textstats)\n\n# Calcula la distancia entre \n# los cap√≠tulos y los agrupa en\n# conglomerados\ntstat_dist &lt;- as.dist(textstat_dist(dtm))\nclust &lt;- hclust(tstat_dist)\n\n# Muestra los resultados\nplot(clust, xlab = \"Distance\", ylab = NULL)",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "escalonado.html#ia-embedding-de-textos",
    "href": "escalonado.html#ia-embedding-de-textos",
    "title": "Escalonado de textos",
    "section": "IA Embedding de textos",
    "text": "IA Embedding de textos\nAunque el curso inicialmente no tuviera la intenci√≥n de cubrir t√©cnicas de inteligencia artificial, creo que ser√≠a interesante presentar algunas herramientas m√°s novedosas y potentes que existen hoy para el an√°lisis de textos. Entre ellas, los embeddings de texto son representaciones vectoriales de palabras o documentos que capturan su significado sem√°ntico en un espacio multidimensional. Estas representaciones permiten comparar textos de manera m√°s efectiva, facilitando tareas como la clasificaci√≥n, la agrupaci√≥n y la b√∫squeda de similitudes.\nPara ello, emplear√© el paquete rollama que permite acceder a modelos de lenguaje avanzados desarrollados por Google. En este caso, utilizaremos el modelo ‚Äúsnowflake-arctic-embed2‚Äù para generar embeddings de los textos del debate sobre la Ley de Memoria Democr√°tica y luego compararlos con un texto espec√≠fico. Se puede tambi√©n emplear modelos online, por medio de APIs, como el modelo ‚Äúembedding-001‚Äù de Google, o similares de OpenAI, Claude, etc.\nLa ventaja del empleo de modelos locales es que son gratuitos y no requieren de claves de API. Podemos procesar una cantidad enorme de textos sin pagar m√°s que la energ√≠a. Desde y siempre, claro, que nuestro ordenador lo permita. No obstante, la mayor√≠a de dichos modelos pueden rodar en un Windows con 16GB de memoria o un Mac con chip M2/M3/M4 est√°ndar. Yo usar√© el proveedor local de modelos de IA llamado Ollama{target=‚Äú_blank} y el modelo ‚Äúsnowflake-arctic-embed2‚Äù. Existen modelos m√°s peque√±os y m√°s grandes. Este tiene un buen desempe√±o para su tama√±o, pero sugiero que prob√©is con varias opciones.\n\n\nCode\n# Calcula la similitud entre vectores\ncosSim &lt;- function(a,b, normalize=TRUE){\n  # assuming unit vectors\n  # the cosine is just the dot-product\n  \n  if(normalize==FALSE){\n    sim &lt;- a %*% b\n  }else{\n    \n    a_nrm &lt;- sqrt(rowSums(a^2))\n    \n    b_nrm &lt;- sqrt(sum(b^2))\n    \n    a_nrm &lt;- a / a_nrm\n    b_nrm &lt;- b / b_nrm\n    \n    cs &lt;- rowSums(a_nrm %*% b_nrm)\n    \n    nrm_sim &lt;- (cs + 1) / 2\n    \n\n    sim &lt;- nrm_sim\n  }\n  \n  return(sim)\n}\n\n# Selecciona los textos m√°s silimares, sea por\n# el n√∫mero de m√°s similares o por el valor de\n# la similitud\nsimTop &lt;- function(sim, top_n=5, top_val=NULL){\n  \n  rx &lt;- data.frame(id=1:length(sim), sim=sim)  \n  \n  rx &lt;- rx[order(rx$sim,decreasing=TRUE),]\n  \n  if(is.null(top_val)==TRUE){\n    rx &lt;- rx[1:top_n,]\n  }else{\n    rx &lt;- rx[rx$sim &gt; top_val,]  \n  }\n  \n  return(rx$id)  \n  \n}\n\n\n# Carga los paquetes\nlibrary(rollama)\nlibrary(quanteda)\n\n# Memoria democr√°tica: 121/000064\nsp &lt;- spa.sessions\nsx &lt;- sp[which(stri_detect_fixed(sp$issue.details,\"121/000064\")==TRUE),]\nsx &lt;- sx[sx$rep.condition!=\"Miembro de la mesa\",]\n\n# Crea un identificador √∫nico\nsx$id &lt;- paste0(sx$session.number,\"_\", sx$speech.order)\n\n# Crea el corpus y lo divide en oraciones\ncp &lt;- corpus(sx, text_field = \"speech.text\", docid_field = \"id\")\ncp &lt;- corpus_reshape(cp, to = \"sentences\")\nsx &lt;- convert(cp, to = \"data.frame\")\n\n# Calcula los embeddings de los textos\nem &lt;- embed_text(text = sx$text, \n                 model = \"snowflake-arctic-embed2\", \n                 verbose = T)\n\n# Convierte los embeddings a formato num√©rico\nex &lt;- sapply(em, as.numeric)\n\n# Agrega los embeddings al data frame\nsx$texto_embedded &lt;- ex\n\n# Ahora podemos buscar textos similares a uno dado\ntx &lt;- \"v√≠ctimas del terrorismo\"\n\n# Calcula el embedding del texto de consulta\nqt &lt;- embed_text(text = tx, \n                 model = \"snowflake-arctic-embed2\", \n                 verbose = T)\n\n# Convierte el embedding a formato num√©rico\nqt &lt;- as.numeric(qt)\n\n# Calcula la similitud coseno entre\n# el texto de consulta y los textos del corpus\nsim &lt;- cosSim(sx$texto_embedded, qt)\n\n# Muestra los 10 textos m√°s similares\nsx$text[simTop(sim, top_n=10)]\n\n\n [1] \"Por cierto, olvid√°ndose de las v√≠ctimas de los terroristas supuestamente antifranquistas, en realidad simples asesinos de la extrema izquierda m√°s radical, de ETA y otros movimientos y grupos terroristas durante la Dictadura.\"                     \n [2] \"Quiero tambi√©n recordar a las asociaciones de v√≠ctimas que nos han tra√≠do hasta aqu√≠.\"                                                                                                                                                                 \n [3] \"En 2020 hubo 193 homenajes a terroristas y el Gobierno no hizo nada.\"                                                                                                                                                                                  \n [4] \"Mientras se tramita una ley de memoria para las v√≠ctimas de la guerra y del franquismo, en paralelo tramitan ustedes una ley de amnesia para las v√≠ctimas del terrorismo.\"                                                                             \n [5] \"Me refiero a la dignificaci√≥n de las v√≠ctimas del franquismo, a su reconocimiento y a su reparaci√≥n plena.\"                                                                                                                                            \n [6] \"Veintid√≥s ni√±os m√°s fueron asesinados por la banda terrorista ETA, sin olvidar a las 857 v√≠ctimas de la banda y a los miles de heridos.\"                                                                                                               \n [7] \"Tenemos una deuda de gratitud con las v√≠ctimas del franquismo.\"                                                                                                                                                                                        \n [8] \"Si lo mat√≥ un terrorista, a tapar su recuerdo, que ahora mismo no nos conviene.\"                                                                                                                                                                       \n [9] \"Todas las v√≠ctimas, todas, son nuestras y deben ser reconocidos sus derechos, para as√≠ alcanzar plena reconciliaci√≥n.\"                                                                                                                                 \n[10] \"Al rev√©s, porque, entre otros aspectos, trata de maquillar la acci√≥n terrorista que durante d√©cadas caus√≥ estragos en Espa√±a, dej√≥ un reguero de 864 asesinados y decenas y decenas de miles de v√≠ctimas que fueron objeto de extorsi√≥n y persecuci√≥n.\"\n\n\nAhora, podemos emplear los embeddings para agrupar los textos en clusters bas√°ndonos en su similitud sem√°ntica:\n\n\nCode\n# Crea los clusters usando k-means\nset.seed(12358)\nkm &lt;- kmeans(ex, centers=10)\n\n# Agrega los clusters al data frame\nsx$cluster &lt;- km$cluster\n\n# Muestra los resultados\nsa &lt;- sx[,c(\"text\",\"rep.name\",\"rep.party\",\"cluster\")]\nsa &lt;- sa[order(sa$cluster),]\n\nlibrary(reactable)\n\nreactable(sa,\n         columns = list(\n           text = colDef(name = \"Texto\"),\n           rep.name = colDef(name = \"Diputado\"),\n           rep.party = colDef(name = \"Partido\"),\n           cluster = colDef(name = \"Cluster\")\n         ),\n         searchable = TRUE,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)\n\n\n\n\n\n\nPodemos tambi√©n usar los resultados del an√°lisis de cluster como un insumo para que la IA nos genere un t√≠tulo para cada cluster:\n\n\nCode\n# Carga el paquete ellmer para usar\n# la IA\nlibrary(ellmer)\n\n# Prepara el prompt para la IA\npt &lt;- \"# ROL\\n\\nAct√∫a como un sistema experto en an√°lisis tem√°tico.\\n\\n# TAREA\\n\\nLe suministrar√© una string CSV con las variables 'text', 'rep.name', 'rep.party', 'cluster'. Su objetivo es analizar los textos ('text') para cada 'cluster' y dar un t√≠tulo a cada uno de ellos de acuerdo con el tema central de los textos.\\n\\n# SALIDA\\n\\nEl formato de salida es una string CSV con el siguiente formato:\\n'cluster', 'titulo'\\n1,Titulo del cluster 1\\n\\n\\n# DATOS\\n\\nEl texto abajo contiene la string CSV de entrada:\\n\\n\"\n\n# Convierte los datos a formato CSV\nlibrary(readr)\nss &lt;- format_csv(sa)\n\n# Completa el prompt con los datos\npt &lt;- paste0(pt, ss, collapse = \"\\n\")\n\n# Crea el objeto de chat con Google Gemini\nchat &lt;- chat_google_gemini(\n    api_args = list(generationConfig = \n                      list(temperature=0.6, \n                           seed=12358)),  \n    model = \"gemini-flash-latest\", \n    echo = \"none\")\n\n# Ejecuta el chat\nout &lt;- chat$chat(pt)\n\n# Extrae la string CSV de la respuesta\nout &lt;- read_csv(out)\n\n# Asigna nombres a las columnas\nnames(out) &lt;- c(\"cluster\",\"titulo\")\n\n# Muestra los resultados\nreactable(out,\n         columns = list(\n           cluster = colDef(name = \"Cluster\", width = 90),\n           titulo = colDef(name = \"Titulo\", width = 800)\n         ),\n         searchable = TRUE,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)\n\n\n\n\n\n\nFinalmente, podemos emplear t√©cnicas de reducci√≥n de dimensionalidad, como el An√°lisis de Componentes Principales (PCA), para visualizar los textos en un espacio bidimensional basado en sus embeddings:\n\n\nCode\n# Realiza el an√°lisis de componentes principales\npc &lt;- prcomp(ex, center = TRUE, scale. = TRUE)\n\nlibrary(ggplot2)\nlibrary(ggiraph)\n\n# Prepara los datos para el gr√°fico\ndf_pca &lt;- data.frame(pc$x)\n\ndf_pca$id &lt;- sx$id\ndf_pca$party &lt;- sx$rep.party\ndf_pca$text &lt;- sx$text\n\n# Crea el gr√°fico interactivo\np &lt;- ggplot(df_pca, aes(x = PC1, y = PC2, color = party)) +\n  geom_point_interactive(aes(tooltip = text)) +\n  theme_minimal() +\n  labs(title = \"PCA de Embeddings de Textos\",\n       x = \"Componente Principal 1\",\n       y = \"Componente Principal 2\",\n       color = \"Partido\")+\n  facet_wrap(~party)+\n  theme_minimal()+\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")+\n  geom_hline(yintercept = 0, \n             linetype = \"dashed\")+\n  geom_vline(xintercept = 0, \n             linetype = \"dashed\")\n\ngirafe(ggobj = p,\n       width_svg = 8,\n       height_svg = 8)",
    "crumbs": [
      "Escalonado de textos"
    ]
  },
  {
    "objectID": "redes.html#introducci√≥n",
    "href": "redes.html#introducci√≥n",
    "title": "Redes sem√°nticas",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\nEl an√°lisis de redes sociales es una metodolog√≠a ya consolidada en las ciencias sociales para estudiar las relaciones entre entidades. Se trata de un m√©todo muy poderoso para entender estructuras de interacci√≥n, definir roles diferenciados y jerarquizar entidades seg√∫n su mayor o menor centralidad en la red. En ese sentido, un partido, un diputado o una palabra no son importantes por si solos. El mismo concepto de centralidad es multifac√©tico, puesto que existen diferentes formas de influencia que son complementarias y que se establecen fundamentalmente por la posici√≥n que cada entidad ocupa en la estructura m√°s amplia de la red.\nPor lo tanto, el an√°lisis de redes sociales permite situar una unidad (persona, instituci√≥n, palabra) en el contexto m√°s amplio en el que se ubica para entender qu√© rol ocupa y su importancia relativa en toda la estructura. Tales caracter√≠sticas nos permiten situar palabras en sus contextos m√°s amplios y revelar asociaciones entre ideas, conceptos o formas discursivas.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#temas-y-redes",
    "href": "redes.html#temas-y-redes",
    "title": "Redes sem√°nticas",
    "section": "Temas y redes",
    "text": "Temas y redes\nUna vez analizada la conectividad de los personajes, a partir de ahora exploraremos dos m√©todos de modelado de t√≥picos. Identificaremos los temas a partir de redes de palabras. Se considera que dos palabras est√°n conectadas si aparecen juntas en el texto. La red resultante permite identificar comunidades de palabras que presentan una mayor densidad de interacciones entre s√≠ que con el resto de la red. En el c√≥digo abajo, se extraen los t√≥picos a partir de la estructura de interacci√≥n entre las palabras:\nHemos creado una red g no dirigida (que no considera el orden) a partir de las coocurrencias de palabras en los di√°logos. Contiene 595 pares √∫nicos de palabras que se han mencionado juntas en m√°s de una ocasi√≥n. A continuaci√≥n, calcularemos las centralidades betwenness y degree para identificar las palabras m√°s importantes en la red y permitir que filtremos los resultados para facilitar su visualizaci√≥n:\n\nFinalmente, visualizamos los resultados en una red de palabras. En ella, los nodos representan las palabras y los enlaces las coocurrencias entre ellas. Los colores de los nodos indican la comunidad a la que pertenecen, mientras que el tama√±o de los nodos refleja su importancia en la red:\n\nAl analizar los resultados en la tabla y el gr√°fico, resulta m√°s sencillo identificar los temas a que pertenecen cada grupo de palabras. No solo eso, tambi√©n se evidencian las conexiones entre distintos temas. El primero tiene que ver con Dionisio y su habitaci√≥n. El segundo trata de objetos centrales como los sombreros de copa, las lucecitas en el puerto, los malabares o las farolas. Los clusters cinco y siete tratan, entre otras cosas, del di√°logo entre Paula y El odioso se√±or. El n√∫mero 8 trata del ni√±o que se ha ahogado y de la novia de Dionisio, llamada ‚Äúni√±a‚Äù por Don Sacramento, su padre.\n\n\nCode\nlibrary(ellmer)\n\npt &lt;- \"# ROL\\n\\nAct√∫a como un analista de datos despecialista en an√°lisis de redes sem√°nticas y modelado de t√≥picos.\\n\\n# TAREA\\n\\nLe suministrar√© una string CSV con palabras y clusteres que corresponden a temas, as√≠ como medidas de centralidad de red.\\nAnaliza los datos y sugiere un t√≠tulo para cada cluster y una breve descripci√≥n de cada tema / cluster.\\n\\n# SALIDA\\n\\nGenera una string CSV con las siguientes variables: cluster, t√≠tulo, descripci√≥n.\\n\\n# RESTRICCIONES\\n\\nNo introducir comentarios o cualquier otra marca que no sea la string CSV de salida.\\n\\n# DATOS DE ENTRADA\\n\\nAbajo puedes encontrar los datos CSV de entrada:\\n\\n\"\n\n\n# Convierte los datos a formato CSV\nlibrary(readr)\nss &lt;- format_csv(d4)\n\n# Completa el prompt con los datos\npt &lt;- paste0(pt, ss, collapse = \"\\n\")\n\n# Crea el objeto de chat con Google Gemini\nchat &lt;- chat_google_gemini(\n    api_args = list(generationConfig = \n                      list(temperature=0.6, \n                           seed=12358)),  \n    model = \"gemini-flash-latest\", \n    echo = \"none\")\n\n# Ejecuta el chat\nout &lt;- chat$chat(pt)\n\n# Extrae la string CSV de la respuesta\nout &lt;- read_csv(out)\n\n# Asigna nombres a las columnas\nnames(out) &lt;- c(\"cluster\",\"titulo\",\"descripcion\")\n\n# Muestra los resultados\nreactable(out,\n         searchable = TRUE,\n         wrap = F,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)\n\n\n\n\n\n\n\n\nCode\nda &lt;- spa.sessions[grep(\"N√∫mero de expediente 121/000064\", \n                        spa.sessions$issue.details),]\n\nda &lt;- da[da$rep.condition!=\"Miembro de la mesa\",]\n\n\nlibrary(ellmer)\n\npt &lt;- \"# ROL\\n\\nAct√∫a como un analista de datos experto en an√°lisis de discurso.\\n\\n# TAREA\\n\\nLe suministrar√© una string CSV con textos del debate parlamentario sobre el proyecto de ley de la memoria historica.\\nAnaliza los datos y extrae los principales temas debatidos.\\n\\n# SALIDA\\n\\nGenera una string CSV con las siguientes variables: tema, t√≠tulo, descripci√≥n (de al menos 500 palabras).\\n\\n# RESTRICCIONES\\n\\nNo introducir comentarios o cualquier otra marca que no sea la string CSV de salida.\\n\\n# DATOS DE ENTRADA\\n\\nAbajo puedes encontrar los datos CSV de entrada:\\n\\n\"\n\n\n# Convierte los datos a formato CSV\nlibrary(readr)\nss &lt;- format_csv(da)\n\n# Completa el prompt con los datos\npt &lt;- paste0(pt, ss, collapse = \"\\n\")\n\n# Crea el objeto de chat con Google Gemini\nchat &lt;- chat_google_gemini(\n    api_args = list(generationConfig = \n                      list(temperature=0.6, \n                           seed=12358)),  \n    model = \"gemini-flash-latest\", \n    echo = \"none\")\n\n# Ejecuta el chat\nout &lt;- chat$chat(pt)\n\n# Extrae la string CSV de la respuesta\not &lt;- read_csv(out)\n\n# Asigna nombres a las columnas\nnames(ot) &lt;- c(\"tema\",\"titulo\",\"descripcion\")\n\n# Muestra los resultados\nreactable(ot,\n         searchable = TRUE,\n         wrap = F,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#centralidad",
    "href": "redes.html#centralidad",
    "title": "Redes sem√°nticas",
    "section": "Centralidad",
    "text": "Centralidad\nEn la teor√≠a de redes, el protagonismo de un nodo puede medirse a trav√©s de distintas m√©tricas de centralidad (Wasserman and Faust 1994). Dichas t√©cnicas permiten evaluar diferentes formas de influencia que cada personaje ejerce en la red. Por ejemplo, si miramos solo el n√∫mero de personajes con quien habla cada uno, podr√≠amos decir que una palabra es m√°s importante que otras pues se encuentra vinculada a muchas otras m√°s. No obstante, si ponderamos todas las formas posibles de interacci√≥n, quiz√°s otras sean m√°s importantes por hacer puente entre ideas, por ejemplo. El esquema abajo muestra la anatom√≠a de una red social y los elementos estructurales que determinan las m√©tricas de centralidad m√°s comunes.\n\n\nAqu√≠ nos concentraremos en siete m√©tricas de centralidad que nos permiten evaluar el protagonismo de los nodos de una la red. Estas son:\n\nBetweenness: mide la cantidad de veces que un nodo act√∫a como intermediario en la red. Cuanto m√°s alto sea el valor de esta m√©trica, m√°s importante ser√° el nodo para conectar diferentes partes de la red. Es una medida de poder de intermediaci√≥n: cuanto m√°s conexiones intermedias tenga un nodo, m√°s control tendr√° sobre la difusi√≥n de informaci√≥n en la red. Esta es la medida que se suele emplear para medir la importancia de las palabras en una red sem√°ntica como ‚Äúpuentes conceptuales‚Äù entre campos tem√°ticos. Su importancia se encuentra en que indica las palabras que conectan distintas partes del discurso. Pero ojo, no miden las m√°s centrales en un tema. Para ello, deber√≠amos complementarla con otras m√©tricas que ya discutiremos a continuaci√≥n.\nDegree: mide el n√∫mero de conexiones que tiene un nodo con otros nodos. Es la medida m√°s com√∫n de centralidad: cuanto m√°s conexiones un nodo tiene, m√°s ‚Äúpoderoso‚Äù es. Sirve para medir los temas m√°s comunes o las palabras m√°s destacadas dentro de un tema.\nCloseness: mide la distancia promedio de un nodo a todos los dem√°s nodos. Se trata de una medida de posici√≥n: cuanto m√°s cerca est√© un nodo de los dem√°s, m√°s r√°pido podr√° difundir informaci√≥n a trav√©s de la red. En t√©rminos pr√°cticos eso significa medir el acceso r√°pido a otros nodos en la red. En an√°lisis de texto, sirve para medir las palabras sem√°nticamente m√°s cercanas al resto de palabras en un discurso.\nEigenvector: mide la importancia de un nodo en funci√≥n de la importancia de sus vecinos. Si un personaje interact√∫a con otro que tambi√©n es importante, su importancia aumenta. Revela las palabras m√°s prestigiosas o nucleares dentro del discurso dominante.\nPageRank: se trata de una medida basada en la anterior, pero que considera qui√©n se dirige a qui√©n. Por ejemplo, si una persona sigue a Taylor Swift en Twitter, no significa nada para su visibilidad en las redes. No obstante, si la cantante decide seguir a esta persona, su peso se ver√° aumentado de forma clara. Por lo tanto, nos permite identificar las palabras m√°s relevantes por su contexto, no solo por la frecuencia. Como Eigenvector, tiende a destacar t√©rminos alineados con el n√∫cleo ideol√≥gico del corpus, pues est√°n cerca de otras palabras dominantes.\nHub: mide la cantidad de conexiones que tiene un nodo con otros nodos importantes. En an√°lisis de textos, sirve para identificar palabras que distribuyen significado o activan otros conceptos. Act√∫an como motores del discurso, conectando hacia m√∫ltiples ideas. En textos pol√≠ticos, suelen ser verbos o conceptos-acci√≥n (‚Äúpromover‚Äù, ‚Äúgarantizar‚Äù, ‚Äúimpulsar‚Äù).\nAuthority: mide la cantidad de conexiones recibidas de otros nodos de tipo hub en la red. Una persona que se dirije a otros nodos influyentes posee un enorme potencial de difusi√≥n de su mensaje. Permite se√±alar aquellas palabras que concentran significado o reciben asociaciones sem√°nticas. Act√∫an como ejes tem√°ticos o nocionales (‚Äújusticia‚Äù, ‚Äúdesarrollo‚Äù, ‚Äúpaz‚Äù). En textos cient√≠ficos o te√≥ricos, pueden ser los conceptos-clave.\n\nEn el c√≥digo abajo, visualizamos la posici√≥n de cada uno de los personajes de la obra seg√∫n las distintas m√©tricas mencionadas:\n\n\nCode\nlibrary(igraph)\nlibrary(tidyverse)\nlibrary(networkD3)\nlibrary(quanteda)\n\ntexto &lt;- \"El gobierno impulsa la reconstrucci√≥n nacional.\nLa reconstrucci√≥n requiere unidad y trabajo conjunto.\nEl trabajo del pueblo fortalece la econom√≠a.\nLa econom√≠a s√≥lida garantiza la justicia social.\nLa justicia social consolida la paz y la democracia.\nEl gobierno promueve la paz a trav√©s del di√°logo.\"\n\n# Tokenizar texto\npalabras &lt;- unlist(strsplit(tolower(texto), \"\\\\W+\"))\npalabras &lt;- palabras[palabras != \"\"]\npalabras &lt;- palabras[!palabras %in% stopwords(\"es\")]\n\n# Crear aristas dirigidas palabra_i ‚Üí palabra_(i+1)\nedges &lt;- tibble(from = head(palabras, -1), to = tail(palabras, -1)) %&gt;%\n  group_by(from, to) %&gt;%\n  summarise(weight = n(), .groups = \"drop\")\n\n\nsimpleNetwork(edges, fontSize = 12)\n\n\n\n\n\n\nAhora calculamos las distintas m√©tricas de centralidad para cada palabra en la red y mostramos los resultados en una tabla interactiva:\n\n\nCode\n# Crea el gr√°fico de red\nge &lt;- graph_from_data_frame(edges)\n\n# Medidas b√°sicas\nV(ge)$degree &lt;- round(\n                degree(ge, \n                       mode = \"all\", \n                       normalized = TRUE),\n                2)\n\nV(ge)$closeness &lt;- round(\n                    closeness(ge, \n                              mode = \"all\", \n                              normalized = TRUE),\n                    2)\n\nV(ge)$betweenness &lt;- round(\n                      betweenness(ge, \n                                  directed = TRUE, \n                                  normalized = TRUE),\n                      2)\n\nV(ge)$eigenvector &lt;- round(\n                      eigen_centrality(ge, \n                                       directed = TRUE)$vector,\n                      2)\n\nV(ge)$hub &lt;- round(\n              hits_scores(ge)$hub,\n              2)\n\nV(ge)$authority &lt;- round(\n                    hits_scores(ge)$authority,\n                    2)\n\n# Prepara la visualizaci√≥n\ncentralidades &lt;- tibble(\n  palabra = names(V(ge)),\n  degree = V(ge)$degree,\n  closeness= V(ge)$closeness,\n  betweenness = V(ge)$betweenness,\n  eigenvector = V(ge)$eigenvector,\n  hub = V(ge)$hub,\n  authority = V(ge)$authority\n)\n\n# Ordena por betweenness\ncentralidades &lt;- centralidades[order(centralidades$betweenness, \n                                       decreasing = T),]\n\n\n# Muestra la tabla de centralidades\nreactable(centralidades, resizable=T)",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#topolog√≠a-de-redes",
    "href": "redes.html#topolog√≠a-de-redes",
    "title": "Redes sem√°nticas",
    "section": "Topolog√≠a de redes",
    "text": "Topolog√≠a de redes\nLa topolog√≠a de una red describe su estructura global y la forma en que los nodos est√°n interconectados. Diferentes topolog√≠as pueden influir en c√≥mo se difunde la informaci√≥n, c√≥mo se forman comunidades y c√≥mo se organizan los temas dentro de un corpus textual. Una rede puede tener forma de estrella en la que un nodo central conecta a todos los dem√°s nodos. Corresponde a la estructura m√°s centralizada de todas, puesto que una unidad es la responsable por la conexi√≥n y la transmisi√≥n de informaci√≥n en la red.\n\n\n\nTopolog√≠a de redes.\n\n\nOtra topolog√≠a com√∫n es la red en malla (mesh), donde los nodos est√°n interconectados de manera m√°s uniforme. Esta estructura permite una mayor redundancia y resiliencia, ya que la informaci√≥n puede fluir a trav√©s de m√∫ltiples caminos. En el contexto del an√°lisis de texto, una red en malla puede indicar un discurso m√°s diversificado y menos dependiente de un solo tema o concepto central.\nLas redes circulares forman cadenas interconectadas que pueden representar flujos de informaci√≥n c√≠clicos o. No hay casi jerarqu√≠a, pero ,si un eslab√≥n se rompe, la estructura de la red cambia hacia una lineal. Si la ruptura se da en dos partes, la red se fragmenta.\nLas redes jer√°rquicas (√°rbol) presentan una estructura en capas, donde los nodos superiores controlan o influyen en los nodos inferiores. Esta topolog√≠a es com√∫n en organizaciones y sistemas estructurados, as√≠ como en tipolog√≠as. En el an√°lisis de texto, una red jer√°rquica puede reflejar un discurso con niveles claros de importancia o autoridad entre los temas tratados.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#comunidades-o-clusters",
    "href": "redes.html#comunidades-o-clusters",
    "title": "Redes sem√°nticas",
    "section": "Comunidades o clusters",
    "text": "Comunidades o clusters\nLa estructura de interacci√≥n de los personajes tambi√©n permite identificar de m√≥dulos o grupos de personaje que presentan una mayor densidad de interacciones entre s√≠ que con el resto de la red. En la teor√≠a de redes, estos grupos se conocen como comunidades. En el c√≥digo abajo, identificamos las comunidades de personajes en Tres sombreros de copa:\n\n\nCode\nga &lt;- g\n\n# Crea un cluster de palabras\n# seg√∫n la conectividad de red\nset.seed(12358)\n\nwc &lt;- cluster_louvain(ga, \n                      weights = E(ga)$weight)\n\n\n# Atribuye la membres√≠a a los \n# grupos\nV(ga)$membership &lt;- wc$membership\n\n\n# Calcula las centralidades de red\n# betweenness y degree\nbt &lt;- round(betweenness(ga),3)\n\nd1 &lt;- data.frame(word=names(bt), \n                 betweenness=round(bt))\n\nbt &lt;- igraph::degree(ga)\n\nd2 &lt;- data.frame(word=names(bt), \n                 degree=bt)\n\nd3 &lt;- merge(d2, d1, by=\"word\")\n\nco &lt;- coreness(ga)\n\nd2 &lt;- data.frame(word=names(co), \n                 coreness=co)\n\nd3 &lt;- merge(d3, d2, by=\"word\")\n\n\nd3 &lt;- d3[order(d3$betweenness, \n               decreasing = T),]\n\n\n# Fusiona los datos de membres√≠a\n# a cada cluster con los datos\n# de las palabras\nda &lt;- igraph::as_data_frame(ga, \n                            what = \"vertices\")\n\n\nnames(da)[1] &lt;- \"word\"\n\nd4 &lt;- merge(d3, da, by=\"word\")\n\nd4 &lt;- d4[order(-d4$membership, \n               d4$betweenness,\n               decreasing = T),]\n\n\n# Seleccion las 20 palabras\n# con m√°s peso en cada grupo\nlibrary(dplyr)\n\nzz &lt;- d4 |&gt; \n  arrange(desc(betweenness)) |&gt;\n  group_by(membership) |&gt;\n  slice(1:10) \n\n\nreactable(zz, resizable=T)\n\n\n\n\n\n\n\n\nCode\nclust_no &lt;- 7\n\n## Which nodes should be kept?\nKeep1 = V(ga)[V(ga)$membership%in%clust_no]\n\n## Get subgraph & plot\nga1  &lt;- induced_subgraph(ga, Keep1)\n\nV(ga1)$weight &lt;- igraph::degree(ga1)\n\n\ndd1 &lt;- igraph_to_networkD3(ga1, group = V(ga1)$membership)\n\n\ndd1$nodes$weight &lt;- igraph::degree(ga1)\n\n\nforceNetwork(Links = dd1$links,  \n             linkColour = \"#C9C7C7\", \n             Nodesize = \"weight\",\n             width = 1200, \n             height = 1800, \n             bounded = T,\n             linkDistance = 80, \n             charge=-30,\n             opacity=1,\n             opacityNoHover=1,\n             Nodes = dd1$nodes, \n             Source = 'source', \n             Target = 'target', \n             NodeID = 'name', \n             Group = 'group',\n             fontSize = 22,\n             radiusCalculation = \"(Math.sqrt(d.nodesize)^0.57)*2\")\n\n\n\n\n\n\n\n\n\n\nCada comunidad se representa por un color distinto. El algoritmo ha identificado ocho comunidades distintas. La primera conformada por Dionisio, Pauloa, Buby, El odioso se√±or, Don Sacramento y Don Rosario. La segunda por Fanny, las tres muchachas y El anciano militar. La tercera por Sagra y El cazador astuto. La cuarta por Madame Olga y El guapo muchacho. La quinta por todos y ‚ÄúEl rom√°ntico enamorado. La sexta por‚ÄùUnos‚Äù y ‚ÄúOtros‚Äù. Las dos √∫ltimas son comunidades de un solo personaje: El coro de viejos extra√±os y El explorador.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#medidas-estructurales",
    "href": "redes.html#medidas-estructurales",
    "title": "Redes sem√°nticas",
    "section": "Medidas estructurales",
    "text": "Medidas estructurales\nLas m√©tricas estructurales permiten evaluar la configuraci√≥n general de la red. De cierta manera, posibilitan la cuantificaci√≥n de la topolog√≠a de la red. Mientras las medidas de centralidad se enfocan en la posici√≥n de los nodos individuales, las medidas estructurales se concentran en la forma global de la red ‚Äîsu cohesi√≥n, fragmentaci√≥n, modularidad o jerarqu√≠a‚Äî, lo que permite interpretar c√≥mo se organiza el sentido dentro de un corpus. Por lo tanto, describen las propiedades globales o mesoestructurales de una red, es decir, la configuraci√≥n de la red como un todo o atributos estructurales intermedios que permiten la identificaci√≥n de subgrupos o subunidades anal√≠ticas (temas, por ejemplo). En una red sem√°ntica revelan c√≥mo se distribuye y conecta el significado entre palabras o conceptos.\n\nDensidad: La densidad mide la proporci√≥n de enlaces existentes respecto a los posibles. Una red densa indica una alta interconexi√≥n entre los nodos, lo que sugiere un discurso cohesivo y bien integrado. En contraste, una red dispersa puede reflejar un discurso fragmentado o con m√∫ltiples temas poco relacionados o desconectados.\nConectividad/Componentes: Indica el n√∫mero de subconjuntos conectados. N√∫mero de campos sem√°nticos independientes o islas de sentido. Un solo componente indica unidad tem√°tica.\nClustering (transitividad): Probabilidad de que los vecinos de un nodo est√©n conectados. Nivel de redundancia local: si un concepto conecta grupos cerrados de palabras o temas. Alta transitividad = lenguaje repetitivo o especializado. Act√∫a a nivel de nodo\nModularidad: Grado en que la red se divide en comunidades o clusters. Por lo tanto, a diferencia de la medida anterior, act√∫a a nivel de red. En an√°lisis de texto, mide el grado de segmentaci√≥n tem√°tica: cu√°ntos subcampos sem√°nticos o discursos paralelos existen. Una alta modularidad suele indicar un discurso plural o polarizado.\nAsortividad: Correlaci√≥n entre grados de los nodos conectados. Si palabras frecuentes se conectan entre s√≠ (alta) o con palabras raras (baja). √ötil para ver si el discurso tiene un ‚Äún√∫cleo elitista‚Äù de t√©rminos.\nCore-periphery: Identifica un n√∫cleo denso de nodos altamente conectados y una periferia menos conectada. En an√°lisis de texto, revela si hay un conjunto central de conceptos clave que estructuran el discurso, rodeado por t√©rminos m√°s marginales o especializados. Frecuente en discursos ideol√≥gicos o narrativos.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#subestructuras-o-subcomponentes-de-una-red",
    "href": "redes.html#subestructuras-o-subcomponentes-de-una-red",
    "title": "Redes sem√°nticas",
    "section": "Subestructuras o subcomponentes de una red",
    "text": "Subestructuras o subcomponentes de una red\nEl an√°lisis de subestructuras topol√≥gicas permite pasar del nivel de palabras o coocurrencias a un nivel m√°s profundo: el de c√≥mo se organiza el pensamiento en red. Podeomos identificar patrones recurrentes de conexiones que revelan c√≥mo se agrupan las ideas y conceptos dentro del discurso.\n\nComponentes: Subconjuntos de nodos conectados entre s√≠, pero no con otros nodos fuera del componente. Indican campos sem√°nticos independientes o islas de sentido dentro del discurso. Una red con un solo componente sugiere unidad tem√°tica, mientras que m√∫ltiples componentes reflejan fragmentaci√≥n o diversidad tem√°tica. Pueden estar compuestos por otros subcomponentes m√°s sencillos estructuralmente, como los cliques o los motivos.\nCliques: Subconjuntos de nodos donde cada nodo est√° conectado a todos los dem√°s. Indican grupos tem√°ticos muy cohesivos o jergas especializadas dentro del discurso. Mientras m√°s grande el clique, m√°s compacto y redundante es el discurso (las palabras se repiten juntas) y menos probable es que haya innovaci√≥n sem√°ntica. Por el contrario, muchos cliques peque√±os sugieren un lenguaje modular y variado, con asociaciones contextuales espec√≠ficas.\nMotivos (motifs): Peque√±os patrones recurrentes de conexiones. Pueden representar patrones sint√°cticos o sem√°nticos t√≠picos, como tr√≠adas de palabras que co-ocurren habitualmente. Los motifs revelan patrones locales de asociaci√≥n: c√≥mo las palabras tienden a vincularse y qu√© formas de conexi√≥n predominan.\nK-cores: Subconjuntos de nodos donde cada nodo est√° conectado a al menos k otros nodos dentro del subconjunto. Indican n√∫cleos tem√°ticos robustos dentro del discurso. Un k-core con un valor alto de k sugiere un grupo central de conceptos clave que est√°n fuertemente interrelacionados, mientras que k-cores con valores bajos reflejan temas m√°s perif√©ricos o especializados.\nComunidades (clusters): Grupos de nodos con alta densidad de conexiones internas y baja densidad de conexiones externas. Indican subcampos sem√°nticos o discursos paralelos dentro del corpus. La identificaci√≥n de comunidades permite entender c√≥mo se segmenta el discurso en temas o √°reas de significado distintas.\nPuentes (bridges): Nodos o enlaces que conectan diferentes comunidades o clusters. Indican palabras o conceptos clave que facilitan la transici√≥n entre temas o √°reas de significado dentro del discurso. Los puentes son cruciales para la cohesi√≥n global del discurso, ya que permiten la integraci√≥n de ideas diversas.\nVac√≠os estructurales (structural holes): √Åreas en la red donde hay pocas o ninguna conexi√≥n entre nodos. Indican lagunas tem√°ticas o √°reas de significado no exploradas dentro del discurso. Identificar estos vac√≠os puede revelar oportunidades para la innovaci√≥n sem√°ntica o la introducci√≥n de nuevos temas.\nbackbone: La estructura esencial de la red, compuesta por los nodos y enlaces m√°s importantes. En an√°lisis de texto, el backbone revela el n√∫cleo conceptual del discurso, destacando las palabras y conexiones que son fundamentales para la comprensi√≥n del tema central.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#elementos-b√°sicos-de-una-red",
    "href": "redes.html#elementos-b√°sicos-de-una-red",
    "title": "Redes sem√°nticas",
    "section": "Elementos b√°sicos de una red",
    "text": "Elementos b√°sicos de una red\nUna red est√° compuesta por nodos y enlaces. Los nodos representan las entidades que se est√°n analizando, como personas, instituciones o palabras. Los enlaces representan las relaciones o interacciones entre esos nodos, como conversaciones, colaboraciones o coocurrencias en un texto.\nA partir de estos elementos b√°sicos, se pueden construir gr√°ficos de red que visualizan las conexiones entre los nodos. Estos gr√°ficos permiten identificar patrones de interacci√≥n, comunidades dentro de la red y nodos clave que desempe√±an roles importantes en la estructura general.\nLas redes pueden ser dirigidas o no dirigidas. En una red dirigida, los enlaces tienen una direcci√≥n espec√≠fica, lo que indica qui√©n inicia la interacci√≥n o relaci√≥n. En una red no dirigida, los enlaces son bidireccionales, lo que significa que la relaci√≥n es mutua entre los nodos conectados. Por ejemplo, las redes sociales representan redes dirigidas. Nosotros podemos seguir a otras personas que no necesariamente nos siguen a nosotros. No hace falta la reciprocidad en la acci√≥n.\nEsas dos piececitas b√°sicas pueden combinarse de m√∫ltiples formas, generando arquitecturas complejas que permiten an√°lisis bastante sofisticados. Emplean dos formas fundamentales (o piezas de LEGO) que constituyen la base de todas las dem√°s estructuras en la red:\n\nd√≠adas: dos nodos conectados por un enlace. Representan la relaci√≥n m√°s b√°sica entre dos entidades. Es la forma de organizaci√≥n social m√°s b√°sica.\ntr√≠adas: tres nodos conectados por enlaces. Permiten analizar relaciones m√°s complejas y la formaci√≥n de grupos dentro de la red.\n\nAbajo pod√©is ver el listado de todas las combinaciones posibles entre tres nodos, lo que se denominan como motivos (motifs) en una red no dirigida:\n\n\n\n\n\n\n\n\n\nY ahora en una red dirigida:\n\n\n\n\n\n\n\n\n\nComo podemos ver, las relaciones pueden ser direccionales o no, rec√≠procas o no, transitivas o c√≠clicas, entre otras muchas posibilidades. Estas estructuras b√°sicas pueden combinarse para formar redes m√°s complejas que reflejan las interacciones reales entre entidades en diversos contextos.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#antes-de-empezar",
    "href": "redes.html#antes-de-empezar",
    "title": "Redes sem√°nticas",
    "section": "Antes de empezar",
    "text": "Antes de empezar\nAntes de empezar, lo mejor es crear los datos para que podamos ilustrar los conceptos b√°sicos de las redes sem√°nticas. Empezamos por crear una funci√≥n llamada genCoocurrence que genera una red de co-ocurrencias con todas las palabras del corpus. Esta funci√≥n tiene tres par√°metros. El primero es el corpus de quanteda con los textos a analizar. El segundo, adjacent=TRUE, corresponde una variable l√≥gica que indica si la co-ocurrencia ser√° adyacente (una palabra y su siguiente), con valor adjacent=TRUE, o general (pares de todas palabras del texto entre s√≠), con valor adjacent=FALSE. Finalmente, el √∫ltimo par√°metro establece si la funci√≥n retorna un objeto graph de igraph (return.graph=TRUE) o un data.frame (return.graph=FALSE).\n\n\nCode\n# Crea la funci√≥n para generar una\n# red de coocurrencias a partir de\n# un corpus de texto\ngenCoocurrence &lt;- function(corpus, \n                           adjacent=TRUE, \n                           return.graph=TRUE){\n  \n  tk &lt;- quanteda::tokens(corpus, \n               remove_punct = T, \n               remove_symbols = T, \n               remove_numbers = T)\n  \n  tk &lt;- quanteda::tokens_remove(tk, \n                      c(quanteda::stopwords(\"es\"),\"‚Äî\",\"‚Äî‚Äå\"))\n  \n  # Crea una base de datos\n  # que ir√° acumular las coocurrencias\n  res &lt;- data.frame()\n  \n  # Para cada di√°logo\n  for(i in 1:length(tk)){\n    \n    # Obtiene las palabras\n    ky &lt;- as.character(tk[[i]])\n    \n    # Si solo hay una palabra\n    # para al siguiente di√°logo\n    if(length(ky) &lt; 2){\n      next\n    }\n    \n    if (adjacent==TRUE){\n      \n      # Genera las coocurrencias usando adyacencia\n      d1 &lt;- data.frame(from = ky[-length(ky)], \n                       to = ky[-1])\n      \n    }else{\n      \n      # Genera las coocurrencias usando ventana\n      d1 &lt;- data.frame(t(combn(ky,2)))\n      \n      names(d1) &lt;- c(\"from\", \"to\")\n    }\n    \n    # Establece el peso\n    d1$weight &lt;- 1\n    \n    # Acumula los resultados\n    res &lt;- rbind(res, d1)\n    \n  }\n  \n  res &lt;- res\n  res$from &lt;- tolower(res$from)\n  res$to &lt;- tolower(res$to)\n  \n  \n  g &lt;- igraph::graph_from_data_frame(res, \n                             directed = FALSE)\n  \n  g &lt;- igraph::simplify(g, \n                        remove.multiple = FALSE)\n  \n  \n  # Crea una nueva base de datos\n  # con la red simplificada\n  res &lt;- igraph::as_data_frame(g)\n  \n  # Agrega las coocurrencias por\n  # diada de palabras\n  ag &lt;- aggregate(weight ~ from + to, \n                  data = res, \n                  FUN = sum)\n  \n  \n  if(return.graph==TRUE){\n  \n    # convierte en un gr√°fico no direccional\n    g &lt;- igraph::graph_from_data_frame(ag, \n                                       directed = FALSE)\n    \n    # Establece la frecuencia como\n    # el peso\n    igraph::E(g)$weight &lt;- ag$weight\n  \n    return(g)  \n  \n  }else{\n    \n    return(ag)\n    \n  }\n  \n}\n\n\nA continuaci√≥n, empleamos la funci√≥n reci√©n generada para crear la red de co-ocurrencias de palabras en el debate sobre la ley de memoria hist√≥rica. En total son 5641 palabras y 148.819 v√≠nculos entre ellas. Se trata de una red sin filtrar. No hemos eliminado ninguna relaci√≥n todav√≠a, de modo que no recomiendo que intentemos visualizarla. Seguramente no obtendremos buenos resultados visuales.\n\n\nCode\nlibrary(quanteda)\n\n\nda &lt;- spa.sessions[grep(\"N√∫mero de expediente 121/000064\", \n                        spa.sessions$issue.details),]\n\nda &lt;- da[da$rep.condition!=\"Miembro de la mesa\",]\n\n\ncp &lt;- corpus(da, text_field = \"speech.text\")\ncp &lt;- corpus_reshape(cp, \n                     to = \"sentences\")\n\n# Genera el gr√°fico a partir\ng &lt;- genCoocurrence(cp)",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#comunidades-como-temas",
    "href": "redes.html#comunidades-como-temas",
    "title": "Redes sem√°nticas",
    "section": "Comunidades como temas",
    "text": "Comunidades como temas\nComo hemos vistos, las comunidades o conglomerados son subgrupos dentro de una red que presentan una mayor densidad de interacciones entre sus miembros que con el resto de la red. En el an√°lisis de redes sem√°nticas, las comunidades pueden interpretarse como temas o subcampos sem√°nticos dentro del corpus analizado. Identificar estas comunidades permite comprender c√≥mo se organiza el discurso en torno a diferentes √°reas tem√°ticas y c√≥mo se relacionan entre s√≠.\nEn nuestro caso, emplearemos el algoritmo de Louvain para detectar comunidades dentro de la red de coocurrencias generada a partir del debate sobre la Ley de Memoria Democr√°tica. Este algoritmo es eficiente y efectivo para identificar grupos densamente conectados en grandes redes. A partir de los resultados podremos ano solo identificar los temas principales, sino que tambi√©n analizar los t√©rminos que caracterizan el debate.\n\n\nCode\n# Usa la red de coocurrencias\n# generada para la Ley de \n# Memoria Democr√°tica\nga &lt;- g\n\n# Crea un cluster de palabras\n# seg√∫n la conectividad de red\nset.seed(12358)\n\nwc &lt;- cluster_louvain(ga, \n                      weights = E(ga)$weight)\n\n\n# Atribuye la membres√≠a a los \n# grupos\nV(ga)$membership &lt;- wc$membership\n\n# Calcula las centralidades de red\n# betweenness y degree\nbt &lt;- round(betweenness(ga),3)\n\nd1 &lt;- data.frame(word=names(bt), \n                 betweenness=round(bt))\n\nbt &lt;- igraph::degree(ga)\n\nd2 &lt;- data.frame(word=names(bt), \n                 degree=bt)\n\nd3 &lt;- merge(d2, d1, by=\"word\")\n\nco &lt;- coreness(ga)\n\nd2 &lt;- data.frame(word=names(co), \n                 coreness=co)\n\nd3 &lt;- merge(d3, d2, by=\"word\")\n\n\nd3 &lt;- d3[order(d3$betweenness, \n               decreasing = T),]\n\n\n# Fusiona los datos de membres√≠a\n# a cada cluster con los datos\n# de las palabras\nda &lt;- igraph::as_data_frame(ga, \n                            what = \"vertices\")\n\n\nnames(da)[1] &lt;- \"word\"\n\nd4 &lt;- merge(d3, da, by=\"word\")\n\nd4 &lt;- d4[order(-d4$membership, \n               d4$betweenness,\n               decreasing = T),]\n\n\n# Seleccion las 20 palabras\n# con m√°s peso en cada grupo\nlibrary(dplyr)\n\nzz &lt;- d4 |&gt; \n  arrange(desc(betweenness)) |&gt;\n  group_by(membership) |&gt;\n  slice(1:10) \n\n# Muestra la tabla de comunidades\nreactable(zz, resizable=T)\n\n\n\n\n\n\n\nEl paso siguiente puede ser averiguar el peso relativo de cada comunidad dentro del debate. Para ello, hemos creado una funci√≥n que calcula\n\n\nCode\n# Funcion para calcular el peso\n# relativo de cada cluster en la\n# red semantica\npesoCluster &lt;- function(g, \n                        cluster_attr = \"cluster\",\n                        centralidad = c(\"degree\", \"betweenness\")) {\n  library(dplyr)\n  library(igraph)\n  \n  # Validaciones b√°sicas\n  if (!cluster_attr %in% vertex_attr_names(g)) stop(\"El grafo no tiene atributo de cluster especificado.\")\n  centralidad &lt;- intersect(centralidad, vertex_attr_names(g))\n  if (length(centralidad) == 0) stop(\"No se encontraron medidas de centralidad en los atributos del grafo.\")\n  \n  # Extraer data frame de nodos y aristas\n  nodes &lt;- igraph::as_data_frame(g, what = \"vertices\")\n  edges &lt;- igraph::as_data_frame(g, what = \"edges\")\n  \n  # --- 1. Peso por tama√±o del cluster ---\n  peso_tama√±o &lt;- nodes %&gt;%\n    count(!!sym(cluster_attr), name = \"n_nodos\") %&gt;%\n    mutate(peso_tama√±o = n_nodos / sum(n_nodos))\n  \n  # --- 2. Peso por centralidad total (degree y betweenness) ---\n  peso_centralidad &lt;- nodes %&gt;%\n    group_by(!!sym(cluster_attr)) %&gt;%\n    summarise(across(all_of(centralidad), sum, .names = \"sum_{col}\")) %&gt;%\n    ungroup()\n  \n  for (c in centralidad) {\n    colname &lt;- paste0(\"sum_\", c)\n    peso_centralidad[[paste0(\"peso_\", c)]] &lt;- peso_centralidad[[colname]] / sum(peso_centralidad[[colname]])\n  }\n  \n  # --- 3. Densidad interna por cluster ---\n  clusters &lt;- unique(nodes[[cluster_attr]])\n  densidades &lt;- data.frame(cluster = clusters, densidad = NA_real_)\n  \n  for (i in seq_along(clusters)) {\n    subg &lt;- induced_subgraph(g, vids = which(nodes[[cluster_attr]] == clusters[i]))\n    densidades$densidad[i] &lt;- if (vcount(subg) &gt; 1) edge_density(subg) else 0\n  }\n  \n  # --- 4. Enlaces externos ---\n  edges$c1 &lt;- nodes[[cluster_attr]][match(edges$from, nodes$name)]\n  edges$c2 &lt;- nodes[[cluster_attr]][match(edges$to, nodes$name)]\n  enlaces_externos &lt;- edges %&gt;%\n    filter(c1 != c2) %&gt;%\n    count(c1, name = \"enlaces_externos\") %&gt;%\n    rename(cluster = c1) %&gt;%\n    mutate(peso_externo = enlaces_externos / sum(enlaces_externos))\n  \n  names(densidades)[1] &lt;- cluster_attr\n  names(enlaces_externos)[1] &lt;- cluster_attr\n  \n  res &lt;- merge(peso_tama√±o, peso_centralidad, by=cluster_attr, all.x=T)\n  res &lt;- merge(res, densidades, by=cluster_attr, all.x=T)\n  res &lt;- merge(res, enlaces_externos, by=cluster_attr, all.x=T)\n  \n  res$densidad[is.na(res$densidad)] &lt;- 0\n  res$peso_externo[is.na(res$peso_externo)] &lt;- 0\n  res$enlaces_externos[is.na(res$enlaces_externos)] &lt;- 0\n  \n  res$peso_tama√±o &lt;- round(res$peso_tama√±o, 4)\n  res$peso_degree &lt;- round(res$peso_degree, 4)\n  res$peso_betweenness &lt;- round(res$peso_betweenness,4)\n  res$densidad &lt;- round(res$densidad, 4)\n  res$peso_externo &lt;- round(res$peso_externo, 4)\n\n    \n  return(res)\n}\n\n\n# Calcula las centralidades de red\nV(ga)$degree &lt;- igraph::degree(ga)\nV(ga)$betweenness &lt;- igraph::betweenness(ga)\n\n# Calcula el peso de cada cluster\npesos &lt;- pesoCluster(ga, \n                     cluster_attr = \"membership\")\n\n\n# Muestra los pesos de cada cluster\nreactable(pesos, \n          resizable=T)\n\n\n\n\n\n\nAhora pasamos a la visualizaci√≥n del cluster 7, que contiene el debate sobre las v√≠ctimas y muestra las posiciones encontradas de actores de la derecha y la izquierda de la pol√≠tica espa√±ola. No visualizamos toda la red, porque ser√≠a poco productivo. Se tratan de 5.641 nodos y 14.638 v√≠nculos, algo muy complicado de representar visualmente. El cluster 7 contiene 460 nudos y 720 v√≠nculos, que resulta un poco m√°s f√°cil de visualizar:\n\n\nCode\n# Define el n√∫mero de cluster\nclust_no &lt;- 7\n\n# Selecciona el cluster 7\nKeep1 = V(ga)[V(ga)$membership%in%clust_no]\n\n# filtra solo los nodos del cluster\nga1  &lt;- induced_subgraph(ga, Keep1)\n\n# Calcula el numero de conexiones de cada nodo\nV(ga1)$weight &lt;- igraph::degree(ga1)\n\n# Elimina los nodos con menos de 3 conexiones\n# para facilitar la visualizaci√≥n\nga1 &lt;- ga1 - V(ga1)$name[V(ga1)$weight&lt;3]\n\n# Convierte a formato networkD3\ndd1 &lt;- igraph_to_networkD3(ga1, group = V(ga1)$membership)\n\n# Asigna el peso de cada nodo\ndd1$nodes$weight &lt;- igraph::degree(ga1)\n\n\n\n# Visualiza la red\nforceNetwork(Links = dd1$links,  \n             linkColour = \"#C9C7C7\", \n             Nodesize = \"weight\",\n             width = 1200, \n             height = 1800, \n             bounded = T,\n             linkDistance = 80, \n             charge=-30,\n             opacity=1,\n             opacityNoHover=1,\n             Nodes = dd1$nodes, \n             Source = 'source', \n             Target = 'target', \n             NodeID = 'name', \n             Group = 'group',\n             fontSize = 22,\n             radiusCalculation = \"(Math.sqrt(d.nodesize)^0.57)*10\")\n\n\n\n\n\n\n\n\n\n\nUna vez generados los clusters de palabras, cada uno de ellos nos indicar√° un tema contenido en el debate. Como hemos visto en el sociograma anterior, resulta posible identificar un nodo central, v√≠ctimas, circundado por otros nodos altamente interconectados como guerra, franquismo, dictadura, eta, cr√≠menes, civil, represi√≥n. Eso nos revela que el discurso se centra en las v√≠ctimas para articular todo el debate sobre la dictadura franquista, la guerra civil y el terrorismo de ETA.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#usando-la-ia-en-la-interpretaci√≥n",
    "href": "redes.html#usando-la-ia-en-la-interpretaci√≥n",
    "title": "Redes sem√°nticas",
    "section": "Usando la IA en la interpretaci√≥n",
    "text": "Usando la IA en la interpretaci√≥n\nEl algoritmo nos ha permitido identificar 34 clusters o comunidades de palabras dentro de la red sem√°ntica generada a partir del debate parlamentario sobre la Ley de Memoria Democr√°tica. Cada uno de estos clusters representa un tema o subcampo sem√°ntico dentro del corpus analizado. No obstante, la s√≠ntesis de tanto material emp√≠rico puede ser abrumadora. Interpretar m√°s de 5600 palabras y darles sentido reconstruyendo un discurso no es una tarea f√°cil. En nuestro ejemplo de ‚Äújuguete‚Äù tenemos un debate √∫nico que puede ser le√≠do directamente, sin la necesidad de una metodolog√≠a tan complicada para su interpretaci√≥n. No obstante, en vol√∫menes a√∫n mayores de texto, resultar√≠a imposible llevar a cabo una lectura de todos los debates o textos.\nEn estos casos, podemos emplear la IA para ayudarnos en la interpretaci√≥n de los temas identificados. A continuaci√≥n, mostramos c√≥mo utilizar Google Gemini para generar t√≠tulos y descripciones para cada cluster identificado en la red sem√°ntica. El prompt que utilizamos es el siguiente:\n# ROL\nAct√∫a como un analista de datos especialista en an√°lisis de redes sem√°nticas y modelado de t√≥picos.\n# TAREA\nLe suministrar√© una string CSV con palabras y clusters que corresponden a temas, as√≠ como medidas de centralidad de red.\nAnaliza los datos y sugiere un t√≠tulo para cada cluster y una breve descripci√≥n de cada tema / cluster, as√≠ como identifica subtemas y etiquedas sint√©ticas.\nTu tarea consiste en:\n\nDescribir el tema central de cada cluster.\nIdentificar los subtemas y marcos discursivos implicados.\nProponer etiquetas sint√©ticas (1 a 3 palabras) para cada cluster.\n\nNo olvides de tener en cuenta las m√©tricas de centralidad de red (degree, betweeness y coreness) para determinar el peso relativo de cada palabra en la red y en el cluster.\n# SALIDA\nGenera una string CSV con las siguientes variables: cluster, tema, t√≠tulo, descripci√≥n (de al menos 500 palabras), subtemas, marco_discursivo, tono, palabras_centrales\n# RESTRICCIONES\nNo introducir comentarios o cualquier otra marca que no sea la string CSV de salida.\n# DATOS DE ENTRADA\nAbajo puedes encontrar los datos CSV de entrada:\n[AQUI LOS DATOS DEL DATA.FRAME]\n\n\nCode\nlibrary(ellmer)\n\npt &lt;- \"# ROL\\n\\nAct√∫a como un analista de datos especialista en an√°lisis de redes sem√°nticas y modelado de t√≥picos.\\n\\n# TAREA\\n\\nLe suministrar√© una string CSV con palabras y clusters que corresponden a temas, as√≠ como medidas de centralidad de red.\\n\\nAnaliza los datos y sugiere un t√≠tulo para cada cluster y una breve descripci√≥n de cada tema / cluster, as√≠ como identifica subtemas y etiquedas sint√©ticas.\\n\\nTu tarea consiste en:\\n\\n1. Describir el tema central de cada cluster.\\n2. Identificar los subtemas y marcos discursivos implicados.\\n3. Proponer etiquetas sint√©ticas (1 a 3 palabras) para cada cluster.\\n\\nNo olvides de tener en cuenta las m√©tricas de centralidad de red (degree, betweeness y coreness) para determinar el peso relativo de cada palabra en la red y en el cluster.\\n\\n# SALIDA\\n\\nGenera una string de valores separados por punto y coma (;) con las siguientes variables: cluster; tema; t√≠tulo; descripci√≥n (de al menos 500 palabras); subtemas; marco_discursivo; tono; palabras_centrales\\n\\n# RESTRICCIONES\\n\\nNo introducir comentarios o cualquier otra marca que no sea la string CSV de salida.\\n\\n# DATOS DE ENTRADA\\n\\nAbajo puedes encontrar los datos CSV de entrada:\\n\\n\"\n\n# Convierte los datos a formato CSV\nlibrary(readr)\nss &lt;- format_csv(d4)\n\n# Completa el prompt con los datos\npt &lt;- paste0(pt, ss, collapse = \"\\n\")\n\n# Crea el objeto de chat con Google Gemini\nchat &lt;- chat_google_gemini(\n    api_args = list(generationConfig = \n                      list(temperature=0.2, \n                           seed=12358)),  \n    model = \"gemini-flash-lite-latest\", \n    echo = \"none\")\n\n# Ejecuta el chat\nout &lt;- chat$chat(pt)\n\n# Extrae la string CSV de la respuesta\nout &lt;- read_delim(out, delim = \";\")\n\n# Asigna nombres a las columnas\nnames(out) &lt;- c(\"cluster\",\"tema\",\"titulo\",\n                \"descripcion\",\"subtemas\",\"marco_discursivo\",\n                \"tono\",\"palabras_centrales\")\n\n# Muestra los resultados\nreactable(out,\n         searchable = TRUE,\n         wrap = F,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)\n\n\n\nPara ilustrar c√≥mo la mezcla de man√°lisis de redes con la IA puede facilitar la interpretaci√≥n de grandes vol√∫menes de texto, a continuaci√≥n mostramos otro ejemplo en el que analizamos el mismo debate. No obstante, en lugar de procesar los discursos por medio de redes sem√°nticas, le suministramos el texto directo del debate al LLM y pedimos que el modelo nos identifique los temas:\n\n\nCode\nda &lt;- spa.sessions[grep(\"N√∫mero de expediente 121/000064\", \n                        spa.sessions$issue.details),]\n\nda &lt;- da[da$rep.condition!=\"Miembro de la mesa\",]\n\nda$rep.name[is.na(da$rep.name)] &lt;- \"Ministro\"\n\nda &lt;- da[, c(\"session.date\",\n             \"session.number\",\n             \"speech.order\",\n             \"speech.text\",\n             \"rep.name\",\n             \"rep.party\")]\n\nda &lt;- da[order(da$speech.order),]\n\nlibrary(ellmer)\n\npt &lt;- \"# ROL\\n\\nAct√∫a como un analista de datos experto en an√°lisis de discurso.\\n\\n# TAREA\\n\\nLe suministrar√© una string CSV con textos del debate parlamentario sobre el proyecto de ley de la memoria historica.\\nAnaliza los datos y extrae los principales temas debatidos, dales un t√≠tulo y genera una descripci√≥n de al menos 500 palabras.\\n\\n# SALIDA\\n\\nGenera una string CSV de valores separados por punto y coma (;) con las siguientes variables: tema; t√≠tulo; descripci√≥n.\\n\\n# RESTRICCIONES\\n\\nNo introducir comentarios o cualquier otra marca que no sea la string CSV de salida.\\n\\n# DATOS DE ENTRADA\\n\\nAbajo puedes encontrar los datos CSV de entrada:\\n\\n\"\n\n\n# Convierte los datos a formato CSV\nlibrary(readr)\nss &lt;- format_csv(da)\n\n# Completa el prompt con los datos\npt &lt;- paste0(pt, ss, collapse = \"\\n\")\n\n# Crea el objeto de chat con Google Gemini\nchat &lt;- chat_google_gemini(\n    api_args = list(generationConfig = \n                      list(temperature=0.2, \n                           seed=12358)),  \n    model = \"gemini-flash-latest\", \n    echo = \"none\")\n\n# Ejecuta el chat\nout &lt;- chat$chat(pt)\n\n# Extrae la string CSV de la respuesta\n# Extrae la string CSV de la respuesta\not &lt;- read_delim(out, delim = \";\")\n\n# Asigna nombres a las columnas\nnames(ot) &lt;- c(\"tema\",\"titulo\",\"descripcion\")\n\n# Muestra los resultados\nreactable(ot,\n         searchable = TRUE,\n         wrap = F,\n         resizable = T,\n         filterable = TRUE,\n         defaultPageSize = 10,\n         highlight = TRUE\n)\n\n\n\nComo podemos observar, el resultado es mucho menos detallado que el obtenido mediante el an√°lisis de redes sem√°nticas combinado con la IA.",
    "crumbs": [
      "An√°lisis de redes"
    ]
  },
  {
    "objectID": "redes.html#filtros",
    "href": "redes.html#filtros",
    "title": "Redes sem√°nticas",
    "section": "Filtros",
    "text": "Filtros\nLa versi√≥n que hemos apresentado es una versi√≥n completa del gr√°fico. No obstante, podemos filtrar los datos para facilitar la comprensi√≥n de la red. Por ejemplo, podemos eliminar las aristas con menos de tres casos para centrarnos en las co-ocurrencias y palabras m√°s relevantes dentro del cluster. Esto nos permite visualizar mejor la red como un todo y vislumbarar las relaciones entre los distintos temas.\n\n\nCode\n# Genera el gr√°fico ahora como data.frame\n# para poder filtrar los v√≠nculos\ngf &lt;- genCoocurrence(cp, \n                    return.graph = FALSE)\n\n# Filtra los v√≠nculos con peso &gt;2\ngf &lt;- gf[gf$weight&gt;2,]\n\n# Reconstruye el grafo\ngb &lt;- graph_from_data_frame(gf, \n                            directed = F)\n\nE(gb)$weight &lt;- gf$weight\n\n# Crea un cluster de palabras\nset.seed(12358)\nwc &lt;- cluster_louvain(gb, \n                      weights = E(gb)$weight)\n\n# Atribuye la membres√≠a a los \n# grupos\nV(gb)$membership &lt;- wc$membership\n\n# Calcula el numero de conexiones de cada nodo\nV(gb)$weight &lt;- igraph::degree(gb)\n\n# Convierte a formato networkD3\ndd &lt;- igraph_to_networkD3(gb, group = V(gb)$membership)\ndd$nodes$weight &lt;- igraph::degree(gb)",
    "crumbs": [
      "An√°lisis de redes"
    ]
  }
]